<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jellyyoung&#39;s Blog</title>
  <subtitle>Practice makes me progressive</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yaodong.ml/"/>
  <updated>2017-04-13T05:37:25.403Z</updated>
  <id>http://yaodong.ml/</id>
  
  <author>
    <name>摇摇果冻</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>机器学习算法笔记——随机森林（Random Forest）</title>
    <link href="http://yaodong.ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%EF%BC%88Random-Forest%EF%BC%89.html"/>
    <id>http://yaodong.ml/机器学习算法笔记——随机森林（Random-Forest）.html</id>
    <published>2017-02-13T05:36:30.000Z</published>
    <updated>2017-04-13T05:37:25.403Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;随机森林算法学习笔记总结。<br><a id="more"></a><br>&emsp;&emsp;随机森林由许多的决策树组成，这些决策树的形成采用了“随机”的方法，所以叫做随机森林。随机森林中的决策树之间是没有关联的。<br>&emsp;&emsp;随机森林中的所有树都使用同样的训练参数。但是每个决策树的训练集是不同的，每个决策树的错误估计采用OOB（Out of Bag）的方法。</p>
<h3 id="Bagging方法"><a href="#Bagging方法" class="headerlink" title="Bagging方法"></a>Bagging方法</h3><p>&emsp;&emsp;Bagging方法就是将所有training data放进一个“黑色”的bag中，然后从这个bag中随机抽取部分数据生成新的训练集。随机森林算法中，样本训练集本省可以使用bagging方法，同样，样本的feature也可以进行bagging。从随机性来看，bagging技术可以有效的减小方差，即减小过拟合程度<br>&emsp;&emsp;随机森林是一种经典而强大的机器学习算法，具有回归和分类的功能。随机森林算法由若干决策树组成，这些决策树一般采用随机的方法生成，因此也叫做随机决策树。随机森林算法中的各决策树之间是没有关联的。</p>
<h3 id="Bootstrap抽样"><a href="#Bootstrap抽样" class="headerlink" title="Bootstrap抽样"></a>Bootstrap抽样</h3><p>&emsp;&emsp;随机森林算法中包含了对输入数据的重复自抽样过程，即所谓的bootstrap抽样。大约三分之一的数据集将用于测试而不是模型的训练，这样的数据被称为out of bag samples。<br>&emsp;&emsp;bootstrap抽样与bagging的区别是：在生成每棵树的时候，每个节点变量都仅仅在随机选出的少数变量中产生。因此，不但样本是随机的，连每个节点变量（Features）的产生都是随机的。<br>&emsp;&emsp;综上可知，随机森林算法的的两个随机采样的过程保证了随机性，所以即使对最终的各决策树不剪枝，也不会出现over-fitting。</p>
<h3 id="随机采样"><a href="#随机采样" class="headerlink" title="随机采样"></a>随机采样</h3><p>&emsp;&emsp;随机森林的随机体现在两个方面：数据集的<strong>行随机采样</strong>和<strong>列随机采样</strong>。行随机采样采用<strong>Bootstraping</strong>的方法对训练数据集进行又放回抽样。列随机采样指的是从所有$M$个特征属性中随机选择$m$个属性，同时保证$m &lt;&lt; M$。相当于把$M$维特征做了低维度的投影。减小特征选择的数量$m$，决策树之间的相关性和决策树的分类能力也会相应的降低；增大m，决策树之间的相关性和决策树的分类能力也会随之增大。<br>&emsp;&emsp;随机采样的后果是生成的每一个决策树的分类能力都很有限，但是将这些性能有限的决策树组合在一起的模型的性能就大大加强了。随机森林的最后结果是由多个决策树的训练结果进行加权组合而得到。</p>
<h3 id="完全分裂"><a href="#完全分裂" class="headerlink" title="完全分裂"></a>完全分裂</h3><p>&emsp;&emsp;随机森林建立决策树的过程不考虑决策树是否会发生过拟合。也就是说，随机森林对采样之后的数据采用完全分裂的方式建立决策树，因此，决策树的某一个叶子结点要么是无法继续分裂的，要么对应的所有样本都属于同一类。再次强调，<strong>随机森林中的两个“随机采样”充分保证了样本的随机性，因此即使不剪枝，也不会出现过拟合</strong>。</p>
<h3 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h3><ol>
<li>通过自助法（bootstrap）重采样技术，从原始训练样本集中有放回地重复随机抽取N个样本生成新的训练样本集合</li>
<li>对随机采样得到的新训练数据集，构建决策树，在每个节点执行以下操作：</li>
</ol>
<ul>
<li>从样本数据的M个features中随机选取m($m&lt;&lt;M$)个feature</li>
<li>对这m个features，选择特定的度量准则分割节点</li>
<li>重复上述操作N次，从而生成与样本数量相等的决策树</li>
</ul>
<ol>
<li>对于每一个测试样例，对k颗决策树的预测结果进行投票。票数最多的结果就是随机森林的分裂（预测）结果</li>
</ol>
<h3 id="特征重要性度量"><a href="#特征重要性度量" class="headerlink" title="特征重要性度量"></a>特征重要性度量</h3><p>&emsp;&emsp;随机森林能够计算特征的重要性，因此随机森林可用于特征选择。<br>&emsp;&emsp;随机森林算法中特征$X$的重要性的计算方法如下：</p>
<ol>
<li>对于随机森林中的每一棵决策树，输入都是经bootstrap采样的随机采样数据，使用OOB数据计算当前决策树模型的袋外错误率，记为$errorOOB1$</li>
<li>对每棵决策树的OOB数据的特征$X$加入噪声干扰，再次计算OOB错误率，记为$errorOOB2$</li>
<li>特征$X$的重要性可通过以下公式来衡量：<script type="math/tex; mode=display">X\_importacne = \frac{1}{N}\sum_{i=1}^{N}(errorOOB2 - errorOOB1)</script>&emsp;&emsp;不难理解，对于同一棵决策树来说，如果对特征$X$加入噪声干扰后，$errorOOB2$如果大幅度降低，说明该特征对样本的分类结果影响很大，即特征更重要。</li>
</ol>
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><p>&emsp;&emsp;随机森林算法有以下优点：</p>
<ul>
<li>属于集成学习模型，适合多分类模型，训练和预测速度快，在数据集上表现良好</li>
<li>对训练数据的容错能力强，即使训练数据中有缺失值，仍然可以保持预测精度不变</li>
<li>随机森林在分类的过程中，对generlization error使用的是内部无偏估计</li>
<li>能够在训练过程中检测到特征之间的相互影响以及各个特征的重要性程度</li>
<li>随机采样和特征属性采样保证了算法不易出现过度拟合</li>
<li>算法训练速度快，方便并行实现</li>
<li>适合用于多分类问题，算法训练和预测速度快，容易实现并行化</li>
<li>可有效估计缺失数据，即有一定程度的数据容错能力，当数据集中有大比例的数据缺失时仍然可以保持精度不变和能够有效地处理大的数据集</li>
<li>不会出现过拟合</li>
<li>能够处理很高维度（feature很多）的数据，并且不用做特征选择</li>
<li>对数据集的适应能力强：既能处理离散型数据，也能处理连续型数据，数据集无需规范化</li>
<li>可检测样本的各特征（维度）之间的相互影响程度，判断特征对所解决问题的重要性程度</li>
<li>可直接处理大规模的的变量（群）</li>
<li>在创建森林即分类的过程中，对泛化误差的估计是内部无偏估计</li>
<li>随机森林算法也是一种数据降维的方法，用于处理缺失值、异常值</li>
<li><h3 id="scikit-learn中的Random-Forest算法"><a href="#scikit-learn中的Random-Forest算法" class="headerlink" title="scikit-learn中的Random Forest算法"></a>scikit-learn中的Random Forest算法</h3>参考代码如下：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sklearn.ensemble.RandomForestClassfier</div><div class="line">clf = RandomForestClassifier([parameters]) <span class="comment">#generator the entity object of classifier</span></div><div class="line">parameters:</div><div class="line">n_estimators：指定随机森林中树的数目，越多越好，不超过内存即可</div><div class="line">criterion:指定在分裂使用的决策算法，取值有“entropy”、“gini”等</div><div class="line">max_features:单个决策树使用特征的最大数量，取值为<span class="string">"Auto"</span>，<span class="string">"None"</span>，<span class="string">"sqrt"</span>，<span class="string">"0.X"</span>。回归问题，max_features=n_features,分类问题，max_features=sqrt(n_features),<span class="string">"sqrt"</span>即为全部特征数目的平均根</div><div class="line">max_depth:默认为<span class="keyword">None</span>，一般可不改动</div><div class="line">min_simples_split:</div><div class="line">min_samples_leaf:最小叶片大小。默认值为<span class="number">1</span>，可设置为<span class="number">50</span>。叶是决策树的末端节点，较小的叶子使模型更容易捕捉训练数据中的噪声。</div><div class="line">min_weight_fraction_leaf:</div><div class="line">max_leaf_nodes:</div><div class="line">min_impurity_split:</div><div class="line">bootstrap:</div><div class="line">oob_score:这是一个随机森林交叉验证方法，取值为boolean类型，<span class="string">"True"</span>,<span class="string">"False"</span></div><div class="line">n_jobs:指定并行训练时使用的进程数。“<span class="number">-1</span>”表示使用所有处理器</div><div class="line">random_state:经验值<span class="string">"random_state=50"</span></div><div class="line">verbose:</div><div class="line">warm_state:</div><div class="line">class_weight:</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;随机森林算法学习笔记总结。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Machine Learning" scheme="http://yaodong.ml/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习算法笔记——随机森林（Random Forest）</title>
    <link href="http://yaodong.ml/machine-learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%EF%BC%88Random-Forest%EF%BC%89.html"/>
    <id>http://yaodong.ml/machine-learning/机器学习算法笔记——随机森林（Random-Forest）.html</id>
    <published>2017-02-13T05:36:30.000Z</published>
    <updated>2017-04-13T05:37:08.662Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;随机森林算法学习笔记总结。<br><a id="more"></a><br>&emsp;&emsp;随机森林由许多的决策树组成，这些决策树的形成采用了“随机”的方法，所以叫做随机森林。随机森林中的决策树之间是没有关联的。<br>&emsp;&emsp;随机森林中的所有树都使用同样的训练参数。但是每个决策树的训练集是不同的，每个决策树的错误估计采用OOB（Out of Bag）的方法。</p>
<h3 id="Bagging方法"><a href="#Bagging方法" class="headerlink" title="Bagging方法"></a>Bagging方法</h3><p>&emsp;&emsp;Bagging方法就是将所有training data放进一个“黑色”的bag中，然后从这个bag中随机抽取部分数据生成新的训练集。随机森林算法中，样本训练集本省可以使用bagging方法，同样，样本的feature也可以进行bagging。从随机性来看，bagging技术可以有效的减小方差，即减小过拟合程度<br>&emsp;&emsp;随机森林是一种经典而强大的机器学习算法，具有回归和分类的功能。随机森林算法由若干决策树组成，这些决策树一般采用随机的方法生成，因此也叫做随机决策树。随机森林算法中的各决策树之间是没有关联的。</p>
<h3 id="Bootstrap抽样"><a href="#Bootstrap抽样" class="headerlink" title="Bootstrap抽样"></a>Bootstrap抽样</h3><p>&emsp;&emsp;随机森林算法中包含了对输入数据的重复自抽样过程，即所谓的bootstrap抽样。大约三分之一的数据集将用于测试而不是模型的训练，这样的数据被称为out of bag samples。<br>&emsp;&emsp;bootstrap抽样与bagging的区别是：在生成每棵树的时候，每个节点变量都仅仅在随机选出的少数变量中产生。因此，不但样本是随机的，连每个节点变量（Features）的产生都是随机的。<br>&emsp;&emsp;综上可知，随机森林算法的的两个随机采样的过程保证了随机性，所以即使对最终的各决策树不剪枝，也不会出现over-fitting。</p>
<h3 id="随机采样"><a href="#随机采样" class="headerlink" title="随机采样"></a>随机采样</h3><p>&emsp;&emsp;随机森林的随机体现在两个方面：数据集的<strong>行随机采样</strong>和<strong>列随机采样</strong>。行随机采样采用<strong>Bootstraping</strong>的方法对训练数据集进行又放回抽样。列随机采样指的是从所有$M$个特征属性中随机选择$m$个属性，同时保证$m &lt;&lt; M$。相当于把$M$维特征做了低维度的投影。减小特征选择的数量$m$，决策树之间的相关性和决策树的分类能力也会相应的降低；增大m，决策树之间的相关性和决策树的分类能力也会随之增大。<br>&emsp;&emsp;随机采样的后果是生成的每一个决策树的分类能力都很有限，但是将这些性能有限的决策树组合在一起的模型的性能就大大加强了。随机森林的最后结果是由多个决策树的训练结果进行加权组合而得到。</p>
<h3 id="完全分裂"><a href="#完全分裂" class="headerlink" title="完全分裂"></a>完全分裂</h3><p>&emsp;&emsp;随机森林建立决策树的过程不考虑决策树是否会发生过拟合。也就是说，随机森林对采样之后的数据采用完全分裂的方式建立决策树，因此，决策树的某一个叶子结点要么是无法继续分裂的，要么对应的所有样本都属于同一类。再次强调，<strong>随机森林中的两个“随机采样”充分保证了样本的随机性，因此即使不剪枝，也不会出现过拟合</strong>。</p>
<h3 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h3><ol>
<li>通过自助法（bootstrap）重采样技术，从原始训练样本集中有放回地重复随机抽取N个样本生成新的训练样本集合</li>
<li>对随机采样得到的新训练数据集，构建决策树，在每个节点执行以下操作：</li>
</ol>
<ul>
<li>从样本数据的M个features中随机选取m($m&lt;&lt;M$)个feature</li>
<li>对这m个features，选择特定的度量准则分割节点</li>
<li>重复上述操作N次，从而生成与样本数量相等的决策树</li>
</ul>
<ol>
<li>对于每一个测试样例，对k颗决策树的预测结果进行投票。票数最多的结果就是随机森林的分裂（预测）结果</li>
</ol>
<h3 id="特征重要性度量"><a href="#特征重要性度量" class="headerlink" title="特征重要性度量"></a>特征重要性度量</h3><p>&emsp;&emsp;随机森林能够计算特征的重要性，因此随机森林可用于特征选择。<br>&emsp;&emsp;随机森林算法中特征$X$的重要性的计算方法如下：</p>
<ol>
<li>对于随机森林中的每一棵决策树，输入都是经bootstrap采样的随机采样数据，使用OOB数据计算当前决策树模型的袋外错误率，记为$errorOOB1$</li>
<li>对每棵决策树的OOB数据的特征$X$加入噪声干扰，再次计算OOB错误率，记为$errorOOB2$</li>
<li>特征$X$的重要性可通过以下公式来衡量：<script type="math/tex; mode=display">X\_importacne = \frac{1}{N}\sum_{i=1}^{N}(errorOOB2 - errorOOB1)</script>&emsp;&emsp;不难理解，对于同一棵决策树来说，如果对特征$X$加入噪声干扰后，$errorOOB2$如果大幅度降低，说明该特征对样本的分类结果影响很大，即特征更重要。</li>
</ol>
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><p>&emsp;&emsp;随机森林算法有以下优点：</p>
<ul>
<li>属于集成学习模型，适合多分类模型，训练和预测速度快，在数据集上表现良好</li>
<li>对训练数据的容错能力强，即使训练数据中有缺失值，仍然可以保持预测精度不变</li>
<li>随机森林在分类的过程中，对generlization error使用的是内部无偏估计</li>
<li>能够在训练过程中检测到特征之间的相互影响以及各个特征的重要性程度</li>
<li>随机采样和特征属性采样保证了算法不易出现过度拟合</li>
<li>算法训练速度快，方便并行实现</li>
<li>适合用于多分类问题，算法训练和预测速度快，容易实现并行化</li>
<li>可有效估计缺失数据，即有一定程度的数据容错能力，当数据集中有大比例的数据缺失时仍然可以保持精度不变和能够有效地处理大的数据集</li>
<li>不会出现过拟合</li>
<li>能够处理很高维度（feature很多）的数据，并且不用做特征选择</li>
<li>对数据集的适应能力强：既能处理离散型数据，也能处理连续型数据，数据集无需规范化</li>
<li>可检测样本的各特征（维度）之间的相互影响程度，判断特征对所解决问题的重要性程度</li>
<li>可直接处理大规模的的变量（群）</li>
<li>在创建森林即分类的过程中，对泛化误差的估计是内部无偏估计</li>
<li>随机森林算法也是一种数据降维的方法，用于处理缺失值、异常值</li>
<li><h3 id="scikit-learn中的Random-Forest算法"><a href="#scikit-learn中的Random-Forest算法" class="headerlink" title="scikit-learn中的Random Forest算法"></a>scikit-learn中的Random Forest算法</h3>参考代码如下：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sklearn.ensemble.RandomForestClassfier</div><div class="line">clf = RandomForestClassifier([parameters]) <span class="comment">#generator the entity object of classifier</span></div><div class="line">parameters:</div><div class="line">n_estimators：指定随机森林中树的数目，越多越好，不超过内存即可</div><div class="line">criterion:指定在分裂使用的决策算法，取值有“entropy”、“gini”等</div><div class="line">max_features:单个决策树使用特征的最大数量，取值为<span class="string">"Auto"</span>，<span class="string">"None"</span>，<span class="string">"sqrt"</span>，<span class="string">"0.X"</span>。回归问题，max_features=n_features,分类问题，max_features=sqrt(n_features),<span class="string">"sqrt"</span>即为全部特征数目的平均根</div><div class="line">max_depth:默认为<span class="keyword">None</span>，一般可不改动</div><div class="line">min_simples_split:</div><div class="line">min_samples_leaf:最小叶片大小。默认值为<span class="number">1</span>，可设置为<span class="number">50</span>。叶是决策树的末端节点，较小的叶子使模型更容易捕捉训练数据中的噪声。</div><div class="line">min_weight_fraction_leaf:</div><div class="line">max_leaf_nodes:</div><div class="line">min_impurity_split:</div><div class="line">bootstrap:</div><div class="line">oob_score:这是一个随机森林交叉验证方法，取值为boolean类型，<span class="string">"True"</span>,<span class="string">"False"</span></div><div class="line">n_jobs:指定并行训练时使用的进程数。“<span class="number">-1</span>”表示使用所有处理器</div><div class="line">random_state:经验值<span class="string">"random_state=50"</span></div><div class="line">verbose:</div><div class="line">warm_state:</div><div class="line">class_weight:</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;随机森林算法学习笔记总结。&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>机器学习算法笔记——GBDT &amp; XGBoost</title>
    <link href="http://yaodong.ml/machine-learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94GBDT-XGBoost.html"/>
    <id>http://yaodong.ml/machine-learning/机器学习算法笔记——GBDT-XGBoost.html</id>
    <published>2017-02-13T05:27:59.000Z</published>
    <updated>2017-04-13T05:30:30.932Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;<strong>GBDT</strong>是一个应用很广泛的算法，可以用于解决分类、回归等问题。GBDT算法也有其他的名字，如MART(Multiple Additive Regression Tree)，GBRT(Gradient Boost Regression Tree)，Tree Net等。<br><a id="more"></a><br>&emsp;&emsp;GBDT的全称是Gradient Boosting Decision Tree，即梯度提升决策树。GBDT属于集成学习模型，通过将多个Decision Tree的决策结果迭代累加起来，作为最终的预测输出，进而达到提升模型性能的目的。<br>&emsp;&emsp;需要注意的是，GBDT算法过程中的树都属于回归树，不是分类树。GBDT的核心在于，每一棵子树学习的是之前所有树的预测结果与实际观测值之间的残差。Gradient Boosting就是拟合Loss function的梯度，将其作为新的弱回归树加入到总的算法中即可。</p>
<h3 id="Gradient-Boosting"><a href="#Gradient-Boosting" class="headerlink" title="Gradient Boosting"></a>Gradient Boosting</h3><p>&emsp;&emsp;首先介绍一下Gradient Boosting的基本概念。Gradient Boosting是一个框架，基于梯度提升算法的学习器叫做<strong>GBM</strong>（Gradient Boosting Machine）。<br>&emsp;&emsp;《统计学习方法》中提到，提升树算法利用加法模型和前向分步算法实现模型的优化过程。但是实际的回归问题中，模型的代价函数（Cost Function）可能不是特殊形式的平方损失函数或指数损失函数等。对于一般形式的损失函数而言，boosting算法对Cost Function的求解和优化会变得比较复杂。Gradient Boosting正是在这样的问题背景下产生，Gradient Boosting的核心思想是迭代生成$K$个具有弱泛化能力的学习器，然后将$K$个弱学习器的预测结果相加，得到最后的预测结果。Gradient Boosting框架中，后续的基学习器$f_{k+1}(x)$都是在前面的基学习器$f_k(x)$的训练效果上得到的，即</p>
<script type="math/tex; mode=display">f_{m+1}(x)=f_m(x)+h(x)</script><p>&emsp;&emsp;Gradient Boosting算法的流程如下：给定样本数据集$T={(x^{(1)},y^{(1)}),…,(x^{(i)},y^{(i)}),…,(x^{(n)},y^{(n)})}$，$x^{(i)}\in R^n$，$y(i)\in {-1,+1}$，设基学习器的数量为$M$，第$m$个基学习器的损失函数为$L(y,f_m(x))$。</p>
<ol>
<li>初始化第一课子树，估计使损失函数最小化的常数值：<script type="math/tex; mode=display">f_0(x)=\arg\min_\gamma\sum_{i=1}^{n}L(y^{(i)},\gamma)</script></li>
<li>对$m=1,2,…,M$:<br><strong>1)</strong> 对i=1,2,…,n，计算当前损失函数的负梯度在当前模型的值，作为当前模型的残差估计：<script type="math/tex; mode=display">r_{mi}=\left[\frac{\partial L(y^{(i)},f(x^{(i)}))}{\partial f(x^{(i)})}\right]_{f(x)=f_{m-1}(x)}</script><strong>2)</strong> 根据${x^{(i)},r_{mi}}$拟合回归树，生成基学习器$h_m(x)$。<br><strong>3)</strong> 计算最优的叶节点区域估计值$\gamma_m$，使损失函数最小化<script type="math/tex; mode=display">\gamma_{m}=\arg\min_\gamma\sum_{i=1}^nL(y^{(i)},f_{k-1}(x^{(i)})+\gamma h_m(x^{(i)}))</script><strong>4)</strong> 更新第$m$个子回归树模型<script type="math/tex; mode=display">f_m(x)=f_{m-1}(x)+\gamma_m h_m(x^{(i)})</script></li>
<li>组合$m$个基学习器，得到最终的模型<script type="math/tex; mode=display">\hat f(x)=\sum_{m=0}^{M}\gamma_m f(x)</script></li>
</ol>
<h4 id="GBDT算法理论推导"><a href="#GBDT算法理论推导" class="headerlink" title="GBDT算法理论推导"></a>GBDT算法理论推导</h4><p>&emsp;&emsp;Gadient Boosting Decision Tree是Gradient Boosting框架下的Decision Tree版本，基本思想是新模型是在前一个模型的损失函数的梯度下降方向建立。下面进行算法推导：<br>&emsp;&emsp;假设模型的参数空间为$P$，$P={p_1,p_2,…}$，$F(x;P)$表示以$P$为参数的关于$x$的函数，也就是GBDT的预测函数。GBDT算法属于前向加法模型，用$\beta$表示每个学习模型的权重，$\alpha$表示每个学习模型的内部参数，则有</p>
<script type="math/tex; mode=display">F(x;P)=\sum_{i=1}^{M}\beta_{m}h(x;\alpha_{m})</script><p>&emsp;&emsp;GBDT算法通过优化参数空间${\alpha,\beta}$来对最终的预测函数$F(x;P)$进行优化。在给定损失函数和训练数据的前提下，对$F(x;P)$的学习即为GBDT的损失函数极小化问题，设模型$F(x;P)$的Loss Function为$\Phi(P)$，则有</p>
<script type="math/tex; mode=display">P^{*}=\arg\min_{F}\Phi(P)</script><script type="math/tex; mode=display">F^*(x;P)=\arg\min_{F}\Phi(P)</script><p>&emsp;&emsp;模型的Loss Function为$\Phi(P)$</p>
<script type="math/tex; mode=display">\Phi(P)=E_{y,x}L(y,F(x;P))=E_{x}[E_{y}(L(y,F(x)))\mid \boldsymbol{x}]</script><p>&emsp;&emsp;基于模型的可加性，有如下推导关系</p>
<script type="math/tex; mode=display">F^{*}(\boldsymbol{x})=\sum_{i=1}^{M}f(\boldsymbol{x})</script><script type="math/tex; mode=display">P^{*}=\sum_{i=1}^{M}p_{m}</script><p>&emsp;&emsp;因此，对损失函数$\Phi(P)$在当前模型$F_{m-1}(\boldsymbol{x})$下求偏导，得到梯度：</p>
<script type="math/tex; mode=display">g_{m}(\boldsymbol{x})=\big{[}\frac{\partial{\Phi(F(\boldsymbol{x}))}}{\partial{F(\boldsymbol{x}})}\big{]}_{F(\boldsymbol{x})=F_{m-1}(\boldsymbol{x})}</script><p>&emsp;&emsp;$F_{m-1}(\boldsymbol{x})$表示前$m-1$个基学习模型的累加：</p>
<script type="math/tex; mode=display">F_{m-1}(\boldsymbol{x})=\sum_{i=1}^{m-1}f_{i}(\boldsymbol{x})</script><p>&emsp;&emsp;第$m$个基学习模型的预测函数$f_{m}(\boldsymbol{x})$的计算公式如下：</p>
<script type="math/tex; mode=display">f_{m}(\boldsymbol{x})=-\rho_{m}g_{m}(\boldsymbol{x})</script><p>&emsp;&emsp;$\rho_{m}$表示第$m$个基学习模型在当前梯度方向上下降的距离，根据损失函数与当前模型的梯度方向可求得：</p>
<script type="math/tex; mode=display">\rho_{m}=\arg\min_{\rho}\big{[}\Phi(P_{m-1}-\rho g_{m}(x))\big{]}</script><script type="math/tex; mode=display">\rho_{m}=\arg\min_{\rho}\big{[}E_{y,x}L(y,F_{m-1}(\boldsymbol{x}))-\rho g_{m}(x)\big{]}</script><p>&emsp;&emsp;GBDT的基本思路是，先训练一个回归树，并计算各样本的目标值与预测值之间的残差，将这个残差作为目标值，训练下一个回归树，继续求残差，直到回归树的数目达到指定值或残差达到允许范围，停止学习。</p>
<p>&emsp;&emsp;回忆一下原始的Boosting算法：算法初始状态下，为每一个样本赋上大小相等的权重值。在之后迭代的每一步中增加被误分类的样本的权重，减少正确分类的样本的权重，使得被误分类的样本被赋上一个很高的权重。进行M次迭代训练，将得到$M$个基学习器，最后将这$M$个根据指定的策略组合起来，即得到最终的模型。</p>
<p>&emsp;&emsp;<strong>Gradient Boost与传统的Boosting的区别</strong></p>
<ol>
<li>Gradient Boot算法在迭代的每一步都是建立一个能使损失函数沿着负梯度方向降低的学习器来弥补之前的学习器的不足。经典的AdaBoosting算法只能处理采用指数损失函数的二分类问题，但由于Gradient Boosting框架可以设置不同的可微损失函数，因此Gradient Boosting算法可以应用到不同的学习任务中；</li>
<li>AdaBoosting算法对异常点（outlier）比较敏感，但是Gradient Boosting算法可以通过引入Bagging思想、加入正则化项等方法抵御训练数据中的噪声，具有更好的健壮性。</li>
</ol>
<p>每次的计算是为了减小上一次的残差（residual）。为了消除残差，可以取损失函数的负梯度在当前模型上的取值作为当前残差的近似值，并拟合下一个子树模型。所以说，在Gradient Boost中，每个新子树模型的建立会使之前模型的残差往梯度方向减少，与传统Boosting算法对正确、错误的样本进行加权有着很大的区别。</p>
<hr>
<h3 id="梯度上升决策树算法（Gradient-Boosting-Decision-Tree，GBDT）"><a href="#梯度上升决策树算法（Gradient-Boosting-Decision-Tree，GBDT）" class="headerlink" title="梯度上升决策树算法（Gradient Boosting Decision Tree，GBDT）"></a>梯度上升决策树算法（Gradient Boosting Decision Tree，GBDT）</h3><blockquote>
<p><strong>GBDT</strong>是一个应用很广泛的算法，可以用于解决分类、回归等问题。GBDT算法也有其他的名字，如MART(Multiple Additive Regression Tree)，GBRT(Gradient Boost Regression Tree)，Tree Net等。</p>
</blockquote>
<p>&emsp;&emsp;首先介绍一下Gradient Boosting的基本概念。Gradient Boost其实是一个框架，可以套入很多不同的算法以对穿传统的机器学习算法进行改进。当损失函数不是平方损失函数或者指数损失函数，而是一般函数时，boost算法对损失函数的求解和优化会变得比较复杂，针对这一问题，学者提出了Gradient Boosting算法。核心思想是利用损失函数的负梯度在当前模型下的取值作为残差的近似，进一步训练得到下一棵回归树。<br>&emsp;&emsp;回忆一下原始的Boosting算法：算法初始状态下，为每一个样本赋上大小相等的权重值。在之后迭代的每一步中增加被误分类的样本的权重，减少正确分类的样本的权重，使得被误分类的样本被赋上一个很高的权重。进行M次迭代训练，将得到M个基学习器，最后将它们根据指定的策略组合起来，得到一个最终的模型。<br>&emsp;&emsp;GBDT中的树都是回归树，不是分类树，但是对GBDT模型调整后可以用于分类问题。GBDT的核心在于，每一棵子树学习的是之前所有树的预测结果与实际观测值之间的残差。Gradient Boosting就是拟合Loss function的梯度，将其作为新的弱回归树加入到总的算法中即可。<br>&emsp;&emsp;Gradient Boost与传统的Boost的区别是，Gradient Boot算法每次的计算是为了减小上一次的残差（residual）。为了消除残差，可以取损失函数的负梯度在当前模型上的取值作为当前残差的近似值，并拟合下一个子树模型。所以说，在Gradient Boost中，每个新子树模型的建立会使之前模型的残差往梯度方向减少，与传统Boosting算法对正确、错误的样本进行加权有着很大的区别。</p>
<p>&emsp;&emsp;GBDT的算法流程如下：给定样本数据集$T={(x^{(1)},y^{(1)}),…,(x^{(i)},y^{(i)}),…,(x^{(m)},y^{(m)})}$，$x^{(i)}\in R^n$，$y(i)\in {-1,+1}$，损失函数为$L(y,f(x))$，设生成的子树的数目是K。</p>
<ol>
<li>初始化第一课子树，估计使损失函数最小化的常数值，建立只有一个根结点的树：<script type="math/tex; mode=display">f_0(x)=\arg\min_c\sum_{i=1}^{m}L(y^{(i)},c)</script></li>
<li>对k=1,2,…,K:<br><strong>1)</strong> 对i=1,2,…,m，计算当前损失函数的负梯度在当前模型的值，作为模型残差的估计：<script type="math/tex; mode=display">g_{ki}=\left[\frac{\partial L(y^{(i)},f(x^{(i)}))}{\partial f(x^{(i)})}\right]_{f(x)=f_{k-1}(x)}</script><strong>2)</strong> 根据$r<em>{ki}$拟合一个回归树，作为下一个子树模型，估计回归树的叶节点对区域$R</em>{kj}$,$j=1,2,…,J$，以拟合残差的近似值：<br><strong>3)</strong> 利用线性搜索估计叶节点区域的值，使损失函数最小化。对$j=1,2,…,J$，计算<script type="math/tex; mode=display">c_{mj}=\arg\min_c\sum_{x_i \in R_{kj}}L(y^{(i)},f_{k-1}(x^{(i)})+c)</script><strong>4)</strong> 更新第k个子回归树模型<script type="math/tex; mode=display">f_k(x)=f_{k-1}(x)+\sum_{j=1}^Jc_{kj}I(x\in R_{kj})</script></li>
<li>得到回归树<script type="math/tex; mode=display">\hat f(x)=f_{K}(x)=\sum_{k=1}^{K}\sum_{j=1}^Jc_{kj}I(x\in R_{kj})</script></li>
</ol>
<h4 id="GBDT的优缺点"><a href="#GBDT的优缺点" class="headerlink" title="GBDT的优缺点"></a>GBDT的优缺点</h4><ul>
<li><strong>优点</strong>：GBDT中的非线性变化很多，因此模型的表达能力强，不需要做复杂的特征工程和特征变换。</li>
<li><strong>缺点</strong>：GBDT中的Boosting迭代过程是一个串行工程，不利于并行实现；此外，GBDT的计算复杂度很高。<h4 id="GBDT的参数调优"><a href="#GBDT的参数调优" class="headerlink" title="GBDT的参数调优"></a>GBDT的参数调优</h4>&emsp;&emsp;GBDT的常用参数如下：</li>
<li>子树的棵数，即基学习器的数量</li>
<li>树的深度</li>
<li>缩放因子</li>
<li>损失函数</li>
<li>数据采样比</li>
<li>特征采样比</li>
</ul>
<h3 id="Xgboost"><a href="#Xgboost" class="headerlink" title="Xgboost"></a>Xgboost</h3><p>&emsp;&emsp;XGBoost的全称是eXtreme Gradient Boosting，是Gradient Boosting算法的高效实现。XGBoost中的基学习器除了可以是CART也可以是其他线性分类器。<br>&emsp;&emsp;XGBoost的目标函数中显式的加入正则化项，当基学习器为CART时，正则化项与树的叶子结点的数量$T$、叶子结点的取值有关。<br>&emsp;&emsp;待整理。</p>
<h3 id="Xgboost与GBDT的区别："><a href="#Xgboost与GBDT的区别：" class="headerlink" title="Xgboost与GBDT的区别："></a>Xgboost与GBDT的区别：</h3><ol>
<li>传统的GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。</li>
<li>关于优化求解问题：传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。</li>
<li>xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性</li>
<li>列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。</li>
<li>对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。</li>
<li>xgboost工具支持并行。<h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3></li>
<li><a href="http://blog.csdn.net/puqutogether/article/details/41957089" target="_blank" rel="external">http://blog.csdn.net/puqutogether/article/details/41957089</a></li>
<li><a href="http://blog.csdn.net/china1000/article/details/51106856" target="_blank" rel="external">http://blog.csdn.net/china1000/article/details/51106856</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;&lt;strong&gt;GBDT&lt;/strong&gt;是一个应用很广泛的算法，可以用于解决分类、回归等问题。GBDT算法也有其他的名字，如MART(Multiple Additive Regression Tree)，GBRT(Gradient Boost Regression Tree)，Tree Net等。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Machine Learning" scheme="http://yaodong.ml/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习算法笔记——概率图模型（Probabilistic Graphical Model）</title>
    <link href="http://yaodong.ml/machine-learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B%EF%BC%88Probabilistic-Graphical-Model%EF%BC%89%EF%BC%89.html"/>
    <id>http://yaodong.ml/machine-learning/机器学习算法笔记——概率图模型（Probabilistic-Graphical-Model））.html</id>
    <published>2017-02-13T05:26:41.000Z</published>
    <updated>2017-04-13T05:30:05.049Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;概率图模型（Probabilistic Graphical Model）学习笔记总结。<br><a id="more"></a><br>&emsp;&emsp;常见的机器学习算法模型一般分为两类：判别模型和生成模型，二者都属于概率模型。机器学习、深度学习的实质是根据给定的训练样本数据来对位置变量进行估计和预测，概率模型将模型的学习任务归结于计算未知变量的概率分布。概率模型中，主要任务是基于可观测变量，即已知的样本数据集，去推测未知变量的条件分布，这称为“推断”（Inference）。<br>当模型中含有隐变量时，假定目标变量集合为$Y$，可观测变量集合为$X$，隐变量集合为$Z$，生成模型考虑模型的联合分布概率$P(X,Z,Y)$，判别式模型考虑条件概率分布$P(Y,Z|X)$。给定具体的样本数据集，推断就是要由$P(X,Z,Y)$或$P(Y,Z|X)$得到条件概率分布$P(Y|X)$，进而对目标变量进行估计和预测。<br>&emsp;&emsp;当模型中含有隐变量时，概率模型的求解往往比较困难。概率图模型很好地解决了这个问题。</p>
<h3 id="概率图模型"><a href="#概率图模型" class="headerlink" title="概率图模型"></a>概率图模型</h3><p>&emsp;&emsp;概率图模型主要分为两类，第一类是使用有向图（无环）表示变量间的依赖关系，称为有向图模型或者<strong>贝叶斯网络</strong>（Bayes Network），第二类是使用无向图表示变量间的相关关系，称为无向图模型或者<strong>马尔科夫网</strong>（Markov Netowork）。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;概率图模型（Probabilistic Graphical Model）学习笔记总结。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Machine Learning" scheme="http://yaodong.ml/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习中的特征选择方法总结</title>
    <link href="http://yaodong.ml/machine-learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93.html"/>
    <id>http://yaodong.ml/machine-learning/机器学习中的特征选择方法总结.html</id>
    <published>2017-02-13T05:21:07.000Z</published>
    <updated>2017-04-13T05:24:38.487Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;本文整理了一些特征选择相关的文章，汇总而来。<br><a id="more"></a><br>&emsp;&emsp;特征工程在机器学习中占有重要的地位。“数据和特征决定了机器学习的上限，而模型和算法仅仅是为了更好地逼近这个上限”。特征工程主要可分为三大类：特征生成，特征选择、特征提取。<br>&emsp;&emsp;为什么要做特征选择？特征选择是指从完备数据集中选择能使相应模型和算法获得最好性能的特征集和的过程。通过特征选择，我们希望提高模型预测的准确性，构造复杂度更低的算法模型，使数据对模型有更好的理解和解释。本文简单总结了比赛和工业应用中常用的特征选择方法。<br>&emsp;&emsp;常用的特征选择的方法有以下几种：<strong>Filtering methods</strong>，<strong>Wrapping methods</strong>，<strong>Embeded methods</strong></p>
<h3 id="Filtering-methods（刷选器）"><a href="#Filtering-methods（刷选器）" class="headerlink" title="Filtering methods（刷选器）"></a>Filtering methods（刷选器）</h3><p>&emsp;&emsp;filtering方法的主要思想是：根据特征对目标变量的响应，对每个特征进行打分（<strong>score</strong>），score的值代表特征的重要性，然后依据每个特征的score进行排序，进而选择特征。<br>&emsp;&emsp;根据<strong>Filtering</strong>的思想进行特征选择的方法有<strong>方差选择法</strong>，<strong>卡方检验（Chi-squared test）</strong>、<strong>信息增益（information gain）</strong>，<strong>相关系数（correlation coefficient）</strong> 。<br>&emsp;&emsp;总结来说，<strong>Filtering methods</strong>侧重于单个特征跟目标变量的相关性。<strong>Filtering methods</strong>不考虑特征之间的相关性，因此倾向于选择冗余的特征。</p>
<h3 id="Wrapper-methods（封装器）"><a href="#Wrapper-methods（封装器）" class="headerlink" title="Wrapper methods（封装器）"></a>Wrapper methods（封装器）</h3><p>&emsp;&emsp;<strong>Wrapper methods</strong>的主要思想是：将特征选择看作是对全部特征的最优子集搜索问题。根据训练数据集生成不同的特征子集，封装器用选定的的特征子集对训练数据进行分类或回归，模型的分类或预测精度作为特征选择的衡量标准，经过比较不同特征子集对应的模型性能，选出表现最好的特征子集。常用的wrapper methods有逐步回归（Stepwise regression）、向前选择（Forward selection）和向后选择（Backward selection）。<br>&emsp;&emsp;Wrapper methods的优点是考虑了特征与特征之间的关联性；缺点是当训练数据规模较小时容易发生过拟合。</p>
<h4 id="递归特征消除算法（Recursive-Feature-Elimination，RFE）"><a href="#递归特征消除算法（Recursive-Feature-Elimination，RFE）" class="headerlink" title="递归特征消除算法（Recursive Feature Elimination，RFE）"></a>递归特征消除算法（Recursive Feature Elimination，RFE）</h4><p>&emsp;&emsp;递归特征消除算法的主要思想是反复的构建算法模型（如SVM、回归模型），然后从中选择最好的（或者最差的）的特征；将被选择的特征从候选特征集中删除，然后在新的特征子集上重复该过程，直到遍历所有特征为止。这个过程中特征被消除的次序就是特征的排序。因此，递归特征消除算法是一种寻找最优特征子集的贪心算法。<br>&emsp;&emsp;Scikit-learn提供了<code>RFE package</code>，可用于特征选择，还提供了<code>RFECV  packge</code>，可以通过交叉验证来对特征进行排序。</p>
<h4 id="基于Regularization做特征选择"><a href="#基于Regularization做特征选择" class="headerlink" title="基于Regularization做特征选择"></a><strong>基于Regularization做特征选择</strong></h4><p>&emsp;&emsp;L1正则方法具有稀疏的特征，即很多特征对应的参数会为零，因此可以训练一个Lasso模型，然后将系数为零的特征去除。需要注意的是，Lasso没有选到的特征不代表不重要，原因是两个具有高相关性的特征可能只保留了一个。<br>&emsp;&emsp;在实际的工作中，Lasso的参数$\lambda$越大，解的稀疏性越强，选定的特征数目越少。如何确定使用多大的$\lambda$？对于一系列$\lambda$，用交叉验证计算模型的$rmse$，然后选择$rmse$的极小值对应的$\lambda$。</p>
<h4 id="基于树模型的特征选择方法"><a href="#基于树模型的特征选择方法" class="headerlink" title="基于树模型的特征选择方法"></a><strong>基于树模型的特征选择方法</strong></h4><p>&emsp;&emsp;可以利用决策树、随机森林、GBDT求解每个feature的importances。以Random Forest为例，主要思想是训练一系列不同的决策树模型，每个决策树的训练数据是对原始数据进行bootstrap自助采样得到的特征子集。</p>
<h3 id="Embeded-methods"><a href="#Embeded-methods" class="headerlink" title="Embeded methods"></a>Embeded methods</h3><p>&emsp;&emsp;Embeded method的主要思想是：在模型既定的情况下学习出对提高模型准确性最好的属性。学习器自身自主选择特征，如使用Regularization做特征选择，或者使用决策树思想，细节这里就不做介绍了。这里还提一下，在做实验的时候，我们有时候会用Random Forest和Gradient boosting做特征选择，本质上都是基于决策树来做的特征选择，只是细节上有些区别。Embeded methods中最常见的是利用正则化项做Feature Selection。</p>
<h3 id="降维算法"><a href="#降维算法" class="headerlink" title="降维算法"></a>降维算法</h3><p>&emsp;&emsp;常见的降维方法除了基于L1惩罚项的模型以外，还有主成分分析算法（PCA）和线性判别分析（LDA）。PCA和LDA的本质都是将原始的样本数据映射到低维度的样本空间中，但是PCA和LDA的映射目标不一样：PCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的分类性能。所以说PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。</p>
<h3 id="Deep-Learning"><a href="#Deep-Learning" class="headerlink" title="Deep Learning"></a>Deep Learning</h3><p>&emsp;&emsp;从数据的角度来理解特征选择，寻找一个更好的空间来描述数据，把原始数据对象映射到这个空间去表达。随着深度学习浪潮的到来，通过深度学习来进行特征选择已经成为一种有效的特征选择方法，尤其是在计算机视觉领域。原因不难理解，Deep Learning具有从原始数据中自动学习特征的能力。这也是Deep Learning又被称为Unsupervised Learning的原因。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;本文整理了一些特征选择相关的文章，汇总而来。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Machine Learning" scheme="http://yaodong.ml/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习算法笔记——类别不均衡问题</title>
    <link href="http://yaodong.ml/machine-learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E7%B1%BB%E5%88%AB%E4%B8%8D%E5%9D%87%E8%A1%A1%E9%97%AE%E9%A2%98.html"/>
    <id>http://yaodong.ml/machine-learning/机器学习算法笔记——类别不均衡问题.html</id>
    <published>2017-01-22T06:24:56.000Z</published>
    <updated>2017-03-21T06:41:46.696Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;类别不均衡（class-imbalance）指的是机器学习的分类任务中不同类别的训练样本样例数目差别很大的情况。为了使算法模型达到更好的分类效果，有必要解决该问题。<br><a id="more"></a><br>&emsp;&emsp;在现实中有很多类别不均衡问题，它是常见的，并且也是合理的，符合人们期望的。例如在欺诈交易识别中，属于欺诈交易的应该是很少部分，即绝大部分交易是正常的，只有极少部分的交易属于欺诈交易。一般来说，如果类别不平衡比例超过4:1，那么分类模型极有可能因为数据类别不平衡而无法达到很好地分类性能。因此在构建分类模型之前，需要对类别不均衡性问题进行处理。<br>&emsp;&emsp;以常见的二分类问题为例，设样本数据集中的正例样本数目为$m^-$ ,反例样本数目为$m^+$。</p>
<h3 id="扩大数据集"><a href="#扩大数据集" class="headerlink" title="扩大数据集"></a>扩大数据集</h3><p>&emsp;&emsp;更多的数据往往战胜更好的算法。遇到类别不均衡问题时，首先应该想到：是否可以增加数据集的规模，因为更大规模的数据意味着更多的信息。</p>
<h3 id="采样（随机采样）"><a href="#采样（随机采样）" class="headerlink" title="采样（随机采样）"></a>采样（随机采样）</h3><p>&emsp;&emsp;可以使用采样（Sampling）策略来改善样本数据集的类别不均衡的成都。主要有以下两种采样方法来降低数据集的样本类别不均衡性：</p>
<ol>
<li>对样本数目较少的一类样本进行采样，增加该类样本的数目，进而提高该类样本在样本数据集中占的比例，这种采样方法称为<strong>过采样（oversampling），也叫上采样</strong>。需要注意的是，过采样法不能简单地对样本数目较少的类别样本进行直接重复采样，否则会招致严重的过拟合。为了解决这一问题，可以在每次生成新数据样本时加入轻微的随机扰动，经验表明这种方法可以有效提高模型的泛化性能。例如，过采样方法的代表性算法——SMOTE算法，就是通过对训练集的正例进行插值来产生额外的正例。</li>
<li>对样本数目较多的一类样本进行采样，即去除一些样本数目较多的类的样本，进而降低该类样本在样本数据集中所占的比例，使采样之后的数据集中正、反类样本数目接近，这种采样方法称为<strong>欠采样（undersampling），也叫下采样</strong>。<br>欠采样的代表性算法是集成学习（Ensemble Learning）机制，将样本数目较多的类别样本划分为多个集合供不同“弱分类器”使用，这样相当于对每个学习器都进行了欠采样，但在全局看却没有丢失过多的信息。</li>
</ol>
<p>&emsp;&emsp;随机采样最大的优点是简单易操作，但缺点也很明显：上采样后的数据集中会反复出现一些重复样本，训练出来的模型会倾向于过拟合；而下采样会导致丢失部分数据，使得算法模型不能完全学习到数据的总体模式。显然，下采样过程会丢失信息，如何解决这个问题？<br>&emsp;&emsp;<strong>Informed undersampling</strong>采样技术可以解决随机欠采样造成的数据信息丢失问题，<strong>informed undersampling</strong>采样技术主要有两种方法分别是<strong>EasyEnsemble</strong>算法和<strong>BalanceCascade</strong>算法。<br>&emsp;&emsp;EasyEnsemble算法,类似于Ensemble learning的Bagging方法，它把数据集$D$划分为两部分，分别是多数类别的样本和少数类别的样本，对于多数类样本$D<em>{max}$，通过$n$次有放回抽样生成$n$份子集，少数类样本分别和这$n$份样本合并训练一个模型，这样可以得到$n$个模型，最终的模型是这$n$个模型预测结果的平均值。<br>&emsp;&emsp;BalanceCascade算法是一种级联算法，该算法从多数类别样本集$S</em>{min}$中有效地选择N且满足$\mid N\mid=\mid D<em>{min}\mid$，将N和$D</em>{min}$合并为新的数据集进行训练，利用Boosting的思想，首先对原始数据集进行一次下采样，生成新的子训练集，使用该数据集合训练一个分类器，对于那些分类正确的样本不放回，然后再下采样，再用新生成的数据集训练下一个分类器，一直迭代进行，最后组合所有分类的结果作为最后的结果。</p>
<h3 id="生成新数据"><a href="#生成新数据" class="headerlink" title="生成新数据"></a>生成新数据</h3><p>&emsp;&emsp;SMOTH算法，构造人工数据样本。SMOTE是一种过采样算法，它构造新的小类样本而不是产生小类中已有的样本的副本，即该算法构造的数据是新样本。SMOTE算法基于距离度量选择小类别下的两个或者更多的相似样本，然后选择其中一个样本，并随机选择一定数量的近邻样本对该样本的一个属性增加噪声，每次只处理一个属性。这样就构造了更多的新生数据样本。算法流程如下：<br>&emsp;&emsp;$Step 1$：随机选定少数类别中的一个样本$x^{(i)}$，以欧氏距离为标准计算它到少数类样本集$D_{min}$中所有样本的距离，得到与其距离度量最小的k个相似样本。<br>&emsp;&emsp;$Step 2$：根据类别不平衡比例设置一个采样比例阈值$T$以确定采样倍率$N$，对于每一个少数类样本$x$，从它的k个相邻样本中随机选择若干个样本，假设选择的近邻为$\hat{x}$。<br>&emsp;&emsp;$Step 3$：对于每一个随机选出的近邻$\hat{x}$，分别与原样本按照如下的公式构建新的样本：</p>
<script type="math/tex; mode=display">x_{new}=x+rand(0,1)\times(\hat{x}-x)</script><h3 id="类别不平衡问题的评价指标"><a href="#类别不平衡问题的评价指标" class="headerlink" title="类别不平衡问题的评价指标"></a>类别不平衡问题的评价指标</h3><script type="math/tex; mode=display">Precision = \frac{TP}{TP+FP}</script><script type="math/tex; mode=display">Recall=\frac {TP}{TP+FN}</script><script type="math/tex; mode=display">F-Measure=\frac {(1+\beta)^2\cdot Recall\cdot Precision}{\beta ^ 2\cdot Recall+Precision}</script><script type="math/tex; mode=display">G-Mean=\sqrt{\frac{TP}{TP+TN}\cdot {\frac {TN}{TN+FP}}}</script><h3 id="转换问题的思考角度"><a href="#转换问题的思考角度" class="headerlink" title="转换问题的思考角度"></a>转换问题的思考角度</h3><p>&emsp;&emsp;对于类别不平衡的二分类问题，可将其看做一分类问题（One Class Learning），或异常检测问题（Novelty Detection）。转换看待问题的角度后，解决问题的重点不再是通过捕捉不同类别样本之间的差别来分类，而是对其中一类样本进行建模，进而解决问题。</p>
<h3 id="尝试不同算法"><a href="#尝试不同算法" class="headerlink" title="尝试不同算法"></a>尝试不同算法</h3><p>&emsp;&emsp;对于分类问题，解决类别不均衡问题的另外一个思路是从算法的角度出发，考虑不同误分类情况代价的差异性对算法进行优化，使得我们的算法在不平衡数据下也能有较好的效果。<br>&emsp;&emsp;在算法层面上解决类别不平衡问题的方法主要是基于代价敏感学习算法（Cost-Sensitive Learning）。<br>&emsp;&emsp;代价敏感学习方法的核心要素是代价矩阵，实际生活中，不同类型的误分类情况导致的代价是不一样的，例如在医疗中，“将病人误疹为健康人”和“将健康人误疹为病人”的代价不同；在信用卡盗用检测中，“将盗用误认为正常使用”与“将正常使用识破认为盗用”的代价也不相同。对于二分类问题，定义代价矩阵。标记$C<em>{ij}$为将类别$j$误分类为类别$i$的代价，显然$C</em>{00}=C<em>{11}=0$，但$C</em>{01}$和$C_{10}$为两种不同的误分类代价，当两者相等时为代价不敏感的学习问题。二者不等时即为代价敏感学习问题。<br>&emsp;&emsp;基于代价矩阵$C$的分析，代价敏感学习方法主要有以下三种实现方式，分别是：</p>
<ol>
<li>从学习模型出发，着眼于对某一具体学习方法的改造，使之能适应不平衡数据下的学习，研究者们针对不同的学习模型如感知机，支持向量机，决策树，神经网络等分别提出了其代价敏感的版本。以代价敏感的决策树为例，可从以下方面对其进行改进以适应不平衡数据的学习，分别是决策阈值的选择、分裂标准的选择、剪枝，在上述三个步骤中都可以将代价矩阵引入，即可解决类别不平衡问题。</li>
<li>从贝叶斯风险理论出发，把代价敏感学习看成是分类结果的一种后处理，按照传统方法学习到一个模型，以实现损失最小为目标对结果进行调整。此方法的优点在于它可以不依赖所用具体的分类器，但是缺点也很明显它要求分类器输出值为概率。</li>
<li>从预处理的角度出发，将代价用于权重的调整，使得分类器满足代价敏感的特性。</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;类别不均衡（class-imbalance）指的是机器学习的分类任务中不同类别的训练样本样例数目差别很大的情况。为了使算法模型达到更好的分类效果，有必要解决该问题。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Machine Learning" scheme="http://yaodong.ml/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>深度学习与浅层神经网络的不同之处</title>
    <link href="http://yaodong.ml/deep-learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%B8%8D%E5%90%8C%E4%B9%8B%E5%A4%84.html"/>
    <id>http://yaodong.ml/deep-learning/深度学习与浅层神经网络的不同之处.html</id>
    <published>2017-01-21T06:36:03.000Z</published>
    <updated>2017-03-21T06:44:18.188Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;本文根据自己对深度学习的基本理解，简单总结了深度学习与浅层神经网络的不同之处。<br><a id="more"></a></p>
<blockquote>
<p>深度学习又称深度神经网络，即包含多个隐藏层的神经网络。由于每个隐藏层都可以对上一层的输出进行非线性变换，因此深度神经网络拥有比浅层前馈式网络更加优异的表达能力，因此可以计算更多更复杂的特征信息。</p>
</blockquote>
<p>&emsp;&emsp;1. <strong>数据集规模</strong>。深度神经网路的隐层数量和隐层神经元数目都远大于传统的浅层神经网络，相应地，深度学习模型的复杂度也远高于浅层网络。从数据集规模的角度考虑，使用大规模的数据集对深度学习模型进行训练，可在一定程度上降低深度神经网络陷入过拟合的风险。<br>&emsp;&emsp;2. <strong>网络的训练速度</strong>。对神经网络进行训练的实质是基于梯度下降思路求解整体代价函数最小化时对应的参数。神经网络的训练速度与网络的参数的数目有直接关系，深度学习模型的隐层数目和隐层神经元数目都远多于浅层神经网络，因此深度神经网络的训练速度远小于浅层神经网络，训练时间远大于浅层神经网络。<br>&emsp;&emsp;3. <strong>网络的训练方式</strong>。浅层神经网络中最为广泛应用的训练算法是基于梯度下降的误差逆传播算法，但是多隐层深度网络一般难以直接用标准BP算法进行训练和求解。因为训练误差在多隐层内逆向传播时，在每一层都要乘以该层的激活函数的导数值，也就是说，误差在每一层传递都会不断衰减。当网络层数很深时，梯度就会不停的衰减，甚至消失，使得整个网络很难训练。这就是所谓的梯度消失问题（Vanishing Gradient Problem），也叫梯度弥散问题。<br>&emsp;&emsp;目前解决“梯度弥散”问题的有效思路是使用ReLU函数作为每个隐层神经元的激活函数，因为相对于sigmoid函数和tanh函数，ReLU函数的导数值更大，这样不仅神经网络的误差可以很好的传播，而且由于ReLU函数的优良特性，神经网路的训练速度也有相应的提高。<br>&emsp;&emsp;4. <strong>深度神经网络的训练思路</strong>。无监督逐层贪婪训练是对多隐层网络进行训练的有效手段。基本思想是， 每次只训练网络的一个隐层，训练时把上一个隐层的输出作为输入，并且本层结点的输出作为下一个隐层的输入。，当前隐层训练结束后再开始训练下一个隐层，即在训练第k个隐藏层时都需要将前k-1层固定。这种训练思想称为“预训练”（pre-training）。预训练全部完成之后，再利用BP算法训练整个网络，即使用BP算法对整个网络进行“微调”。需要注意的是，每一层的预训练可以是有监督的（例如将每一层的分类误差作为目标函数），也可以是无监督的。当用无标签数据训练完网络后，相比于随机初始化而言，各层初始权重会位于参数空间中较好的位置上。然后我们可以从靠近最优值的位置出发，进一步微调权重。从经验上来说，以这些位置为起点开始进行梯度下降训练，更有可能收敛到比较好的局部极值点，这是因为无标签数据已经提供了大量输入数据中包含的模式的先验信息。<br>&emsp;&emsp;5. <strong>深度神经网络的“参数共享”机制</strong>。深度神经网络的参数规模较大，使用“参数共享”的机制能有效地节省训练开销，缩短训练时间。这个策略在卷积神经网络（Convolutional Neural Network，CNN）中已得到成功的应用。<br>&emsp;&emsp;6. <strong>强调了特征学习的重要性</strong>。浅层神经网络模型的特征工程主要靠人工构建特征工程，进而对样本进行分类或预测。Deep Learning含有多个隐层，对样本的特征属性通过逐层特征变换，将样本的原特征空间映射到新的特征空间，从而使分类或预测更加简单。与人工构造特征工程相比，Deep Learning利用大数据来学习特征，更能够刻画数据的丰富内在信息。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;本文根据自己对深度学习的基本理解，简单总结了深度学习与浅层神经网络的不同之处。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Deep Learning" scheme="http://yaodong.ml/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Deep Learning学习笔记——蒙特卡罗方法（MCMC）</title>
    <link href="http://yaodong.ml/deep-learning/Deep-Learning%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97%E6%96%B9%E6%B3%95%EF%BC%88MCMC%EF%BC%89.html"/>
    <id>http://yaodong.ml/deep-learning/Deep-Learning学习笔记——蒙特卡罗方法（MCMC）.html</id>
    <published>2017-01-21T06:34:15.000Z</published>
    <updated>2017-03-21T06:42:07.062Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;本文对马尔科夫链蒙特卡罗（Markov Chain Monte Carlo，MCMC）作简单的介绍。主要参考《Pattern Recognition and Machinle Learning》和Ian Goodfellow的《Deep Learning》。</p>
<a id="more"></a>
<p>&emsp;&emsp;蒙特卡罗是一种在优先计算资源下近似计算的方法。蒙特卡罗方法通过对概率分布的数据进行采样，用采样点的“频率”估计该“概率”分布。MCMC可解决高维空间中的积分和优化不容易直接计算求解的问题。<br>&emsp;&emsp;给定一个概率分布$p(x)$，如何让计算机生成尽可能服从该概率分布的样本集？这是统计模拟领域的采样问题。如何从$p(x)$中采样，能使采样样本尽可能符合概率分布规律，一个方案是<strong>重要采样</strong>，但更常用的方案是使用一个趋于目标分布估计的序列，即马尔科夫链蒙特卡罗方法（MCMC）。</p>
<h3 id="马尔科夫链"><a href="#马尔科夫链" class="headerlink" title="马尔科夫链"></a>马尔科夫链</h3><p>&emsp;&emsp;马尔科夫链是MCMC的理论基础，在随机过程和信息论基础课上都学过这个概念，做一下简单的回顾。<br>&emsp;&emsp;马尔科夫链表示的是一个状态序列${x<em>1,x_2,…,x</em>{t-1},x<em>t,x</em>{t+1}}$，其中每个状态的取值为有限个，在当前状态已知的条件下，将来状态的取值只与当前状态有关，与过去的状态无关。马尔科夫链的数学定义如下：</p>
<script type="math/tex; mode=display">P(X_{t+1}=x|X_t,X_{t−1},...)=P(X_{t+1}=x|X_t)</script><p>&emsp;&emsp;<strong>马尔科夫链定理</strong>：如果一个非周期马尔科夫链具有转移概率矩阵$P$，且它的任何两个状态是互通的，那么$\lim<em>{n \rightarrow \infty}P</em>{ij}^{n}$存在且与$i$无关，记$\lim<em>{n \rightarrow \infty}P</em>{ij}^{n}=\pi(j)$，有</p>
<script type="math/tex; mode=display">\pi(j)=\sum_{i=0}^{\infty}\pi(i)P_{ij}</script><p>&emsp;&emsp;其中，$\pi$是方程$\pi P=\pi$的唯一非负解，其中</p>
<script type="math/tex; mode=display">\pi=\left[\pi(1),\pi(2),...,\pi(j),...,\right],\sum_{i=0}{\infty}\pi(i)=1</script><p>&emsp;&emsp;$\pi$称为马尔科夫链的平稳分布。定理的证明相对比较复杂，在这里直接使用定理的结论。从初始概率分布$\pi_0$出发，在马尔科夫链上做状态转移，记$X_i$的概率分布为$\pi_i$，则有</p>
<script type="math/tex; mode=display">X_0 \sim \pi_0(x)</script><script type="math/tex; mode=display">X_i \sim \pi_i(x) , \pi_i(x) =\pi_{i-1}(x)P=\pi_0(x)P^n</script><p>&emsp;&emsp;由马尔科夫链定理，概率分布$\pi_i(x)$将收敛到平稳分布$\pi(x)$，假设到第n步时马尔科夫链收敛，则有</p>
<script type="math/tex; mode=display">X_0 \sim \pi_0(x)</script><script type="math/tex; mode=display">X_1 \sim \pi_1(x)</script><script type="math/tex; mode=display">X_2 \sim \pi_2(x)</script><script type="math/tex; mode=display">X_n \sim \pi_n(x)</script><script type="math/tex; mode=display">X_{n+1} \sim \pi_{n}(x)</script><script type="math/tex; mode=display">X_{n+2} \sim \pi_n(x)</script><p>&emsp;&emsp; 如果我们从一个具体的初始状态$x<em>0$开始，沿着马尔科夫链按照概率转移矩阵$P$做跳转，那么我们得到一个转移序列${x_0,x_1,x_2,…,x_n,x</em>{n+1},x<em>{n+2}}$，由于马尔科夫链的收敛性质， ${x_n,x</em>{n+1},x_{n+2},…}$都将是平稳分布$\pi(x)$的采样样本。</p>
<h3 id="问题的由来"><a href="#问题的由来" class="headerlink" title="问题的由来"></a>问题的由来</h3><p>&emsp;&emsp;在机器学习问题中，常常遇到计算和或者积分的问题。当和或积分形式过于复杂无法直接计算时，通常可以将待求的和或积分视为某概率分布下的期望值，然后通过（无偏）估计来近似这个期望。这就是马尔科夫采样解决的问题。令<br>$s=\sum_xp(x)f(x)=E_p[f(x)]$ 或 $s=\int_xp(x)f(x)=E_p[f(x)]$为待求的和或积分，$p(x)$为关于随机变量$x$的概率分布或者概率密度函数。</p>
<h3 id="Metropolis-Hastings算法"><a href="#Metropolis-Hastings算法" class="headerlink" title="Metropolis-Hastings算法"></a>Metropolis-Hastings算法</h3><p>&emsp;&emsp;对于给定的概率分布$p(x)$，我们希望能快速生成它对应的样本。Metropolis算法是最早的基于马尔科夫链的蒙特卡罗方法。思路是构造一个转移矩阵为$P$的马尔科夫链，使得该马尔科夫链的平稳分布恰好是$p(x)$，从任何一个初始状态$x<em>0$出发沿着马尔科夫链转移，得到一个转移序列${x_0,x_1,x_2,…,x_n,x</em>{n+1},x<em>{n+2},…}$，由于马尔科夫链的收敛性质， ${x_n,x</em>{n+1},x_{n+2},…}$都将是平稳分布$\pi(x)$的采样样本。<br>&emsp;&emsp;基于马尔科夫链做采样的关键问题是如何构造转移矩阵$P$，使得马氏链的平稳分布恰好是我们想要的概率分布$p(x)$。首先，非周期马尔科夫链必须具备<strong>细致平稳条件</strong>：如果非周期马尔科夫链的转移矩阵$P$和分布$\pi(x)$满足</p>
<script type="math/tex; mode=display">\pi(i)P_{ij} = \pi(j)P_{ji} \quad for \quad all \quad i.j</script><p>&emsp;&emsp;则$\pi(x)$是马尔科夫链的平稳分布。细致平稳条件的物理含义就是对于任何两个状态$i$、$j$，从$i$转移出去到j而丢失的概率质量恰好会被从$j$转移回i的概率质量补充回来，所以状态$i$上的概率质量π(i)是稳定的，从而$\pi(x)$是马尔科夫链的平稳分布。<br>&emsp;&emsp;用$q(i,j)$或$q(j|i)$表示从状态i转移到状态j的概率。通常情况下，马尔科夫链的细致平稳条件不成立，即</p>
<script type="math/tex; mode=display">p(i)q(i,j) \neq p(j)q(j,i)</script><p>&emsp;&emsp;所以$p[(x)$不太可能是这个马尔科夫链的平稳分布。因此，需要对马尔科夫链加入影响因子，使得细致平稳条件成立，引入$\alpha$，使得下式成立：</p>
<script type="math/tex; mode=display">p(i)q(i,j)\alpha(i,j) = p(j)q(j,i)\alpha(j,i)</script><p>&emsp;&emsp;根据对称性，一般取</p>
<script type="math/tex; mode=display">\alpha(i,j) = p(j)q(j,i)</script><script type="math/tex; mode=display">\alpha(j,i) = p(i)q(i,j)</script><p>&emsp;&emsp;在改造转移矩阵中引入的$\alpha(i,j)$称为接受率，物理意义上可理解为在原来的马尔科夫链上，从状态i以概率$q(i,j)$转移到状态$j$时，以$\alpha(i,j)$的概率接受这个转移。<br>&emsp;&emsp;不过，马尔科夫链Q在转移的过程中的接受率$\alpha(i.j)$可能偏小，这样引发的问题是在采样过程中会出现大量的拒绝跳转，使得马尔科夫链的收敛速度太慢。解决此问题的措施是将细致平稳条件中的$\alpha(i,j),\alpha(j,i)$等比例放大，提高采样中的跳转接受率。即</p>
<script type="math/tex; mode=display">\alpha = \min \left \{ \frac {p(j)q(j,i)}{p(i)q(i,j)},1\right \}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;本文对马尔科夫链蒙特卡罗（Markov Chain Monte Carlo，MCMC）作简单的介绍。主要参考《Pattern Recognition and Machinle Learning》和Ian Goodfellow的《Deep Learning》。&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Deep Learning" scheme="http://yaodong.ml/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习算法笔记——集成学习（Ensemble Learning）</title>
    <link href="http://yaodong.ml/machine-learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%EF%BC%88Ensemble-Learning%EF%BC%89.html"/>
    <id>http://yaodong.ml/machine-learning/机器学习算法笔记——集成学习（Ensemble-Learning）.html</id>
    <published>2017-01-21T06:31:19.000Z</published>
    <updated>2017-03-21T06:32:10.863Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;集成学习，通过构建或结合多个弱分类器形成一个强分类器，使组合得到的分类器具有更好的泛化性能，从而达到提升模型分类性能的方法。严格来说，集成学习并不算是一种分类器，而是一种将多个弱分类器结合的策略。<br><a id="more"></a></p>
<h3 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h3><p>&emsp;&emsp;Boosting算法是一族可将弱学习器提升为强学习器的算法。<br>&emsp;&emsp;Boosting算法的理论基础是，在PAC（概率近似正确）的框架下，一定可以将多个弱学习器组合成强学习器。<br>&emsp;&emsp;Boosting算法的工作机制：先从初始样本数据训练出一个基学习器，再<strong>根据基学习器的性能表现改变不同类别训练样本的权重，即加大被误分类的样本的权重，或减小分类正确样本的权重，使得分类错误的样本在模型的后续学习过程中得到更多的关注</strong>。然后基于调整权值后的眼样本数据集重新训练下一个基学习器；如此反复进行，直至基学习器数目达到给定的值T，最后将这T个弱分类器进行加权结合，形成一个集成学习器。<br>&emsp;&emsp;大多数的Boosting方法都是改变训练数据的概率分布，即训练数据样本的权值分布，针对不同的训练数据分布调用不同的弱学习器，进而训练出一系列弱分类器。Boosting方法中应用较为广泛的有提升树模型（Boosting Tree）和自适应提升算法（Adaptive Boosting）。关于AdaBoosting算法的详细笔记，见本人的另一篇博客，<a href="http://yaodong.ml/">机器学习算法笔记——AdaBoosting算法</a>。</p>
<h3 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h3><p>&emsp;&emsp;Bagging思路的关键问题是保证各基学习器之间的多样性和准确性。</p>
<h4 id="Bootstraping"><a href="#Bootstraping" class="headerlink" title="Bootstraping"></a>Bootstraping</h4><p>&emsp;&emsp;简单介绍一下Bootstrapping法，BootSstrap Sampling称为自助采样，具体是通过对有限的样本进行多次有放回随机抽样，重新建立起新的样本集。<br>&emsp;&emsp;给定样本数据集$D$，对$D$进行Boostraping得到新$D_1$的具体做法如下：每次随机从D中采样一个样本，将其拷贝到$D_1$中，并把该样本放回到原数据集$D$中，将这种“拷贝再放回”执行$m$次，就得到了包含$m$个样本的数据集$D_1$。显然，Bootstraping方法改变了原始数据集的分布，使得$D$中有一部分样本不出现在$D_1$中，但有一部分样本多次出现在$D_1$中。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>&emsp;&emsp;从偏差-方差的角度来看：Boosting主要关注降低偏差，因此Boosting能基于泛化性能相对较弱的学习器构建出很强的集成模型；而Bagging主要关注模型的方差，因此Bagging在不进行剪枝的决策树、神经网络等易受样本扰动的学习器上更为明显。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;集成学习，通过构建或结合多个弱分类器形成一个强分类器，使组合得到的分类器具有更好的泛化性能，从而达到提升模型分类性能的方法。严格来说，集成学习并不算是一种分类器，而是一种将多个弱分类器结合的策略。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Machine Learning" scheme="http://yaodong.ml/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习算法笔记——AdaBoosting算法</title>
    <link href="http://yaodong.ml/machine-learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94Boosting%20&amp;%20Adaboosting.html"/>
    <id>http://yaodong.ml/machine-learning/机器学习算法笔记——Boosting &amp; Adaboosting.html</id>
    <published>2017-01-21T06:29:46.000Z</published>
    <updated>2017-04-13T05:24:25.283Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;从算法模型的角度来说，AdaBoosting算法是前向分布加法算法的特例，也就是说，AdaBoosting算法是一个加法模型，其学习算属于为前向分步算法。<br><a id="more"></a><br>&emsp;&emsp;AdaBoosting算法的损失函数是指数函数。回忆一下，机器学习算法中，常见的损失函数有0-1损失函数、对数似然损失函数、均方误差函数、Hinge函数、指数损失函数等。</p>
<h3 id="Adaboost算法"><a href="#Adaboost算法" class="headerlink" title="Adaboost算法"></a>Adaboost算法</h3><p>&emsp;&emsp;对AdaBoosting算法作简单推导。AdaBoosting算法的Loss Function定义如下：</p>
<script type="math/tex; mode=display">L\big[ y,f(\boldsymbol{x})\big]=e^{-yf(\boldsymbol{x})}</script><p>&emsp;&emsp;给定样本数据集$D={(x^{(1)},y^{(1)}),…,(x^{(i)},y^{(i)}),…,(x^{(N)},y^{(N)})}$，$x^{(i)}\in R^n$，$y(i)\in {-1,+1}$，指数损失函数的标准形式为：</p>
<script type="math/tex; mode=display">L\big[ y,f(\boldsymbol{x})\big]=\sum_{i=1}^{N}e^{-y^{(i)}f(x^{(i)})}</script><p>&emsp;&emsp;Adaboosting算法的损失函数为：</p>
<script type="math/tex; mode=display">L\big[ y,f(\boldsymbol{x})\big]=\frac{1}{N}\sum_{i=1}^{N}e^{-y^{(i)}f(x^{(i)})}</script><ol>
<li>初始化数据集的样本权值分布<script type="math/tex; mode=display">W=(w_{11},...,w_{1i},...,w_{1n}),\quad w_{1i}=\frac{1}{N},\quad i=1,2,...,N</script></li>
<li><p>训练$m$个基学习器：<br><strong>1)</strong> 对于第$i$个样本，根据具有权值分布$W_m$的训练数据集，求得基分类器$G_m(x)$：</p>
<script type="math/tex; mode=display">G_m(x): \mathcal{X} \rightarrow \{-1,+1\}</script><p><strong>2)</strong> 求得$G_m(x)$在训练数据集上的加权分类误差率：</p>
<script type="math/tex; mode=display">e_m=P(G_m(x^{(i)})≠y^{(i)})=\sum_{i=1}^{m}w_{mi}I(G_m(x^{(i)})≠y^{(i)})</script><p><strong>3)</strong> 计算$G_m(x)$的系数$\alpha_m$，$\alpha_m$表示第m个基学习器在最终的组合学习器中的比重：</p>
<script type="math/tex; mode=display">\alpha_m=\frac{1}{2}log\frac{1-e_m}{e_m}</script><p>很显然，$\alpha_m$随着$e_m$的增大而减小，所以分类误差率越小的基学习器在最终的组合模型中占的比重越大。<br><strong>4)</strong> 更新样本权值分布向量：</p>
<script type="math/tex; mode=display">W_{m+1} = (w_{m+1,1},...,w_{m+1,N})</script><script type="math/tex; mode=display">w_{m+1,i}=\frac {w_{mi}}{Z_m}e^{-\alpha _my^{(i)}G_m(x_i)}</script><p>这里$Z<em>m$是规范化因子，目的是将$W</em>{m+1}$进行归一化，使得$W_{m+1}$符合概率分布的归一性，$Z_m$由以下公式给出：</p>
<script type="math/tex; mode=display">Z_m=\sum_{i=1}^Nw_{mi}e^{-\alpha _my^{(i)}G_m(x^{(i)})}</script><p>根据$w_{m+1,i}$的表达式可知，被$G_m(x)$分类器误分类的样本的权值得以扩大，而被$G_m(x)$正确分类的样本的权值会缩小。因此，误分类样本在下一次的基学习器训练中起到更大的作用。这个<strong>不改变训练数据，而改变训练数据的权值分布，正是AdaBoost方法的重要特点</strong>。</p>
</li>
<li><p>步骤2中求得了$M$个基学习器的权值分布，将这$M$个基学习器组合起来，构成最终的组合学习器：</p>
<script type="math/tex; mode=display">f(\boldsymbol{x})=\sum_{m=1}^{M}\alpha_mG_{m}(x)</script><script type="math/tex; mode=display">G(x)=sign\big[f(x)\big]=sign\left(\sum_{m=1}^{M}\alpha_mG_{m}(x))\right)</script><p><strong>注意</strong>，所有的$\alpha_m$相加之和并不等于1。</p>
</li>
</ol>
<h3 id="前向分步算法"><a href="#前向分步算法" class="headerlink" title="前向分步算法"></a>前向分步算法</h3><p>&emsp;&emsp;从算法模型的角度来说，AdaBoosting算法是前向分布加法算法的特例，也就是说，AdaBoosting算法是一个加法模型，损失函数为指数函数的前向分步算法。<br>&emsp;&emsp;加法模型的理论基础是算法模型的可加性，加法模型是由多个基学习模型加权组合得到。定义加法模型$f(\boldsymbol{x})$为</p>
<script type="math/tex; mode=display">f(\boldsymbol{x})=\sum_{i=1}^{M}\beta_{m}h(\boldsymbol{x};\alpha_m)</script><p>&emsp;&emsp;其中，$\beta_m$是第m个基学习器的权重，$\alpha_m$表示第m个基学习模型的参数，$h(\boldsymbol{x})$表示第m个基学习模型的模型函数。<br>&emsp;&emsp;给定训练数据集合损失函数$\mathcal{L}\big[f(\boldsymbol{x}),y\big]$的前提下，对加法模型$f(\boldsymbol{x})$的学习将转化为经验风险最小化问题，即在训练集上求解模型的损失函数最小化问题：</p>
<script type="math/tex; mode=display">(\boldsymbol{\alpha},\boldsymbol{\beta})=\arg\min_{\alpha_{m},\beta_{m}}\sum_{i=1}^{N}L\big[y^{(i)},\sum_{i=1}^{M}\beta_{m}h(\boldsymbol{x};\alpha_m)\big]</script><p>&emsp;&emsp;前向分步算法的核心思想是从前往后训练$M$个基学习模型，每个模型都在前一个模型的基础上进一步训练得到。另外，每个基学习模型只学习一个基函数$h_m(\boldsymbol{x})$及其组合权重$\beta_m$，最后将这$M$个基学习模型进行加权组合，进而逐步逼近优化Loss Function。每一步的优化可用公式表示如下：</p>
<script type="math/tex; mode=display">\min_{\alpha,\beta}\sum_{i=1}^{N}L\big[y^{(i)},\sum_{i=1}^{M}\beta h(\boldsymbol{x};\alpha)\big]</script><p>&emsp;&emsp;给定样本数据集$D={(x^{(1)},y^{(1)}),…,(x^{(i)},y^{(i)}),…,(x^{(N)},y^{(N)})}$，$x^{(i)}\in R^n$，$y(i)\in {-1,+1}$，模型的损失函数为$L(y,f(\boldsymbol{x}))$，前向分步算法步骤总结如下：</p>
<ol>
<li>初始化$f(\boldsymbol{x})$：$f_0(x)=0$</li>
<li>训练M个基学习模型，对$m=1,2,…M$：<br><strong>a)</strong> 计算当前参数下的损失函数，并通过优化方法，求得目标参数$\alpha_m$，$\beta_m$：<script type="math/tex; mode=display">(\alpha_m,\beta_m)=\min_{\alpha,\beta}\sum_{i=1}^{N}L\big[y^{(i)},f_{m-1}(\boldsymbol{x})+\beta_{m}h(x^{(i)};\alpha)\big]</script><strong>b)</strong> 实时更新$f_{m}(\boldsymbol{x})$<script type="math/tex; mode=display">f_{m}(\boldsymbol{x})=f_{m-1}(\boldsymbol{x})+\beta_{m}h(\boldsymbol{x};\alpha_m)</script></li>
<li>对$M$个基学习模型进行加权组合，得到最后的Additive Model：<script type="math/tex; mode=display">f((\boldsymbol{x})=f_{M}(\boldsymbol{x})=\sum_{i=1}^{M}\beta_{m}h(\boldsymbol{x};\alpha_m)</script></li>
</ol>
<hr>
<p>&emsp;&emsp;前向分步算法将同时求解所有参数$(\boldsymbol{\alpha},\boldsymbol{\beta})$的优化问题简化为逐次求解各个基学习模型的参数$(\alpha_m,\beta_m)$优化问题。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;从算法模型的角度来说，AdaBoosting算法是前向分布加法算法的特例，也就是说，AdaBoosting算法是一个加法模型，其学习算属于为前向分步算法。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Machine Learning" scheme="http://yaodong.ml/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习算法笔记——偏差与方差</title>
    <link href="http://yaodong.ml/machine-learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE.html"/>
    <id>http://yaodong.ml/machine-learning/机器学习算法笔记——偏差与方差.html</id>
    <published>2017-01-21T06:26:55.000Z</published>
    <updated>2017-03-21T06:28:11.513Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;最近在复习基本机器学习、深度学习的基础模型和理论算法，总结一下机器学习中常见的几个概念。<br><a id="more"></a><br>&emsp;&emsp;偏差可以理解为算法模型对训练数据集的拟合程度，也可以理解为模型再训练数据集上的表现。而方差描述的是用样本集训练得到的模型在测试机上的性能表现。偏差度量了算法模型的期望预测与真实结果的偏离程度，即偏差描述的是算法模型本身的拟合能力；方差度量了同样大小的数据集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;最近在复习基本机器学习、深度学习的基础模型和理论算法，总结一下机器学习中常见的几个概念。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Machine Learning" scheme="http://yaodong.ml/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习算法笔记——正则化、交叉验证</title>
    <link href="http://yaodong.ml/machine-learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%AD%A3%E5%88%99%E5%8C%96%E3%80%81%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81.html"/>
    <id>http://yaodong.ml/machine-learning/机器学习算法笔记——正则化、交叉验证.html</id>
    <published>2017-01-21T06:23:34.000Z</published>
    <updated>2017-03-21T06:24:16.655Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;<br><a id="more"></a></p>
<h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p>&emsp;&emsp;正则化是机器学习算法模型中避免过拟合的一种策略。这里提一句，避免模型陷入过拟合的另外一个策略是增加训练数据集的规模。正则化也是结构风险最小化策略的实现，是在结构风险上加入正则化项（惩罚项），正则化项一般是模型复杂度的单调递增函数。模型越复杂，正则化项对模型的惩罚程度越大。<strong>通常来说，正则化项一般是算法模型参数向量的范数。</strong><br>&emsp;&emsp;从“大牛即权威”的角度来说，正则化操作符合奥卡姆剃刀（Occam’s razor）原理，即在所有可供选择的算法模型中，能够很好拟合数据并且算法复杂度更低的模型才是最好的模型。从贝叶斯概率框架的角度来说，正则化项对应于模型的先验概率：复杂模型具有较小的先验概率，简单模型具有较大的先验概率。也就是说，通过正则化项，我们可以对模型加入人对该模型的先验知识，使得算法模型能get到我们想要的特性，例如稀疏、平滑、低秩等。从算法复杂度的角度来说，正则化操作的目的是在保证误差极小化的基础上降低算法复杂度。因此，为了保证算法的泛化性能，在算法的学习和训练过程中加入正则化项是必要的。<br>&emsp;&emsp;加入正则化项的有监督学习的目标函数如下：</p>
<script type="math/tex; mode=display">w^*=\arg\min_w=\sum_iL(y^{(i)},f(x^{(x)};w))+\lambda\Omega(w)</script><p>&emsp;&emsp;上面说到，正则化项一般是算法模型参数向量的范数，如零范数、1-范数、2-范数、迹范数、Frobenius范数和核范数等等，接下来，简单介绍一下正则化中常用的几个范数。</p>
<h4 id="L0范数和L1范数"><a href="#L0范数和L1范数" class="headerlink" title="L0范数和L1范数"></a>L0范数和L1范数</h4><p>&emsp;&emsp;正则化项为L1范数的目标函数形式如下：</p>
<script type="math/tex; mode=display">J(w,\lambda)=\sum_iL(y^{(i)},f(x^{(x)};w))+\frac {\lambda}{m}\sum_i \mid w \mid</script><p>&emsp;&emsp;L0范数是指参数向量中非0的元素的个数。如果用L0范数来规则化一个参数矩阵$W$的话，就是希望$W$的大部分元素都是0，换句话说，L0正则化项的目的是使参数$W$是稀疏的。<br>&emsp;&emsp;L1范数是指参数向量中各元素的绝对值之和，也叫“稀疏规则算子”（Lasso regularization），L1范数和L0范数的关系是——<strong>L1范数是L0范数的最优凸近似</strong>。<br>但是在大多数paper中，参数矩阵的稀疏化操作都是通过L1范数（$\parallel W \parallel_1$）实现。由于L0范数很难优化求解（NP难问题），L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解。因此，L1范数的使用更为普遍。<br>&emsp;&emsp;<strong>L1范数和L0范数可以实现稀疏，L1因具有比L0更好的优化求解特性而被广泛应用。</strong></p>
<h4 id="L2范数"><a href="#L2范数" class="headerlink" title="L2范数"></a>L2范数</h4><p>&emsp;&emsp;正则化项为L2范数的目标函数形式如下：</p>
<script type="math/tex; mode=display">J(w,\lambda)=\sum_iL(y^{(i)},f(x^{(x)};w))+\frac {\lambda}{2m}\sum_i \parallel w \parallel_2^2</script><p>&emsp;&emsp;L2范数在机器学习算法的正则化项中也很常见，L2范数（$\parallel W \parallel_2$）在回归问题里叫做“岭回归”（Ridge Regression）,部分学者把它叫做“权值衰减”（Weight Decay）。L2范数的强大之处在于，将其应用在机器学习中能有效改善过拟合问题。<br>&emsp;&emsp;L2范数是指参数矩阵$\parallel W \parallel$ 的各元素的平方和然后求平方根。L2范数的规则项$\parallel W \parallel_2$最小，意味着$W$的每个元素都很小，都接近于0但不等于0。而越小的参数说明算法模型越简单，越简单的模型则越不容易产生过拟合现象。总结起来就是，<strong>通过L2范数，可以实现对算法模型空间的限制，从而在一定程度上避免过拟合，提升模型的泛化能力。</strong><br>&emsp;&emsp;此外，L2范数应用于机器学习算法的正则化项亦可以从理论和数值优化计算的角度去解释，大家有兴趣可以去查阅相关的论文。</p>
<h3 id="交叉验证（Cross-Validation，CV）"><a href="#交叉验证（Cross-Validation，CV）" class="headerlink" title="交叉验证（Cross Validation，CV）"></a>交叉验证（Cross Validation，CV）</h3><p>&emsp;&emsp;交叉验证是用来验证算法模型的分类性能的一种统计分析方法，基本思想是把在某种意义下将原始数据集$D$进行分组，一部分做为训练集（training set），另一部分做为验证集（validation set）。首先用训练集对分类器进行模型训练，再利用验证集来测试训练好的模型的性能表现，以此来做为评价分类器的性能指标。下面对常见的交叉验证方法做简单介绍。</p>
<h4 id="Hold-Out-Method"><a href="#Hold-Out-Method" class="headerlink" title="Hold-Out Method"></a>Hold-Out Method</h4><p>&emsp;&emsp;将原始数据随机分为两组，一组做为训练集，一组做为验证集。利用训练集训练分类器，然后利用验证集验证模型，记录最后的分类准确率作为此分类器的性能指标。<br>&emsp;&emsp;此种方法的好处是简单易操作，只需随机把原始数据分为两组即可。但是严格意义来说Hold-Out Method并不能算是CV，因为这种方法只是随机地对原始数据进行分组，并没有使用交叉的思想，所以最后验证集分类准确率的高低与原始数据的分组有很大的关系，所以这种方法得到的结果其实并不具有说服性。</p>
<h4 id="Double-Cross-Validation（2-fold-Cross-Validation，2-CV）"><a href="#Double-Cross-Validation（2-fold-Cross-Validation，2-CV）" class="headerlink" title="Double Cross Validation（2-fold Cross Validation，2-CV）"></a>Double Cross Validation（2-fold Cross Validation，2-CV）</h4><p>&emsp;&emsp;将数据集分成两个样本数目相等的子集，进行两回合的分类器训练。在第一回合中，一个子集作为training set，另一个便作为testing set；在第二回合中，则将training set与testing set对换，再次训练分类器，并得到两次训练的分类性能。<br>&emsp;&emsp;实际中2-CV并不常用，主要原因是training set样本数太少，通常不足以代表母体样本的分布，导致testing阶段辨识率容易出现明显落差。此外，2-CV中子集集的变异度较大，往往无法达到“实验过程必须可以被复制”的要求。</p>
<h4 id="K-fold-Cross-Validation（K-折交叉验证，记为K-CV）"><a href="#K-fold-Cross-Validation（K-折交叉验证，记为K-CV）" class="headerlink" title="K-fold Cross Validation（K-折交叉验证，记为K-CV）"></a>K-fold Cross Validation（K-折交叉验证，记为K-CV）</h4><p>&emsp;&emsp;将原始数据集$D$划分为K个大小相似的互斥子集（一般是均分），每个子集都通过对原始数据的分层采样得到，以尽可能保证数据分布的一致性。然后，每次分别用K-1个子集的并集作为训练集，余下的那个子集当作验证集，这样就可以得到K组训练集/测试集组合，从而可进行K次训练和测试。最后用这K个模型的验证集的分类准确率的均值作为此K-CV下分类器的性能指标。<br>&emsp;&emsp;此外，为减小因样本划分不同而引入的差别，k折交叉验证通常要随机使用不同的划分重复p次，最终的结果是这p次K折交叉验证结果的均值。K-CV可以有效的避免过学习以及欠学习状态的发生，最后得到的结果也比较具有说服性。</p>
<h4 id="Leave-One-Out-Cross-Validation（留一法，LOO-CV）"><a href="#Leave-One-Out-Cross-Validation（留一法，LOO-CV）" class="headerlink" title="Leave-One-Out Cross Validation（留一法，LOO-CV）"></a>Leave-One-Out Cross Validation（留一法，LOO-CV）</h4><p>&emsp;&emsp; 如果设原始数据有m个样本，那么LOO-CV就m-CV，即每个样本单独作为验证集，其余的m-1个样本作为训练集，所以LOO-CV会得到m个模型，用这m个模型最终的验证集的分类准确率的均值作为此下LOO-CV分类器的性能指标。相比于前面的K-CV，LOO-CV有两个明显的优点：<br>（1）每一回合中几乎所有的样本皆用于训练模型，因此最接近原始样本的分布，这样评估所得的结果比较可靠。<br>（2）实验过程中没有随机因素会影响实验数据，确保实验过程是可以被复制的。</p>
<p>&emsp;&emsp;但LOO-CV的缺点则是计算成本高，因为需要建立的模型数量与原始数据样本数量相同，当原始数据样本数量相当多时，LOO-CV在实作上便有困难几乎就是不显示，除非每次训练分类器得到模型的速度很快，或是可以用并行化计算减少计算所需的时间。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Machine Learning" scheme="http://yaodong.ml/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习算法笔记——模型性能度量（Precision/Recall/ROC/AUC）</title>
    <link href="http://yaodong.ml/machine-learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F%EF%BC%88Precision-Recall-ROC-AUC%EF%BC%89.html"/>
    <id>http://yaodong.ml/machine-learning/机器学习算法笔记——模型性能度量（Precision-Recall-ROC-AUC）.html</id>
    <published>2017-01-21T06:21:48.000Z</published>
    <updated>2017-03-21T06:47:01.837Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;本文总结了评估模型性能的几个度量指标。<br><a id="more"></a><br>&emsp;&emsp;分类问题中，最直接的模型性能度量指标是错误率和精度。但在实际的应用中，更常用的模型评价指标是查准率（precision）与查全率（recall）。查准率也叫准确率，查全率也叫召回率。<br>&emsp;&emsp;在二分类问题中，将关注的类别记为正类，其他类记为父类。分类模型在测试数据集上的预测结果可分为以下几类：</p>
<blockquote>
<p>TP（真正例）——将正类样本预测为正类的样本数量<br>FP（假正例）——将负类样本预测为正类的样本数量<br>FN（假反例）——将正类样本预测为负类的样本数量<br>TN（真反例）——将负类样本预测为负类的样本数量</p>
</blockquote>
<p>&emsp;&emsp;查准率定义为</p>
<script type="math/tex; mode=display">precision=\frac{TP}{TP+FP}</script><p>&emsp;&emsp;查全率定义为</p>
<script type="math/tex; mode=display">recall=\frac{TP}{TP+FN}</script><p>&emsp;&emsp;查准率与查全率是一对矛盾的度量。因此还定义了$F_1$度量，$F_1$定义为查准率和查全率的调和均值</p>
<script type="math/tex; mode=display">\frac{1}{F_1}=\frac{1}{precision}+\frac{1}{recall}=\frac{2TP}{2TP+FP+FN}</script><p>&emsp;&emsp;不同的实际应用场景对查准率和查全率的重视程度有所不同。$F<em>1$度量还有更一般的形式——$F</em>{\beta}$，根据$\beta$的取值可调整$F_{\beta}$对查准率和查全率的重视程度。</p>
<script type="math/tex; mode=display">F_\beta=\frac{(1+\beta ^2)\times P \times R}{(\beta^2 \times P)+R}</script><p>&emsp;&emsp;其中$\beta &gt; 0$，度量了查全率对查准率的相对重要性，$\beta = 1$时退化为标准的$F_1$，$\beta &gt; 1$时更重视查全率，$\beta &gt; 1$时更重视查准率。</p>
<h3 id="P-R曲线"><a href="#P-R曲线" class="headerlink" title="P-R曲线"></a>P-R曲线</h3><p>&emsp;&emsp;P-R曲线即Precision-Recall曲线，是以查准率为纵轴、查全率为横轴作图所得的曲线。P-R曲线能直观显示出分类模型在样本集上的查全率、查准率。如果模型A的P-R曲线完全被模型B的P-R曲线包住，则可断言模型B的性能优于模型A。如果两个分类模型的P-R曲线有交叉，则难以一般性地断言二者孰优孰劣，只能在具体的查准率或查全率条件下再对二者进行评估。更多情况下，比较合理的判据是比较P-R曲线与坐标轴围成的面积的大小，它在一定程度上表征了分类模型在查准率与查全率上取得相对“双高”的比例。</p>
<h3 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h3><p>&emsp;&emsp;ROC的全称是Receiver Operating Characteristic，译为受控者工作特征。ROC曲线是以“真正例率”TPR为纵轴、以“假正例率”FPR为横轴绘制的曲线。TPR、FPR定义如下：</p>
<script type="math/tex; mode=display">TPR = \frac{TP}{TP+FN}</script><script type="math/tex; mode=display">FPR=\frac{FP}{TN+FP}</script><p>&emsp;&emsp;TPR也叫灵敏度（sensitivity），TPR即召回率；FPR也叫特异度（specificity），FPR可以理解为在负类样本中，把负类样本误分为正样本的比例。TPR、FPR二者是相互对立的关系，此消彼长。<br>&emsp;&emsp;一般分类器对样本进分类的思路为，对测试样本产生一个回归值或概率预测，记为score，并将score与分类阈值（Threshold）比较，进而对样本进行分类。可以根据模型对每个样本计算出的“似然值”对测试样本进行从大到小重新排序，从这个角度来说，分类问题就相当于是将该排序序列在某个“截断点”将样本集分为两部分，前一部分的最终预测结果为正例，后一半的预测结果为反例。对于一个分类模型，选择不同的分类阈值，可以生成不同对（TPR，FPR），将这些（TPR，FPR）坐标值绘制在坐标轴中，就得到了当前分类模型的ROC曲线。一般来说，分类阈值可取每个样本的预测“似然值”score，这样就可以生成m对（TPR，FPR）坐标值，m为样本的数量。显然，在ROC曲线空间中，离右上角（0,1）点越近，说明算法的性能越好。<br>&emsp;&emsp;如果分类模型的ROC曲线被模型B的ROC曲线完全“包住”，则可断言模型B的分类性能由于模型A；若两个模型的ROC曲线发生交叉，则不能直接判断那个模型的性能更好。较为合理的判据是比较ROC曲线下的面积，即AUC值。<br>&emsp;&emsp;ROC曲线有个很好的特性：当测试集中的正负样本的分布发生变化时，ROC曲线能保持不变。</p>
<h3 id="AUC值"><a href="#AUC值" class="headerlink" title="AUC值"></a>AUC值</h3><p>&emsp;&emsp;AUC的全称是Area Under Curve，AUC值定义为ROC曲线下的面积。显然，AUC的取值在0.5~1之间。AUC作为数值，可以直接评价分类模型的好坏，AUC值越大，模型的分类性能越好。<br>&emsp;&emsp;另外，根据ROC曲线的绘制规则可知，AUC值可以从概率的角度理解：随机地挑选一个正样本，当前的分类模型根据样本的“似然值”将该正样本排在负样本的前面的概率即为AUC，AUC值越大，当前的分类算法将正样本排在负样本前面的概率越大。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;本文总结了评估模型性能的几个度量指标。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Machine Learning" scheme="http://yaodong.ml/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>数据结构与算法之堆与堆排序</title>
    <link href="http://yaodong.ml/algorithms/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E4%B9%8B%E5%A0%86%E4%B8%8E%E5%A0%86%E6%8E%92%E5%BA%8F.html"/>
    <id>http://yaodong.ml/algorithms/数据结构与算法之堆与堆排序.html</id>
    <published>2017-01-05T06:19:24.000Z</published>
    <updated>2017-03-21T06:21:06.954Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;<br><a id="more"></a></p>
<h3 id="优先队列"><a href="#优先队列" class="headerlink" title="优先队列"></a>优先队列</h3><blockquote>
<p>&emsp;&emsp;<strong>优先队列</strong>（priority queue）是一种特殊的队列。优先队列与普通队列的不同之处在于，优先队列中的元素被赋予优先级。当访问元素时，队列中具有最高优先级的元素将优先出队，优先队列的行为特征是<strong>最高级先出</strong>（First In Largest Out）。</p>
</blockquote>
<p>优先队列至少满足以下两种操作：</p>
<ol>
<li>允许插入（insert）操作。优先队列允许插入操作，相当于普通队列中的入队操作。二者的区别在于，普通队列的入队操作只需将待操作元素插入队尾，而优先队列的插入操作，需要将元素插入到优先队列的正确位置中。</li>
<li>允许删除操作（deleteMin，deleteMax）。优先队列中的元素如果是数字，则元素的优先级对应于数字的大小。优先队列的删除操作相当于普通队列的出队操作，但优先队列需要找到并删除优先级最高的元素。</li>
</ol>
<p>优先队列的应用：</p>
<ol>
<li>作业系统中的调度程序：当一个作业完成后，需要在所有等待调度的作业队列中选择一个优先级最高的作业来执行，并且也可以添加一个新的作业到作业的优先队列中。</li>
</ol>
<hr>
<p>&emsp;&emsp;可以使用链表实现优先队列，但这将花费$O(N)$的时间复杂度进行删除；也可以使用二叉查找树实现优先队列，但二叉查找树的插入和删除操作的平均时间复杂度都是$O(logN)$，且二叉树的许多操作对于优先队列来说是不必要的，因此将二叉查找树用于优先队列过于复杂了。<br>&emsp;&emsp;一般使用二叉堆(Heap)来实现优先队列。二叉堆的插入和删除的最差时间复杂度为$O(logN)$ ，插入操作的的平均时间复杂对为$O(1)$ ，因此二叉堆可以以线性时间建立有$N$项的优先队列。</p>
<h3 id="二叉堆"><a href="#二叉堆" class="headerlink" title="二叉堆"></a>二叉堆</h3><p>&emsp;&emsp;二叉堆也称为堆，这里所说的堆是数据结构中的堆，而不是内存模型中的堆。二叉堆就结构上来说，是一棵完全二叉树。二叉堆满足<strong>结构性</strong>和<strong>堆序性</strong>：</p>
<ul>
<li>结构性：二叉堆应该满足完全二叉树的树结构</li>
<li>堆序性：二叉堆中，父节点的键值总是大于等于（或小于等于）任何一个子节点的键值。且每个节点的左子树和右子树仍然是一个二叉堆（最大堆或最小堆）。</li>
</ul>
<p>&emsp;&emsp;将根节点最大的堆叫做<strong>最大堆</strong>或大根堆，根节点最小的堆叫做<strong>最小堆</strong>或小根堆。常见的堆有二叉堆、左倾堆、斜堆、斐波那契堆等等。<br>&emsp;&emsp;由于二叉堆中的元素是有序排列的，而且二叉堆结构上是一棵完全二叉树，因此二叉堆可以使用一个数组和一个代表当前堆的大小的整数$N$组成。<br>&emsp;&emsp;回忆完全二叉树的性质：</p>
<blockquote>
<ol>
<li>一棵高度为$h$的完全二叉树的节点个数为$2^h$到$2^{h+1}-1$个；</li>
<li>完全二叉树的高度为$\lfloor O(logN)\rfloor$；</li>
<li>假设完全二叉树中的第一个元素在数组中的索引为$0$，那么父节点与子节点的位置关系如下：索引为$i$的左孩子的索引是 $2i+1$；索引为$i$的右孩子的索引是$2i+2$；索引为$i$的父结点的索引是$\lfloor \frac {i-1} 2\rfloor$。</li>
<li>假设完全二叉树中的第一个元素在数组中的索引为$1$，那么父节点与子节点的位置关系如下：索引为$i$的左孩子的索引是 $2i$；索引为$i$的右孩子的索引是$2i+1$；索引为$i$的父结点的索引是$\lfloor \frac i 2\rfloor$。</li>
</ol>
</blockquote>
<h4 id="插入（insert）操作"><a href="#插入（insert）操作" class="headerlink" title="插入（insert）操作"></a>插入（insert）操作</h4><p>&emsp;&emsp; 为将一个元素$X$插入到堆中，我们在下一个可用位置创建一个空穴，否则该堆将不是完全树。如果$X$可以放在该空穴中而不破坏堆的堆序性，那么插入操作完成。否则，我们把空穴的父节点上的元素移入该空穴中，这样空穴就朝着根的方向上冒一步。继续改过程直到 X 能被放入空穴中为止。这种实现过程称之为<strong>上滤（percolate up）</strong>：新元素在堆中上滤直到找出正确的插入位置。参考代码如下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">insert</span><span class="params">( AnyType x )</span></span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">if</span>( currentSize == array.length - <span class="number">1</span> )</div><div class="line">        enlargeArray( array.length * <span class="number">2</span> + <span class="number">1</span> );</div><div class="line"></div><div class="line">        <span class="comment">// Percolate up</span></div><div class="line">    <span class="keyword">int</span> hole = ++currentSize;</div><div class="line">    <span class="keyword">for</span>( array[ <span class="number">0</span> ] = x; x.compareTo( array[ hole / <span class="number">2</span> ] ) &lt; <span class="number">0</span>; hole /= <span class="number">2</span> )</div><div class="line">        array[ hole ] = array[ hole / <span class="number">2</span> ];</div><div class="line">    array[ hole ] = x;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h4 id="删除最小值（deleteMin）操作"><a href="#删除最小值（deleteMin）操作" class="headerlink" title="删除最小值（deleteMin）操作"></a>删除最小值（deleteMin）操作</h4><p>&emsp;&emsp;基于二叉堆的优先队列模型，出队的应该是最小元素。按照最小堆的堆序性，找出最小元十分容易，困难之处在于删除该节点，因为删除节点会破坏二叉堆的结构型，还需要进行额外的操作以保证二叉堆的结构完整。<br>&emsp;&emsp;当删除一个最小元素时，要在根节点建立一个空穴。由于现在堆少了一个元素，因此堆中最后一个元素 X 必须移动到该堆的某个地方。如果$X$可以直接被放到空穴中，那么<strong>deleteMin</strong>完成。不过这一般不太可能，因此我们将空穴的两个儿子中的较小者移入空穴，这样就把空穴向下推了一层，重复该步骤直到$X$可以被放入空穴中。因此，我们的做法是将 $X$置入沿着从根开始包含最小儿子的一条路径上的一个正确的位置。这种策略称为下滤（percolate down）。参考代码如下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> AnyType <span class="title">findMin</span><span class="params">( )</span></span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">if</span>( isEmpty( ) )</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnderflowException( );</div><div class="line">    <span class="keyword">return</span> array[ <span class="number">1</span> ];</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">public</span> AnyType <span class="title">deleteMin</span><span class="params">( )</span></span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">if</span>( isEmpty( ) )</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnderflowException( );</div><div class="line"></div><div class="line">    AnyType minItem = findMin( );</div><div class="line">    array[ <span class="number">1</span> ] = array[ currentSize-- ];</div><div class="line">    percolateDown( <span class="number">1</span> );</div><div class="line"></div><div class="line">    <span class="keyword">return</span> minItem;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">percolateDown</span><span class="params">( <span class="keyword">int</span> hole )</span></span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">int</span> child;</div><div class="line">    AnyType tmp = array[ hole ];</div><div class="line"></div><div class="line">    <span class="keyword">for</span>( ; hole * <span class="number">2</span> &lt;= currentSize; hole = child )</div><div class="line">    &#123;</div><div class="line">        child = hole * <span class="number">2</span>;</div><div class="line">        <span class="keyword">if</span>( child != currentSize &amp;&amp;</div><div class="line">                array[ child + <span class="number">1</span> ].compareTo( array[ child ] ) &lt; <span class="number">0</span> )</div><div class="line">            child++;</div><div class="line">        <span class="keyword">if</span>( array[ child ].compareTo( tmp ) &lt; <span class="number">0</span> )</div><div class="line">            array[ hole ] = array[ child ];</div><div class="line">        <span class="keyword">else</span></div><div class="line">            <span class="keyword">break</span>;</div><div class="line">    &#125;</div><div class="line">    array[ hole ] = tmp;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h3 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h3><p>&emsp;&emsp;堆排序是基于二叉堆的排序算法。堆分为“最大堆”和“最小堆”。最大堆用于升序排序，最小堆用于降序排序。<br>&emsp;&emsp;以升序排序为例，堆排序就是把最大堆堆顶的最大元素取出（为了尽可能不使用外部存储空间，取出操作通过与数组末位元素交换实现），再将剩余的堆调整为最大堆，如此反复进行，直至将堆中元素取完结束。</p>
<h4 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h4><p>堆排序算法的基本思想总结如下：</p>
<ol>
<li>初始化堆：将待排序的数组<strong>array[0,n-1]</strong>构造为最大堆（buildHeap）</li>
<li>交换数据：将array[0]与array[n-1]交换，使得当前堆的最大值存于array[n-1]</li>
<li>将array[0]~array[n-2]调整为最大堆，并将array[0]与array[n-2]交换</li>
<li>不断将数组相应位置的元素调整为最大堆，并将最大值取出，一直进行下去，直到堆中所有元素都被取出。</li>
</ol>
<p>&emsp;&emsp;由于堆的性质，堆排序可借助数组实现，假设第一个元素的下标为0，则索引为$i$的结点的左孩子索引为$2i+1$，右孩子的索引为$2i+2$，索引为$i$的结点如果有孩子结点，则其左孩子结点的索引下标为$\lfloor \frac{i-1}{2}\rfloor$。<br>&emsp;&emsp;将数组的部分元素调整为最大堆的程序buildHeap如下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">adjustHeap</span><span class="params">(<span class="keyword">int</span>[] array,<span class="keyword">int</span> start,<span class="keyword">int</span> end)</span> </span>&#123;</div><div class="line">    <span class="keyword">int</span> cur = start;</div><div class="line">    <span class="keyword">int</span> left = <span class="number">2</span> * cur + <span class="number">1</span>;</div><div class="line">    <span class="keyword">int</span> temp = array[cur];</div><div class="line">    <span class="keyword">for</span>(;left &lt;= end;current = left,left = <span class="number">2</span> * cur + <span class="number">1</span>) &#123;</div><div class="line">        <span class="keyword">if</span>(left &lt; end &amp;&amp; array[left] &lt; array[left + <span class="number">1</span>]) &#123;</div><div class="line">            left++;<span class="comment">//找到当前节点的孩子结点中较大的那个</span></div><div class="line">        &#125;</div><div class="line">        <span class="keyword">if</span>(temp &gt;= a[left]) &#123;</div><div class="line">            <span class="keyword">break</span>;</div><div class="line">        &#125;<span class="keyword">else</span> &#123;</div><div class="line">            array[cur] = array[left];</div><div class="line">            a[left] = temp;         </div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">swap</span><span class="params">(<span class="keyword">int</span>[] array,<span class="keyword">int</span> start,<span class="keyword">int</span> end)</span> </span>&#123;</div><div class="line">    <span class="keyword">int</span> temp = array[start];</div><div class="line">    array[start] = array[end];</div><div class="line">    array[end] =  temp;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">heapSort</span><span class="params">(<span class="keyword">int</span>[] array)</span> </span>&#123;</div><div class="line">    <span class="keyword">int</span>  i = <span class="number">0</span>;</div><div class="line">    <span class="keyword">int</span> n = array.length;</div><div class="line">    <span class="comment">//从最后一个非叶子结点开始，依次将array[i]~array[n-1]调整为最大堆</span></div><div class="line">    <span class="keyword">for</span>(i = n/<span class="number">2</span> - <span class="number">1</span>;i &gt;= <span class="number">0</span>;i--) &#123;</div><div class="line">        adjustHeap(array,i,n-<span class="number">1</span>);</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = n - <span class="number">1</span>;i &gt; <span class="number">0</span>;i--) &#123;</div><div class="line">        swap(aray,<span class="number">0</span>,i);</div><div class="line">        adjustHeap(array,<span class="number">0</span>,i-<span class="number">1</span>);</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h4 id="算法稳定性"><a href="#算法稳定性" class="headerlink" title="算法稳定性"></a>算法稳定性</h4><p>&emsp;&emsp;堆排序是不稳定的算法，它不满足稳定算法的定义。堆排序算法在交换数据的时候，是比较父结点和子节点之间的数据，所以，即便是存在两个数值相等的兄弟节点，它们的相对顺序在排序也可能发生变化。</p>
<h4 id="算法复杂度"><a href="#算法复杂度" class="headerlink" title="算法复杂度"></a>算法复杂度</h4><p>&emsp;&emsp;堆排序的时间复杂度是$O(Nlog_2N)$。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="数据结构" scheme="http://yaodong.ml/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning算法笔记——判别模型与生成模型</title>
    <link href="http://yaodong.ml/machine-learning/Machine%20Learning%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B.html"/>
    <id>http://yaodong.ml/machine-learning/Machine Learning算法笔记——判别模型与生成模型.html</id>
    <published>2017-01-03T10:43:52.000Z</published>
    <updated>2017-03-14T07:30:12.275Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;本博文简单介绍了判别模型与生成模型的定义、优缺点以及相互之间的关系。<br><a id="more"></a></p>
<h3 id="生成模型和判别模型的定义"><a href="#生成模型和判别模型的定义" class="headerlink" title="生成模型和判别模型的定义"></a>生成模型和判别模型的定义</h3><p>&emsp;&emsp;监督学习的任务就是从数据中学习一个模型（也叫分类器），对给定的输入<strong>X</strong>预测相应的输出<strong>Y</strong>。决策函数为<strong>Y=<em>f</em>(X)</strong>或者条件概率分布<strong>P(Y|X)</strong>。实际上通过条件概率分布<strong>P(Y|X)</strong>进行预测也是隐含着表达成决策函数<strong>Y=<em>f</em>(X)</strong>的形式的。<br>&emsp;&emsp;监督学习方法分为<strong>生成方法</strong>（Generative approach）和<strong>判别方法</strong>（Discriminative approach），相应的机器学习模型分别称为<strong>生成模型</strong>（Generative Model）和<strong>判别模型</strong>（Discriminative Model）：</p>
<ul>
<li><strong>判别模型</strong>：由数据直接学习决策函数Y=f(X)或者条件概率分布P(Y|X)作为预测的模型，即判别模型。<strong>基本思想</strong>是有限样本条件下建立判别函数，不考虑样本的产生模型，直接研究预测模型。典型的判别模型包括<strong>k近邻</strong>，<strong>感知级</strong>，<strong>决策树</strong>，<strong>支持向量机</strong>等。</li>
<li><strong>生成模型</strong>：由数据学习联合概率密度分布<strong>P(X,Y)</strong>，然后求出条件概率分布<strong>P(Y|X)</strong>作为预测的模型，即生成模型：<strong>P(Y|X)= P(X,Y)/ P(X)</strong>。基本思想是首先建立样本的联合概率概率密度模型<strong>P(X,Y)</strong>，然后再得到后验概率<strong>P(Y|X)</strong>，再利用它进行分类。注意是先求<strong>P(X,Y)</strong>才得到<strong>P(Y|X)</strong>的，这个过程还要先求出<strong>P(X)</strong>。<strong>P(X)</strong>就是你的训练样本数据的概率分布。当数据样本非常多时，得到的<strong>P(X)</strong>才能很好的描述你数据真正的分布。典型的生成模型有：<strong>朴素贝叶斯</strong>和<strong>隐马尔科夫模型</strong>等。<h3 id="生成模型和判别模型的优缺点"><a href="#生成模型和判别模型的优缺点" class="headerlink" title="生成模型和判别模型的优缺点"></a>生成模型和判别模型的优缺点</h3>&emsp;&emsp;<strong>生成模型</strong>的特点：</li>
</ul>
<ul>
<li>生成方法学习联合概率密度分布<strong>P(X,Y)</strong>，所以可以从统计学的角度表示数据的分布情况，能够反映同类数据本身的相似度，但它不关心到底划分各类的分类边界在哪；生成方法可以还原出联合概率分布P(Y|X)，而判别方法不能；</li>
<li>生成方法的学习收敛速度更快，即当样本容量增加的时候，学习模型可以更快的收敛于真实模型，当存在隐变量时，仍可以用生成方法学习。此时判别方法就不能用。</li>
</ul>
<p>&emsp;&emsp;<strong>判别模型</strong>的特点：</p>
<ul>
<li>判别模型直接学习决策函数<strong>Y=<em>f</em>(X)</strong>或者条件概率分布<strong>P(Y|X)</strong>，因此不能反映训练数据本身的特性；</li>
<li>但它寻找不同类别之间的最优分类面，反映的是异类数据之间的差异。</li>
<li>直接面对预测，往往学习的准确率更高。</li>
<li>由于直接学习<strong>P(Y|X)</strong>或<strong>P(X)</strong>，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。</li>
</ul>
<h3 id="生成模型和判别模型的联系"><a href="#生成模型和判别模型的联系" class="headerlink" title="生成模型和判别模型的联系"></a>生成模型和判别模型的联系</h3><p>&emsp;&emsp;<strong>由生成模型可以得到判别模型，但由判别模型得不到生成模型</strong>。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;本博文简单介绍了判别模型与生成模型的定义、优缺点以及相互之间的关系。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yaodong.ml/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning算法笔记——特征工程</title>
    <link href="http://yaodong.ml/machine-learning/Machine%20Learning%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B.html"/>
    <id>http://yaodong.ml/machine-learning/Machine Learning算法笔记——特征工程.html</id>
    <published>2017-01-03T08:48:47.000Z</published>
    <updated>2017-03-14T07:30:00.897Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;<strong>特征工程（Feature Engineering）</strong>包括特征构建(<strong>Construction</strong>)、特征提取(<strong>Extraction</strong>)、特征选择(<strong>Selection</strong>)三个部分。本博文简单记录了特征工程的相关知识和实战应用经验。<br><a id="more"></a></p>
<blockquote>
<p><strong>Feature engineering</strong> is the process of using domain knowledge of the data to create features that make machine learning algorithms work. Feature engineering is fundamental to the application of machine learning, and is both difficult and expensive. The need for manual feature engineering can be obviated by automated feature learning<br><strong>特征工程</strong>是利用数据科学领域的相关知识来创建、提取、选择能使机器学习算法达到最佳性能的特征的过程。</p>
</blockquote>
<h3 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h3><p>&emsp;&emsp;特征选择，即从特征集合中挑选一组最具统计意义的特征子集，以提高机器学习算法的性能表现，并达到数据降维的效果。通常需要衡量单独每个特征与类别标签之间的相关性。实际机器学习应用中，表示单个特征与类别标签之间相关关系的参数指标有：<strong>皮尔逊相关系数</strong>、<strong>信息增益</strong>、<strong>信息增益比</strong>和<strong>基尼指数</strong>等。</p>
<h4 id="皮尔逊相关系数"><a href="#皮尔逊相关系数" class="headerlink" title="皮尔逊相关系数"></a>皮尔逊相关系数</h4><p>&emsp;&emsp;两个变量（特征与标签向量）之间的pearson相关系数定义为两个变量之间的协方差和标准差的商。计算公式为：</p>
<script type="math/tex; mode=display">\gamma^2_{xy}=\dfrac{cov(x,y)}{\sigma_x\sigma_y}=\dfrac{E[(X-\mu_x)(Y-\mu_y)]}{\sigma_x\sigma_y}</script><p>&emsp;&emsp;这里的$x$表示某个特征的观测值，$Y$表示类别标签。pearson相关系数的取值在0到1之间。</p>
<h4 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h4><p>&emsp;&emsp;信息增益的概念来源于信息科学的分支。<strong>熵</strong>（entropy）是随机变量不确定性的度量。熵越大，表示随机变量的不确定性就越大。<br>设随机变量X为有限个值的离散随机变量，其概率分布为</p>
<script type="math/tex; mode=display">P(X=x_i)=p_i</script><p>&emsp;&emsp;熵的定义为</p>
<script type="math/tex; mode=display">H(X)=-\sum_{i=1}^np_ilog(p_i)</script><p>&emsp;&emsp;条件熵：H(Y|X)表示已知随机变量X的条件下随机变量Y的不确定性，定义<script type="math/tex">H(Y|X)=\sum_{i=1}^np_iH(Y|X=x_i)</script><br>&emsp;&emsp;其中$p_i=P(X=x_i)$。这里X表示样本数据集的某个特征，即表示根据某个特征划分后，数据Y的熵。如果某个特征有更强的分类能力，则条件熵$H(Y|X)$越小，表示不确定性越小。</p>
<h4 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h4><p>&emsp;&emsp;信息增益定义为特征A对训练数据集D的信息增益 $g(D,A)$ 定义为集合D的经验熵 $H(D)$ 与特征A在给定条件下D的经验条件熵$H(D|A)$之差，即<script type="math/tex">g(D,A)=H(D)-H(D|A)</script><br> &emsp;&emsp;信息增益$g(D,A)$表示特征A使得对数据集D的分裂的不确定性减少的程度。所以信息增益越大，表明不确定性减小越多，即特征具有更强的分类能力。<br>&emsp;&emsp;根据信息增益准则的特征选择方法是：对训练数据集（或其子集）D,计算每个特征的信息增益，并比较其大小将其排序，选择最大的信息增益对应的特征。</p>
<h4 id="信息增益比"><a href="#信息增益比" class="headerlink" title="信息增益比"></a>信息增益比</h4><p>&emsp;&emsp;信息增益比也是度量特征分类能力的方法。特征A对训练数据集D的信息增益比$g_R(D,A)$定义为其信息增益与训练数据集D关于特征A的熵之比，即</p>
<script type="math/tex; mode=display">g_R(D,A)=\frac{g(D,A)}{H_A(D)}</script><p>&emsp;&emsp;其中</p>
<script type="math/tex; mode=display">H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}</script><p>&emsp;&emsp;$|D|$表示训练样本集D中样本数量，$|D_i|$表示训练数据D中特征A取第i个值的总数目。信息增益比越大，表明特征分类能力越强。<br>&emsp;&emsp;需要注意的是，<strong>以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题</strong>。</p>
<h4 id="基尼指数"><a href="#基尼指数" class="headerlink" title="基尼指数"></a>基尼指数</h4><p>&emsp;&emsp;基尼指数表示样本集合的不确定性程度，基尼指数越小，对应的特征分类能力越强。</p>
<h3 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h3><p>&emsp;&emsp;原则上讲，特征提取应该在特征选择之前。特征提取的对象是未经处理的原始数据（raw data），它的目的是自动地构建新的特征，将原始数据转换为一组具有明显物理现实意义或者统计意义或核的特征。实际的机器学习应用中，常见的特征提取的方法有：</p>
<ul>
<li><strong>PCA</strong>（Principal Component Analysis，主成分分析）</li>
<li><strong>ICA</strong> （Independent component analysis，独立成分分析）</li>
<li><strong>LDA</strong> （Linear Discriminant Analysis，线性判别分析）</li>
</ul>
<h3 id="特征构建"><a href="#特征构建" class="headerlink" title="特征构建"></a>特征构建</h3><p>&emsp;&emsp;特征构建指的是结合所研究问题的实际背景从原始数据中人工构建新的特征。这一步需要花大量的时间和精力去研究真实的数据，思考问题的潜在形式和数据结构，同时能够更好地应用到预测模型中。<br>&emsp;&emsp;特征构建需要很强的洞察力和分析能力，从原始数据中找出具有物理意义的特征，并将其处理成一个或一组新的特征，便于应用到机器学习算法模型中。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>&emsp;&emsp;特征工程包括特征提取、特征构建和特征选择这三个子问题。在实际的机器学习应用中，每一个步骤都很重要。将这三个子问题的重要性排序为：<script type="math/tex">特征构建>特征提取>特征选择</script><br>如果特征构建做的不好，则会直接影响特征提取，进而影响了特征选择，最终影响机器学习算法模型的性能表现。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;&lt;strong&gt;特征工程（Feature Engineering）&lt;/strong&gt;包括特征构建(&lt;strong&gt;Construction&lt;/strong&gt;)、特征提取(&lt;strong&gt;Extraction&lt;/strong&gt;)、特征选择(&lt;strong&gt;Selection&lt;/strong&gt;)三个部分。本博文简单记录了特征工程的相关知识和实战应用经验。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Machine Learning" scheme="http://yaodong.ml/tags/Machine-Learning/"/>
    
      <category term="机器学习" scheme="http://yaodong.ml/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning算法笔记——决策树</title>
    <link href="http://yaodong.ml/machine-learning/Machine%20Learning%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95.html"/>
    <id>http://yaodong.ml/machine-learning/Machine Learning算法笔记——决策树算法.html</id>
    <published>2017-01-03T04:54:14.000Z</published>
    <updated>2017-04-13T05:32:05.345Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;本博文介绍了机器学习中的决策树算法。<br><a id="more"></a></p>
<h2 id="决策树-Decision-Tree"><a href="#决策树-Decision-Tree" class="headerlink" title="决策树(Decision Tree)"></a>决策树(Decision Tree)</h2><p>&emsp;&emsp;决策树是一种基本的分类与回归算法，属于<strong>贪婪算法</strong>，其模型呈现为树形结构，可理解为基于特征或模型属性对实例进行分类或回归的过程<br>&emsp;&emsp;决策树的特点：</p>
<ul>
<li><strong>优点</strong>：计算复杂度不高，输出结果可直观理解数据，对中间值得缺失不敏感，可以处理不相关特征数据</li>
<li><strong>缺点</strong>：可能会出现Over Fittting</li>
<li>适用数据类型：数值型和标称型</li>
</ul>
<p>&emsp;&emsp;决策树的路径及其对应的<code>if-then</code>重要性质：路径之间是互斥且完备的。也就是说，每一个实例都被决策树的一条路径覆盖，且只能被一条路径或者一条规则覆盖。这里的覆盖是指实例的特征与路径上的特征或实例满足规则的条件</p>
<h3 id="决策树模型"><a href="#决策树模型" class="headerlink" title="决策树模型"></a>决策树模型</h3><p>&emsp;&emsp;决策树模型学习过程可分为3个步骤：<strong>特征选择</strong>，<strong>决策树的生成</strong>，<strong>决策树的修剪</strong></p>
<h4 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h4><p>&emsp;&emsp;构建决策树，实质是对训练数据集进行超平面划分，不同的样本特征在划分数据集时重要性不同，因此选择特征顺序的不同将会生成不同的决策树。为使数据集的分类结果更纯净，更能直观表达数据的本质属性，构造决策树之前先评估不同特征的重要性。</p>
<h3 id="决策树生成算法"><a href="#决策树生成算法" class="headerlink" title="决策树生成算法"></a>决策树生成算法</h3><p>&emsp;&emsp;<strong>ID3</strong>与<strong>C4.5</strong>都是决策树的经典分类决策树算法。<strong>ID3</strong>算法与<strong>C4.5</strong>算法的不同之处在于ID3算法采用信息增益作为特征选择准则，而C4.5采用的是信息增益比作为准则</p>
<h4 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a>ID3算法</h4><p>&emsp;&emsp;<strong>ID3</strong>算法的核心是在决策树的各个节点上应用信息增益准则选择特征，递归地构建决策树。<br>&emsp;&emsp;<strong>ID3</strong>算法的实质是用最大似然法进行概率模型的选择。算法思路为：</p>
<ul>
<li>从根节点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子节点</li>
<li>再对子结点递归调用以上方法，构建决策树。直到所有特征的信息增益均很小或没有特征可以选择为止。</li>
</ul>
<h4 id="C4-5算法"><a href="#C4-5算法" class="headerlink" title="C4.5算法"></a>C4.5算法</h4><p>&emsp;&emsp;<strong>ID3</strong>算法由于只有树的生成，所以该算法生成的树容易产生过拟合。<strong>C4.5</strong>算法对<strong>ID3</strong>算法进行了改进，选用信息增益比作为特征选择准则。</p>
<h3 id="决策树的剪枝"><a href="#决策树的剪枝" class="headerlink" title="决策树的剪枝"></a>决策树的剪枝</h3><p>&emsp;&emsp;用决策树生成算法递归产生决策树，容易出现过拟合，原因在于决策树的生成过程过多考虑如何提高对训练数据的正确分类，从而构建的决策树趋于复杂<br>&emsp;&emsp;在决策树学习中，将已生成的树进行简化的过程称为剪枝，具体来说就是剪掉一些子树或叶子结点，并将其根节点或父结点作为新的叶子结点，进而简化决策树模型<br>&emsp;&emsp;决策树的剪枝通过极小化决策树整体的损失函数（代价函数）来实现，即<script type="math/tex">C_\alpha(T)=C(T)+\alpha|T|</script></p>
<h4 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a>CART算法</h4><p>&emsp;&emsp;<strong>CART</strong>算法是一种既可以用于分类也可用作回归的决策树算法。<strong>CART</strong>算法分为以下两步：</p>
<ol>
<li>决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大</li>
<li>决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，此过程中，用损失函数最小化作为剪枝的标准</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;本博文介绍了机器学习中的决策树算法。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Machine Learning" scheme="http://yaodong.ml/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>不忘初心，未来可期——2017启笔</title>
    <link href="http://yaodong.ml/writings/%E4%B8%8D%E5%BF%98%E5%88%9D%E5%BF%83%EF%BC%8C%E6%9C%AA%E6%9D%A5%E5%8F%AF%E6%9C%9F%E2%80%94%E2%80%942017%E5%90%AF%E7%AC%94.html"/>
    <id>http://yaodong.ml/writings/不忘初心，未来可期——2017启笔.html</id>
    <published>2017-01-01T14:39:05.000Z</published>
    <updated>2017-03-14T06:53:14.794Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;博客好久没更了，感觉一直都在碌碌无为地忙碌着。毫无准备地跨进了2017，哦，本命年。坐在这里也不知道该写点什么，过去的一年里有遗憾，有后悔，一切都回不去了。<br>&emsp;&emsp;2017，不忘初心，不辜负。<br><a id="more"></a><br>&emsp;&emsp;总感觉新年和圣诞节是不一样的。圣诞节是西方的一个宗教节日，国人大概只是给狂欢冠以堂皇之名吧。一直在寻找一个时间节点，去重启自己的生活节奏，1月1日新年伊始，这个time point不能再合适。嗯，过往的二十年里一直都是这样拖延和给自己找借口，大多杰出的人总能从当下开始，而我，总要拖到下一个自己设定的时间点。<br>&emsp;&emsp;不知道习惯性三分钟热度的自己，十天半个月之后会不会已经忘了自己写过这篇博客，忘记博客中吹过的牛逼。或者，未来的某个时间，觉得这篇博客在打脸，偷偷地删掉博客也是有可能的哦。<br>&emsp;&emsp;过去这一年都干了什么？脑海里好像已经没有太多值得回忆的片段，过去的一年里，没有上进心，没有太多生活的动力。似乎还没有从保研的那一年的生活节奏中走出来。<strong>保研后遗症好严重，嗯，把自己懒和不努力的锅甩给了保研</strong>。<br>&emsp;&emsp;仔细想来，能第一时间浮现在脑海的应该也是大喜大悲过的吧。1月份时研一考试周天天泡纪忠楼206复习看书，期末考试没考好，好几门课的成绩渣到爆，后来因为学位课成绩很差而不开心好一阵子。从研一下开始已经决意脱离通信，入了计算机的坑。研一下天天看教程学Java，现在撸代码的速度已经可以和单身贵族的手速相媲美了。研一时的冲动和不成熟导致跟女朋友分手，之后的整个研一下都在浑浑噩噩中度过，那段时间算是读研以来最灰暗的日子了，从那之后研一剩下的几个月都发生了什么，我都没有太多记忆了。从那时才知道网易云音乐每首歌的评论列表都是一个个伤心的故事，也一天天地单曲循环<code>我不愿让你一个人</code>、<code>只是没有如果</code>、<code>走着走着就散了</code>；也在从那时开始养成了一个人打篮球的习惯，到现在还能保持空位投篮60%以上的命中率。校运动会时跟范特、柳旭一起组队代表学院参加了1分钟投篮大赛，也是从那时开始，三人变老铁。大概是因为研一下跟大师姐经常一起上课吧，上课经常性跑神，大师姐会经常给灌鸡汤，就这样后来和大师姐也成为了很好的朋友。感谢你们一直都在，昨天还一起跨年，特哥，旭哥，大师姐，新年快乐！<br>&emsp;&emsp;研一结束时搬回四牌楼校区，在九龙湖校区的一年基本没有留下太多回忆，更多的是不甘。下半年的生活似乎也没有太多波澜了：每个月按时给导师交学习报告、刷算法刷leetcode、看看机器学习知识、水一水比赛。暑假基本没回家，7月份主要在刷数据结构和算法，用Github+Hexo搭建了博客；8月份主要学了机器学习的理论基础和Java Web。9月中旬左右被导师派往苏州出差到现在。平时除了做导师的通信项目之外，一天天都在瞎折腾，机器学习、算法、Java、Python、数据比赛、Linux、网络爬虫…，也没学明白个所以然来。研二上这半年的黄金时间又被自己霍霍没了，WTF。12月份把微信朋友圈停用了，转向<a href="http://weibo.com/u/2685489433" target="_blank" rel="external">微博</a>的怀抱。会不会，朋友圈是下一个QQ空间，微博又是下一个朋友圈…<br>&emsp;&emsp;现在总结来，在过去的一年里，应该是在不断交学费的过程中更成熟了吧：不再高傲自大；多一点耐心，不要对自己爱的人发脾气；做决定先考虑后果，任何时候不要把事情搞成无法挽回的局面。</p>
<hr>
<p>&emsp;&emsp;今天是2017年第一天，新的一年祝看到博客的大家天天好心情，校招季拿到心仪的offer！<br>&emsp;&emsp;深夜坐在电脑前写这篇博客，依然会有20多年一如既往的那种“放假回家我要把书都带上，回家好好学习”的既视感。新的一年，总该给自己定几个小目标，哪怕实现不了装装逼也是好的。<br>&emsp;&emsp;背景音乐是<code>who is fancy</code>的<strong><code>goodbye</code></strong>，音乐荒，求推荐好听的歌曲，语种不限。<br>&emsp;&emsp;晚安！</p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=30706076&auto=1&height=66"></iframe>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;博客好久没更了，感觉一直都在碌碌无为地忙碌着。毫无准备地跨进了2017，哦，本命年。坐在这里也不知道该写点什么，过去的一年里有遗憾，有后悔，一切都回不去了。&lt;br&gt;&amp;emsp;&amp;emsp;2017，不忘初心，不辜负。&lt;br&gt;
    
    </summary>
    
      <category term="随笔" scheme="http://yaodong.ml/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
  </entry>
  
  <entry>
    <title>算法设计思想之分支界限</title>
    <link href="http://yaodong.ml/algorithms/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3%E4%B9%8B%E5%88%86%E6%94%AF%E7%95%8C%E9%99%90.html"/>
    <id>http://yaodong.ml/algorithms/算法设计思想之分支界限.html</id>
    <published>2016-12-15T08:13:30.000Z</published>
    <updated>2017-03-14T07:30:38.507Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;本博文记录了博主关于分支界限算法设计思想的学习总结。<br><a id="more"></a><br>&emsp;&emsp; 类似于回溯法，<strong>分支限界法（branch-and-bound method）</strong>也是一种在问题的解空间树T上搜索问题解的算法。但分支限界法的求解目标则是找出满足约束条件的一个解，或是在满足约束条件的解中找出使某一目标函数值达到极大或极小的解，即在某种意义下的最优解。<br>&emsp;&emsp;<strong>分支限界法（branch-and-bound method）</strong>， “<strong>分支</strong>”是指采用广度优先遍历的策略，依次生成扩展结点的所有分支（即：儿子结点）；“<strong>限界</strong>”是在结点扩展过程中，计算结点的上界（或下界），并根据“剪枝函数”剪去搜索树的某些分支，从而提高搜索效率。</p>
<h3 id="分支限界算法的基本思想"><a href="#分支限界算法的基本思想" class="headerlink" title="分支限界算法的基本思想"></a>分支限界算法的基本思想</h3><p>&emsp;&emsp;按照广度优先遍历的原则，一个活结点一旦成为扩展结点（E-结点）R后，算法将依次生成它的全部孩子结点，将那些导致不可行解或导致非最优解的儿子舍弃，其余儿子加入活结点表中。然后，从活结点表中取出一个结点作为当前扩展结点。重复上述结点扩展过程，直至找到问题的解或判定无解为止。</p>
<h3 id="常见的分支限界法"><a href="#常见的分支限界法" class="headerlink" title="常见的分支限界法"></a>常见的分支限界法</h3><h4 id="FIFO分支限界法-队列式分支限界法"><a href="#FIFO分支限界法-队列式分支限界法" class="headerlink" title="FIFO分支限界法(队列式分支限界法)"></a>FIFO分支限界法(队列式分支限界法)</h4><p>&emsp;&emsp;基本思想：按照队列先进先出(FIFO)原则选取下一个活结点为扩展结点。<br>&emsp;&emsp;搜索策略：一开始，根结点是唯一的活结点，根结点入队。从活结点队中取出根结点后，作为当前扩展结点。对当前扩展结点，先从左到右地产生它的所有儿子，用约束条件检查，把所有满足约束函数的儿子加入活结点队列中。再从活结点表中取出队首结点（队中最先进来的结点）为当前扩展结点，……，直到找到一个解或活结点队列为空为止。</p>
<h4 id="Least-Cost分支限界法-优先队列式分支限界法"><a href="#Least-Cost分支限界法-优先队列式分支限界法" class="headerlink" title="Least Cost分支限界法(优先队列式分支限界法)"></a>Least Cost分支限界法(优先队列式分支限界法)</h4><p>&emsp;&emsp;基本思想：为了加速搜索的进程，应采用有效地方式选择活结点进行扩展。按照优先队列中规定的优先级选取优先级最高的结点成为当前扩展结点。<br>&emsp;&emsp;搜索策略：对每一活结点计算一个优先级（某些信息的函数值），并根据这些优先级；从当前活结点表中优先选择一个优先级最高（最有利）的结点作为扩展结点，使搜索朝着解空间树上有最优解的分支推进，以便尽快地找出一个最优解。再从活结点表中下一个优先级别最高的结点为当前扩展结点，……，直到找到一个解或活结点队列为空为止。</p>
<h3 id="分支限界算法与回溯法的区别"><a href="#分支限界算法与回溯法的区别" class="headerlink" title="分支限界算法与回溯法的区别"></a>分支限界算法与回溯法的区别</h3><p>&emsp;&emsp;分支限界算法与回溯法都是在所给定问题的解空间树上搜索问题的解的算法。但二者也有一些不同之处：</p>
<ul>
<li><strong>算法目标不同</strong>：回溯算法的目的是找出解空间树中满足约束条件的所有解，而分支限界法的求解目标则是找出满足约束条件的一个解，或是在满足约束条件的条件下找出在某种意义下的最优解。 </li>
<li><strong>搜索方式的不同</strong>：回溯法以深度优先遍历的方式搜索解空间树，而分支限界法则以广度优先遍历或以最小耗费优先的方式搜索解空间树</li>
</ul>
<h3 id="分支界限的应用"><a href="#分支界限的应用" class="headerlink" title="分支界限的应用"></a>分支界限的应用</h3><ol>
<li>队列式分支限界法：按照队列先进先出（FIFO）原则选取下一个结点为扩展结点。 </li>
<li>优先队列式分支限界法：按照优先队列中规定的优先级选取优先级最高的结点成为当前扩展结点。</li>
<li>单源最短路径问题</li>
<li>装载问题、批处理作业问题、布线问题</li>
<li>0-1背包问题</li>
<li>旅行售货员问题</li>
<li>栈式搜索方法，按照FILO</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;本博文记录了博主关于分支界限算法设计思想的学习总结。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="算法" scheme="http://yaodong.ml/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="算法设计" scheme="http://yaodong.ml/tags/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>算法设计思想之回溯算法</title>
    <link href="http://yaodong.ml/algorithms/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3%E4%B9%8B%E5%9B%9E%E6%BA%AF%E7%AE%97%E6%B3%95.html"/>
    <id>http://yaodong.ml/algorithms/算法设计思想之回溯算法.html</id>
    <published>2016-12-15T08:12:20.000Z</published>
    <updated>2017-03-14T07:30:53.607Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;本博文记录了博主关于回溯算法设计思想的学习总结。<br><a id="more"></a></p>
<blockquote>
<p><strong>回溯（Backtracking）算法</strong>也叫试探法，属于暴力求解的范畴。<strong>回溯（Backtracking）算法</strong>是一种既有系统性又有跳跃性的的搜索算法，适用于求解具有约束条件，并且有多个候选解的问题。</p>
</blockquote>
<h3 id="回溯算法的设计思想"><a href="#回溯算法的设计思想" class="headerlink" title="回溯算法的设计思想"></a>回溯算法的设计思想</h3><p>&emsp;&emsp;<strong>回溯算法</strong>采用试探的思想，尝试分步解决一个问题。在分步解决问题的过程中，当它通过尝试发现现有的分步答案不能得到有效的正确的解答的时候，它将取消上一步甚至是上几步的计算，再通过其它的可能的分步解答再次尝试寻找问题的答案。</p>
<p>&emsp;&emsp;<strong>回溯算法</strong>在包含问题的所有解的解空间树中，按照深度优先遍历的策略，从根结点出发搜索整个解空间树。<br>&emsp;&emsp;当算法搜索至解空间树的任一结点时，先判断该结点是否肯定不包含问题的解。如果肯定不包含，则跳过对以该结点为根的子树的所有搜索，病逐层向其祖先结点回溯。否则，进入该子树，继续按深度优先遍历的策略进行搜索。<br>&emsp;&emsp;<strong>回溯算法</strong>通常用最简单的递归方法来实现，反复重复上述的步骤后会出现两种情况：</p>
<ol>
<li>找到了符合要求的正确答案</li>
<li>遍历了所有可能的分步方法后，宣告该问题没有符合要求的解</li>
</ol>
<p>&emsp;&emsp;回溯法是设计递归过程的一种重要方法，回溯算法的实质是先序遍历一颗状态树的过程，只不过这棵状态树不是遍历前预先建立的，而是隐含在遍历过程中。</p>
<h3 id="回溯算法的相关概念"><a href="#回溯算法的相关概念" class="headerlink" title="回溯算法的相关概念"></a>回溯算法的相关概念</h3><h4 id="约束函数-amp-限界函数"><a href="#约束函数-amp-限界函数" class="headerlink" title="约束函数&amp;限界函数"></a>约束函数&amp;限界函数</h4><p>&emsp;&emsp;约束函数可根据所求解问题中的限制条件构造。约束函数描述了给定问题的合法解的一般特征，用于DFS深度优先遍历过程中去除不合法的解，从而避免无效搜索。此外，约束函数是对于任何状态空间树上的节点都有效、等价的。</p>
<h4 id="状态空间树"><a href="#状态空间树" class="headerlink" title="状态空间树"></a>状态空间树</h4><p>&emsp;&emsp;状态空间树是对问题的所有解的图形描述。树上的每个子节点的解都只有一个部分与父节点不同。</p>
<h4 id="扩展节点、活结点、死结点"><a href="#扩展节点、活结点、死结点" class="headerlink" title="扩展节点、活结点、死结点"></a>扩展节点、活结点、死结点</h4><p>&emsp;&emsp;扩展节点，是当前正在求出它的子节点的节点，在DFS中，只允许有一个扩展节点。<br>&emsp;&emsp;节点本身和其父节点满足约束函数和限界条件的结点称为活结点。活结点需要进行DFS递归遍历<br>&emsp;&emsp;死结点反之，死结点是不满足约束函数的结点，DFS过程不必遍历死结点的子节点。</p>
<h3 id="回溯算法的求解步骤"><a href="#回溯算法的求解步骤" class="headerlink" title="回溯算法的求解步骤"></a>回溯算法的求解步骤</h3><ul>
<li>针对所给问题，定义问题的解空间</li>
<li>确定易于搜索的解空间结构</li>
<li>构造约束函数和限界函数，避免冗余的无效搜索</li>
<li>以深度优先遍历（<strong>DFS</strong>）搜索解空间树，并在搜索过程中用剪枝函数避免无效搜索</li>
</ul>
<h3 id="回溯算法的适用情形"><a href="#回溯算法的适用情形" class="headerlink" title="回溯算法的适用情形"></a>回溯算法的适用情形</h3><ol>
<li>给定的问题有很多组解，要求寻找问题的解集或者寻找满足某些约束条件的最佳解时，可以考虑使用回溯法</li>
<li>回溯算法虽然属于暴力求解范畴，但是回溯思想能避免很多不必要的穷举式搜索。因此回溯算法适用于求解一些组合数很大的问题</li>
<li>回溯算法的终止条件要注意以下两种情况：</li>
</ol>
<ul>
<li>回溯算法用于求问题的所有解时，要回溯到解状态树的根结点，且根结点的所有子树都已被搜索遍才结束</li>
<li>回溯算法用于求问题的任一解时，只要搜索到问题的一个解就可以结束</li>
</ul>
<h3 id="回溯算法的应用"><a href="#回溯算法的应用" class="headerlink" title="回溯算法的应用"></a>回溯算法的应用</h3><ul>
<li>八皇后问题</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;本博文记录了博主关于回溯算法设计思想的学习总结。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="算法" scheme="http://yaodong.ml/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="算法设计" scheme="http://yaodong.ml/tags/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/"/>
    
  </entry>
  
</feed>
