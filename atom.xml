<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jellyyoung&#39;s Blog</title>
  <subtitle>Practice makes me progressive</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yaodong.ml/"/>
  <updated>2017-03-21T06:41:46.696Z</updated>
  <id>http://yaodong.ml/</id>
  
  <author>
    <name>摇摇果冻</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>机器学习算法笔记——类别不均衡问题</title>
    <link href="http://yaodong.ml/machine-learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E7%B1%BB%E5%88%AB%E4%B8%8D%E5%9D%87%E8%A1%A1%E9%97%AE%E9%A2%98.html"/>
    <id>http://yaodong.ml/machine-learning/机器学习算法笔记——类别不均衡问题.html</id>
    <published>2017-01-22T06:24:56.000Z</published>
    <updated>2017-03-21T06:41:46.696Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;类别不均衡（class-imbalance）指的是机器学习的分类任务中不同类别的训练样本样例数目差别很大的情况。为了使算法模型达到更好的分类效果，有必要解决该问题。<br><a id="more"></a><br>&emsp;&emsp;在现实中有很多类别不均衡问题，它是常见的，并且也是合理的，符合人们期望的。例如在欺诈交易识别中，属于欺诈交易的应该是很少部分，即绝大部分交易是正常的，只有极少部分的交易属于欺诈交易。一般来说，如果类别不平衡比例超过4:1，那么分类模型极有可能因为数据类别不平衡而无法达到很好地分类性能。因此在构建分类模型之前，需要对类别不均衡性问题进行处理。<br>&emsp;&emsp;以常见的二分类问题为例，设样本数据集中的正例样本数目为$m^-$ ,反例样本数目为$m^+$。</p>
<h3 id="扩大数据集"><a href="#扩大数据集" class="headerlink" title="扩大数据集"></a>扩大数据集</h3><p>&emsp;&emsp;更多的数据往往战胜更好的算法。遇到类别不均衡问题时，首先应该想到：是否可以增加数据集的规模，因为更大规模的数据意味着更多的信息。</p>
<h3 id="采样（随机采样）"><a href="#采样（随机采样）" class="headerlink" title="采样（随机采样）"></a>采样（随机采样）</h3><p>&emsp;&emsp;可以使用采样（Sampling）策略来改善样本数据集的类别不均衡的成都。主要有以下两种采样方法来降低数据集的样本类别不均衡性：</p>
<ol>
<li>对样本数目较少的一类样本进行采样，增加该类样本的数目，进而提高该类样本在样本数据集中占的比例，这种采样方法称为<strong>过采样（oversampling），也叫上采样</strong>。需要注意的是，过采样法不能简单地对样本数目较少的类别样本进行直接重复采样，否则会招致严重的过拟合。为了解决这一问题，可以在每次生成新数据样本时加入轻微的随机扰动，经验表明这种方法可以有效提高模型的泛化性能。例如，过采样方法的代表性算法——SMOTE算法，就是通过对训练集的正例进行插值来产生额外的正例。</li>
<li>对样本数目较多的一类样本进行采样，即去除一些样本数目较多的类的样本，进而降低该类样本在样本数据集中所占的比例，使采样之后的数据集中正、反类样本数目接近，这种采样方法称为<strong>欠采样（undersampling），也叫下采样</strong>。<br>欠采样的代表性算法是集成学习（Ensemble Learning）机制，将样本数目较多的类别样本划分为多个集合供不同“弱分类器”使用，这样相当于对每个学习器都进行了欠采样，但在全局看却没有丢失过多的信息。</li>
</ol>
<p>&emsp;&emsp;随机采样最大的优点是简单易操作，但缺点也很明显：上采样后的数据集中会反复出现一些重复样本，训练出来的模型会倾向于过拟合；而下采样会导致丢失部分数据，使得算法模型不能完全学习到数据的总体模式。显然，下采样过程会丢失信息，如何解决这个问题？<br>&emsp;&emsp;<strong>Informed undersampling</strong>采样技术可以解决随机欠采样造成的数据信息丢失问题，<strong>informed undersampling</strong>采样技术主要有两种方法分别是<strong>EasyEnsemble</strong>算法和<strong>BalanceCascade</strong>算法。<br>&emsp;&emsp;EasyEnsemble算法,类似于Ensemble learning的Bagging方法，它把数据集$D$划分为两部分，分别是多数类别的样本和少数类别的样本，对于多数类样本$D<em>{max}$，通过$n$次有放回抽样生成$n$份子集，少数类样本分别和这$n$份样本合并训练一个模型，这样可以得到$n$个模型，最终的模型是这$n$个模型预测结果的平均值。<br>&emsp;&emsp;BalanceCascade算法是一种级联算法，该算法从多数类别样本集$S</em>{min}$中有效地选择N且满足$\mid N\mid=\mid D<em>{min}\mid$，将N和$D</em>{min}$合并为新的数据集进行训练，利用Boosting的思想，首先对原始数据集进行一次下采样，生成新的子训练集，使用该数据集合训练一个分类器，对于那些分类正确的样本不放回，然后再下采样，再用新生成的数据集训练下一个分类器，一直迭代进行，最后组合所有分类的结果作为最后的结果。</p>
<h3 id="生成新数据"><a href="#生成新数据" class="headerlink" title="生成新数据"></a>生成新数据</h3><p>&emsp;&emsp;SMOTH算法，构造人工数据样本。SMOTE是一种过采样算法，它构造新的小类样本而不是产生小类中已有的样本的副本，即该算法构造的数据是新样本。SMOTE算法基于距离度量选择小类别下的两个或者更多的相似样本，然后选择其中一个样本，并随机选择一定数量的近邻样本对该样本的一个属性增加噪声，每次只处理一个属性。这样就构造了更多的新生数据样本。算法流程如下：<br>&emsp;&emsp;$Step 1$：随机选定少数类别中的一个样本$x^{(i)}$，以欧氏距离为标准计算它到少数类样本集$D_{min}$中所有样本的距离，得到与其距离度量最小的k个相似样本。<br>&emsp;&emsp;$Step 2$：根据类别不平衡比例设置一个采样比例阈值$T$以确定采样倍率$N$，对于每一个少数类样本$x$，从它的k个相邻样本中随机选择若干个样本，假设选择的近邻为$\hat{x}$。<br>&emsp;&emsp;$Step 3$：对于每一个随机选出的近邻$\hat{x}$，分别与原样本按照如下的公式构建新的样本：</p>
<script type="math/tex; mode=display">x_{new}=x+rand(0,1)\times(\hat{x}-x)</script><h3 id="类别不平衡问题的评价指标"><a href="#类别不平衡问题的评价指标" class="headerlink" title="类别不平衡问题的评价指标"></a>类别不平衡问题的评价指标</h3><script type="math/tex; mode=display">Precision = \frac{TP}{TP+FP}</script><script type="math/tex; mode=display">Recall=\frac {TP}{TP+FN}</script><script type="math/tex; mode=display">F-Measure=\frac {(1+\beta)^2\cdot Recall\cdot Precision}{\beta ^ 2\cdot Recall+Precision}</script><script type="math/tex; mode=display">G-Mean=\sqrt{\frac{TP}{TP+TN}\cdot {\frac {TN}{TN+FP}}}</script><h3 id="转换问题的思考角度"><a href="#转换问题的思考角度" class="headerlink" title="转换问题的思考角度"></a>转换问题的思考角度</h3><p>&emsp;&emsp;对于类别不平衡的二分类问题，可将其看做一分类问题（One Class Learning），或异常检测问题（Novelty Detection）。转换看待问题的角度后，解决问题的重点不再是通过捕捉不同类别样本之间的差别来分类，而是对其中一类样本进行建模，进而解决问题。</p>
<h3 id="尝试不同算法"><a href="#尝试不同算法" class="headerlink" title="尝试不同算法"></a>尝试不同算法</h3><p>&emsp;&emsp;对于分类问题，解决类别不均衡问题的另外一个思路是从算法的角度出发，考虑不同误分类情况代价的差异性对算法进行优化，使得我们的算法在不平衡数据下也能有较好的效果。<br>&emsp;&emsp;在算法层面上解决类别不平衡问题的方法主要是基于代价敏感学习算法（Cost-Sensitive Learning）。<br>&emsp;&emsp;代价敏感学习方法的核心要素是代价矩阵，实际生活中，不同类型的误分类情况导致的代价是不一样的，例如在医疗中，“将病人误疹为健康人”和“将健康人误疹为病人”的代价不同；在信用卡盗用检测中，“将盗用误认为正常使用”与“将正常使用识破认为盗用”的代价也不相同。对于二分类问题，定义代价矩阵。标记$C<em>{ij}$为将类别$j$误分类为类别$i$的代价，显然$C</em>{00}=C<em>{11}=0$，但$C</em>{01}$和$C_{10}$为两种不同的误分类代价，当两者相等时为代价不敏感的学习问题。二者不等时即为代价敏感学习问题。<br>&emsp;&emsp;基于代价矩阵$C$的分析，代价敏感学习方法主要有以下三种实现方式，分别是：</p>
<ol>
<li>从学习模型出发，着眼于对某一具体学习方法的改造，使之能适应不平衡数据下的学习，研究者们针对不同的学习模型如感知机，支持向量机，决策树，神经网络等分别提出了其代价敏感的版本。以代价敏感的决策树为例，可从以下方面对其进行改进以适应不平衡数据的学习，分别是决策阈值的选择、分裂标准的选择、剪枝，在上述三个步骤中都可以将代价矩阵引入，即可解决类别不平衡问题。</li>
<li>从贝叶斯风险理论出发，把代价敏感学习看成是分类结果的一种后处理，按照传统方法学习到一个模型，以实现损失最小为目标对结果进行调整。此方法的优点在于它可以不依赖所用具体的分类器，但是缺点也很明显它要求分类器输出值为概率。</li>
<li>从预处理的角度出发，将代价用于权重的调整，使得分类器满足代价敏感的特性。</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;类别不均衡（class-imbalance）指的是机器学习的分类任务中不同类别的训练样本样例数目差别很大的情况。为了使算法模型达到更好的分类效果，有必要解决该问题。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Machine Learning" scheme="http://yaodong.ml/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>深度学习与浅层神经网络的不同之处</title>
    <link href="http://yaodong.ml/deep-learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%B8%8D%E5%90%8C%E4%B9%8B%E5%A4%84.html"/>
    <id>http://yaodong.ml/deep-learning/深度学习与浅层神经网络的不同之处.html</id>
    <published>2017-01-21T06:36:03.000Z</published>
    <updated>2017-03-21T06:44:18.188Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;本文根据自己对深度学习的基本理解，简单总结了深度学习与浅层神经网络的不同之处。<br><a id="more"></a></p>
<blockquote>
<p>深度学习又称深度神经网络，即包含多个隐藏层的神经网络。由于每个隐藏层都可以对上一层的输出进行非线性变换，因此深度神经网络拥有比浅层前馈式网络更加优异的表达能力，因此可以计算更多更复杂的特征信息。</p>
</blockquote>
<p>&emsp;&emsp;1. <strong>数据集规模</strong>。深度神经网路的隐层数量和隐层神经元数目都远大于传统的浅层神经网络，相应地，深度学习模型的复杂度也远高于浅层网络。从数据集规模的角度考虑，使用大规模的数据集对深度学习模型进行训练，可在一定程度上降低深度神经网络陷入过拟合的风险。<br>&emsp;&emsp;2. <strong>网络的训练速度</strong>。对神经网络进行训练的实质是基于梯度下降思路求解整体代价函数最小化时对应的参数。神经网络的训练速度与网络的参数的数目有直接关系，深度学习模型的隐层数目和隐层神经元数目都远多于浅层神经网络，因此深度神经网络的训练速度远小于浅层神经网络，训练时间远大于浅层神经网络。<br>&emsp;&emsp;3. <strong>网络的训练方式</strong>。浅层神经网络中最为广泛应用的训练算法是基于梯度下降的误差逆传播算法，但是多隐层深度网络一般难以直接用标准BP算法进行训练和求解。因为训练误差在多隐层内逆向传播时，在每一层都要乘以该层的激活函数的导数值，也就是说，误差在每一层传递都会不断衰减。当网络层数很深时，梯度就会不停的衰减，甚至消失，使得整个网络很难训练。这就是所谓的梯度消失问题（Vanishing Gradient Problem），也叫梯度弥散问题。<br>&emsp;&emsp;目前解决“梯度弥散”问题的有效思路是使用ReLU函数作为每个隐层神经元的激活函数，因为相对于sigmoid函数和tanh函数，ReLU函数的导数值更大，这样不仅神经网络的误差可以很好的传播，而且由于ReLU函数的优良特性，神经网路的训练速度也有相应的提高。<br>&emsp;&emsp;4. <strong>深度神经网络的训练思路</strong>。无监督逐层贪婪训练是对多隐层网络进行训练的有效手段。基本思想是， 每次只训练网络的一个隐层，训练时把上一个隐层的输出作为输入，并且本层结点的输出作为下一个隐层的输入。，当前隐层训练结束后再开始训练下一个隐层，即在训练第k个隐藏层时都需要将前k-1层固定。这种训练思想称为“预训练”（pre-training）。预训练全部完成之后，再利用BP算法训练整个网络，即使用BP算法对整个网络进行“微调”。需要注意的是，每一层的预训练可以是有监督的（例如将每一层的分类误差作为目标函数），也可以是无监督的。当用无标签数据训练完网络后，相比于随机初始化而言，各层初始权重会位于参数空间中较好的位置上。然后我们可以从靠近最优值的位置出发，进一步微调权重。从经验上来说，以这些位置为起点开始进行梯度下降训练，更有可能收敛到比较好的局部极值点，这是因为无标签数据已经提供了大量输入数据中包含的模式的先验信息。<br>&emsp;&emsp;5. <strong>深度神经网络的“参数共享”机制</strong>。深度神经网络的参数规模较大，使用“参数共享”的机制能有效地节省训练开销，缩短训练时间。这个策略在卷积神经网络（Convolutional Neural Network，CNN）中已得到成功的应用。<br>&emsp;&emsp;6. <strong>强调了特征学习的重要性</strong>。浅层神经网络模型的特征工程主要靠人工构建特征工程，进而对样本进行分类或预测。Deep Learning含有多个隐层，对样本的特征属性通过逐层特征变换，将样本的原特征空间映射到新的特征空间，从而使分类或预测更加简单。与人工构造特征工程相比，Deep Learning利用大数据来学习特征，更能够刻画数据的丰富内在信息。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;本文根据自己对深度学习的基本理解，简单总结了深度学习与浅层神经网络的不同之处。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Deep Learning" scheme="http://yaodong.ml/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Deep Learning学习笔记——蒙特卡罗方法（MCMC）</title>
    <link href="http://yaodong.ml/deep-learning/Deep-Learning%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97%E6%96%B9%E6%B3%95%EF%BC%88MCMC%EF%BC%89.html"/>
    <id>http://yaodong.ml/deep-learning/Deep-Learning学习笔记——蒙特卡罗方法（MCMC）.html</id>
    <published>2017-01-21T06:34:15.000Z</published>
    <updated>2017-03-21T06:42:07.062Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;本文对马尔科夫链蒙特卡罗（Markov Chain Monte Carlo，MCMC）作简单的介绍。主要参考《Pattern Recognition and Machinle Learning》和Ian Goodfellow的《Deep Learning》。</p>
<a id="more"></a>
<p>&emsp;&emsp;蒙特卡罗是一种在优先计算资源下近似计算的方法。蒙特卡罗方法通过对概率分布的数据进行采样，用采样点的“频率”估计该“概率”分布。MCMC可解决高维空间中的积分和优化不容易直接计算求解的问题。<br>&emsp;&emsp;给定一个概率分布$p(x)$，如何让计算机生成尽可能服从该概率分布的样本集？这是统计模拟领域的采样问题。如何从$p(x)$中采样，能使采样样本尽可能符合概率分布规律，一个方案是<strong>重要采样</strong>，但更常用的方案是使用一个趋于目标分布估计的序列，即马尔科夫链蒙特卡罗方法（MCMC）。</p>
<h3 id="马尔科夫链"><a href="#马尔科夫链" class="headerlink" title="马尔科夫链"></a>马尔科夫链</h3><p>&emsp;&emsp;马尔科夫链是MCMC的理论基础，在随机过程和信息论基础课上都学过这个概念，做一下简单的回顾。<br>&emsp;&emsp;马尔科夫链表示的是一个状态序列${x<em>1,x_2,…,x</em>{t-1},x<em>t,x</em>{t+1}}$，其中每个状态的取值为有限个，在当前状态已知的条件下，将来状态的取值只与当前状态有关，与过去的状态无关。马尔科夫链的数学定义如下：</p>
<script type="math/tex; mode=display">P(X_{t+1}=x|X_t,X_{t−1},...)=P(X_{t+1}=x|X_t)</script><p>&emsp;&emsp;<strong>马尔科夫链定理</strong>：如果一个非周期马尔科夫链具有转移概率矩阵$P$，且它的任何两个状态是互通的，那么$\lim<em>{n \rightarrow \infty}P</em>{ij}^{n}$存在且与$i$无关，记$\lim<em>{n \rightarrow \infty}P</em>{ij}^{n}=\pi(j)$，有</p>
<script type="math/tex; mode=display">\pi(j)=\sum_{i=0}^{\infty}\pi(i)P_{ij}</script><p>&emsp;&emsp;其中，$\pi$是方程$\pi P=\pi$的唯一非负解，其中</p>
<script type="math/tex; mode=display">\pi=\left[\pi(1),\pi(2),...,\pi(j),...,\right],\sum_{i=0}{\infty}\pi(i)=1</script><p>&emsp;&emsp;$\pi$称为马尔科夫链的平稳分布。定理的证明相对比较复杂，在这里直接使用定理的结论。从初始概率分布$\pi_0$出发，在马尔科夫链上做状态转移，记$X_i$的概率分布为$\pi_i$，则有</p>
<script type="math/tex; mode=display">X_0 \sim \pi_0(x)</script><script type="math/tex; mode=display">X_i \sim \pi_i(x) , \pi_i(x) =\pi_{i-1}(x)P=\pi_0(x)P^n</script><p>&emsp;&emsp;由马尔科夫链定理，概率分布$\pi_i(x)$将收敛到平稳分布$\pi(x)$，假设到第n步时马尔科夫链收敛，则有</p>
<script type="math/tex; mode=display">X_0 \sim \pi_0(x)</script><script type="math/tex; mode=display">X_1 \sim \pi_1(x)</script><script type="math/tex; mode=display">X_2 \sim \pi_2(x)</script><script type="math/tex; mode=display">X_n \sim \pi_n(x)</script><script type="math/tex; mode=display">X_{n+1} \sim \pi_{n}(x)</script><script type="math/tex; mode=display">X_{n+2} \sim \pi_n(x)</script><p>&emsp;&emsp; 如果我们从一个具体的初始状态$x<em>0$开始，沿着马尔科夫链按照概率转移矩阵$P$做跳转，那么我们得到一个转移序列${x_0,x_1,x_2,…,x_n,x</em>{n+1},x<em>{n+2}}$，由于马尔科夫链的收敛性质， ${x_n,x</em>{n+1},x_{n+2},…}$都将是平稳分布$\pi(x)$的采样样本。</p>
<h3 id="问题的由来"><a href="#问题的由来" class="headerlink" title="问题的由来"></a>问题的由来</h3><p>&emsp;&emsp;在机器学习问题中，常常遇到计算和或者积分的问题。当和或积分形式过于复杂无法直接计算时，通常可以将待求的和或积分视为某概率分布下的期望值，然后通过（无偏）估计来近似这个期望。这就是马尔科夫采样解决的问题。令<br>$s=\sum_xp(x)f(x)=E_p[f(x)]$ 或 $s=\int_xp(x)f(x)=E_p[f(x)]$为待求的和或积分，$p(x)$为关于随机变量$x$的概率分布或者概率密度函数。</p>
<h3 id="Metropolis-Hastings算法"><a href="#Metropolis-Hastings算法" class="headerlink" title="Metropolis-Hastings算法"></a>Metropolis-Hastings算法</h3><p>&emsp;&emsp;对于给定的概率分布$p(x)$，我们希望能快速生成它对应的样本。Metropolis算法是最早的基于马尔科夫链的蒙特卡罗方法。思路是构造一个转移矩阵为$P$的马尔科夫链，使得该马尔科夫链的平稳分布恰好是$p(x)$，从任何一个初始状态$x<em>0$出发沿着马尔科夫链转移，得到一个转移序列${x_0,x_1,x_2,…,x_n,x</em>{n+1},x<em>{n+2},…}$，由于马尔科夫链的收敛性质， ${x_n,x</em>{n+1},x_{n+2},…}$都将是平稳分布$\pi(x)$的采样样本。<br>&emsp;&emsp;基于马尔科夫链做采样的关键问题是如何构造转移矩阵$P$，使得马氏链的平稳分布恰好是我们想要的概率分布$p(x)$。首先，非周期马尔科夫链必须具备<strong>细致平稳条件</strong>：如果非周期马尔科夫链的转移矩阵$P$和分布$\pi(x)$满足</p>
<script type="math/tex; mode=display">\pi(i)P_{ij} = \pi(j)P_{ji} \quad for \quad all \quad i.j</script><p>&emsp;&emsp;则$\pi(x)$是马尔科夫链的平稳分布。细致平稳条件的物理含义就是对于任何两个状态$i$、$j$，从$i$转移出去到j而丢失的概率质量恰好会被从$j$转移回i的概率质量补充回来，所以状态$i$上的概率质量π(i)是稳定的，从而$\pi(x)$是马尔科夫链的平稳分布。<br>&emsp;&emsp;用$q(i,j)$或$q(j|i)$表示从状态i转移到状态j的概率。通常情况下，马尔科夫链的细致平稳条件不成立，即</p>
<script type="math/tex; mode=display">p(i)q(i,j) \neq p(j)q(j,i)</script><p>&emsp;&emsp;所以$p[(x)$不太可能是这个马尔科夫链的平稳分布。因此，需要对马尔科夫链加入影响因子，使得细致平稳条件成立，引入$\alpha$，使得下式成立：</p>
<script type="math/tex; mode=display">p(i)q(i,j)\alpha(i,j) = p(j)q(j,i)\alpha(j,i)</script><p>&emsp;&emsp;根据对称性，一般取</p>
<script type="math/tex; mode=display">\alpha(i,j) = p(j)q(j,i)</script><script type="math/tex; mode=display">\alpha(j,i) = p(i)q(i,j)</script><p>&emsp;&emsp;在改造转移矩阵中引入的$\alpha(i,j)$称为接受率，物理意义上可理解为在原来的马尔科夫链上，从状态i以概率$q(i,j)$转移到状态$j$时，以$\alpha(i,j)$的概率接受这个转移。<br>&emsp;&emsp;不过，马尔科夫链Q在转移的过程中的接受率$\alpha(i.j)$可能偏小，这样引发的问题是在采样过程中会出现大量的拒绝跳转，使得马尔科夫链的收敛速度太慢。解决此问题的措施是将细致平稳条件中的$\alpha(i,j),\alpha(j,i)$等比例放大，提高采样中的跳转接受率。即</p>
<script type="math/tex; mode=display">\alpha = \min \left \{ \frac {p(j)q(j,i)}{p(i)q(i,j)},1\right \}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;本文对马尔科夫链蒙特卡罗（Markov Chain Monte Carlo，MCMC）作简单的介绍。主要参考《Pattern Recognition and Machinle Learning》和Ian Goodfellow的《Deep Learning》。&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Deep Learning" scheme="http://yaodong.ml/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习算法笔记——集成学习（Ensemble Learning）</title>
    <link href="http://yaodong.ml/machine-learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%EF%BC%88Ensemble-Learning%EF%BC%89.html"/>
    <id>http://yaodong.ml/machine-learning/机器学习算法笔记——集成学习（Ensemble-Learning）.html</id>
    <published>2017-01-21T06:31:19.000Z</published>
    <updated>2017-03-21T06:32:10.863Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;集成学习，通过构建或结合多个弱分类器形成一个强分类器，使组合得到的分类器具有更好的泛化性能，从而达到提升模型分类性能的方法。严格来说，集成学习并不算是一种分类器，而是一种将多个弱分类器结合的策略。<br><a id="more"></a></p>
<h3 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h3><p>&emsp;&emsp;Boosting算法是一族可将弱学习器提升为强学习器的算法。<br>&emsp;&emsp;Boosting算法的理论基础是，在PAC（概率近似正确）的框架下，一定可以将多个弱学习器组合成强学习器。<br>&emsp;&emsp;Boosting算法的工作机制：先从初始样本数据训练出一个基学习器，再<strong>根据基学习器的性能表现改变不同类别训练样本的权重，即加大被误分类的样本的权重，或减小分类正确样本的权重，使得分类错误的样本在模型的后续学习过程中得到更多的关注</strong>。然后基于调整权值后的眼样本数据集重新训练下一个基学习器；如此反复进行，直至基学习器数目达到给定的值T，最后将这T个弱分类器进行加权结合，形成一个集成学习器。<br>&emsp;&emsp;大多数的Boosting方法都是改变训练数据的概率分布，即训练数据样本的权值分布，针对不同的训练数据分布调用不同的弱学习器，进而训练出一系列弱分类器。Boosting方法中应用较为广泛的有提升树模型（Boosting Tree）和自适应提升算法（Adaptive Boosting）。关于AdaBoosting算法的详细笔记，见本人的另一篇博客，<a href="http://yaodong.ml/">机器学习算法笔记——AdaBoosting算法</a>。</p>
<h3 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h3><p>&emsp;&emsp;Bagging思路的关键问题是保证各基学习器之间的多样性和准确性。</p>
<h4 id="Bootstraping"><a href="#Bootstraping" class="headerlink" title="Bootstraping"></a>Bootstraping</h4><p>&emsp;&emsp;简单介绍一下Bootstrapping法，BootSstrap Sampling称为自助采样，具体是通过对有限的样本进行多次有放回随机抽样，重新建立起新的样本集。<br>&emsp;&emsp;给定样本数据集$D$，对$D$进行Boostraping得到新$D_1$的具体做法如下：每次随机从D中采样一个样本，将其拷贝到$D_1$中，并把该样本放回到原数据集$D$中，将这种“拷贝再放回”执行$m$次，就得到了包含$m$个样本的数据集$D_1$。显然，Bootstraping方法改变了原始数据集的分布，使得$D$中有一部分样本不出现在$D_1$中，但有一部分样本多次出现在$D_1$中。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>&emsp;&emsp;从偏差-方差的角度来看：Boosting主要关注降低偏差，因此Boosting能基于泛化性能相对较弱的学习器构建出很强的集成模型；而Bagging主要关注模型的方差，因此Bagging在不进行剪枝的决策树、神经网络等易受样本扰动的学习器上更为明显。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;集成学习，通过构建或结合多个弱分类器形成一个强分类器，使组合得到的分类器具有更好的泛化性能，从而达到提升模型分类性能的方法。严格来说，集成学习并不算是一种分类器，而是一种将多个弱分类器结合的策略。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Machine Learning" scheme="http://yaodong.ml/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习算法笔记——AdaBoosting算法</title>
    <link href="http://yaodong.ml/machine-learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94AdaBoosting%E7%AE%97%E6%B3%95.html"/>
    <id>http://yaodong.ml/machine-learning/机器学习算法笔记——AdaBoosting算法.html</id>
    <published>2017-01-21T06:29:46.000Z</published>
    <updated>2017-03-21T06:30:38.105Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a>
<p>&emsp;&emsp;从算法模型的角度来说，AdaBoosting算法是前向分布加法算法的特例，也就是说，AdaBoosting算法是一个加法模型，其学习算属于为前向分步算法。<br>&emsp;&emsp;AdaBoosting算法的损失函数是指数函数。回忆一下，机器学习算法中，常见的损失函数有0-1损失函数、对数似然损失函数、均方误差函数、Hinge函数、指数损失函数等。</p>
<p>&emsp;&emsp;下面对AdaBoosting算法作简单推导。给定样本数据集$S={(x^{(1)},y^{(1)}),…,(x^{(i)},y^{(i)}),…,(x^{(m)},y^{(m)})}$，$x^{(i)}\in R^n$，$y(i)\in {-1,+1}$。</p>
<ol>
<li>初始化数据集的样本权值分布<script type="math/tex; mode=display">D=(w_{11},...,w_{1i},...,w_{1n})</script>其中$w_{1i}=\frac{1}{m},i=1,2,…,m$。</li>
<li><p>训练$m$个基学习器：<br><strong>1)</strong> 对于第$i$个样本，根据当前的样本权值分布$D_m$，求得基分类器$G_m(x)$，其中$G_m(x)\in {-1,+1}$；<br><strong>2)</strong> 并求得$G_m(x)$在训练数据集上的带权分类误差率：</p>
<script type="math/tex; mode=display">e_m=P(G_m(x^{(i)})≠y^{(i)})=\sum_{i=1}^{m}w_{mi}I(G_m(x^{(i)})≠y^{(i)})</script><p><strong>3)</strong> 接下来计算$G_m(x)$的系数$\alpha_m$，$\alpha_m$表示第m个基学习器在最终的组合学习器中的比重：</p>
<script type="math/tex; mode=display">\alpha_m=\frac{1}{2}log\frac{1-e_m}{e_m}</script><p>很显然，$\alpha_m$随着$e_m$的增大而减小，所以分类误差率越小的基学习器在最终的组合模型中占的比重越大。<br><strong>4)</strong> 更新样本权值分布向量：</p>
<script type="math/tex; mode=display">D_{m+1} = (w_{m+1,1},...,w_{m+1,n})</script><script type="math/tex; mode=display">w_{m+1,i}=\frac {w_{mi}}{Z_m}e^{-\alpha _my^{(i)}G_m(x_i)}</script><p>这里$Z<em>m$是规范化因子，目的是将$D</em>{m+1}$进行归一化，使得$D_{m+1}$符合概率分布的归一性，$Z_m$由以下公式给出：</p>
<script type="math/tex; mode=display">Z_m=\sum_{i=1}^Nw_{mi}e^{-\alpha _my^{(i)}G_m(x_i)}</script><p>根据$w_{m+1,i}$的表达式可知，被$G_m(x)$分类器误分类的样本的权值得以扩大，而被$G_m(x)$正确分类的样本的权值会缩小。因此，误分类样本在下一次的基学习器训练中起到更大的作用。这个<strong>不改变训练数据，而改变训练数据的权值分布，正是AdaBoost方法的重要特点</strong>。</p>
</li>
<li><p>步骤2中求得了m个基学习器的权值分布，将这m个基学习器组合起来，构成最终的组合学习器：</p>
<script type="math/tex; mode=display">f_{(x)}=\sum_{m=1}^{M}\alpha_mG_{m}(x)</script><script type="math/tex; mode=display">G_{(x)}=sign\left((f(x)\right)=sign\left(\sum_{m=1}^{M}\alpha_mG_{m}(x))\right)</script><p><strong>注意</strong>，所有的$\alpha_m$相加之和并不等于1。</p>
</li>
</ol>
<hr>
<h3 id="梯度上升决策树算法（Gradient-Boosting-Decision-Tree，GBDT）"><a href="#梯度上升决策树算法（Gradient-Boosting-Decision-Tree，GBDT）" class="headerlink" title="梯度上升决策树算法（Gradient Boosting Decision Tree，GBDT）"></a>梯度上升决策树算法（Gradient Boosting Decision Tree，GBDT）</h3><blockquote>
<p><strong>GBDT</strong>是一个应用很广泛的算法，可以用于解决分类、回归等问题。GBDT算法也有其他的名字，如MART(Multiple Additive Regression Tree)，GBRT(Gradient Boost Regression Tree)，Tree Net等。</p>
</blockquote>
<p>&emsp;&emsp;首先介绍一下Gradient Boosting的基本概念。Gradient Boost其实是一个框架，可以套入很多不同的算法以对穿传统的机器学习算法进行改进。当损失函数不是平方损失函数或者指数损失函数，而是一般函数时，boost算法对损失函数的求解和优化会变得比较复杂，针对这一问题，学者提出了Gradient Boosting算法。核心思想是利用损失函数的负梯度在当前模型下的取值作为残差的近似，进一步训练得到下一棵回归树。<br>&emsp;&emsp;回忆一下原始的Boosting算法：算法初始状态下，为每一个样本赋上大小相等的权重值。在之后迭代的每一步中增加被误分类的样本的权重，减少正确分类的样本的权重，使得被误分类的样本被赋上一个很高的权重。进行M次迭代训练，将得到M个基学习器，最后将它们根据指定的策略组合起来，得到一个最终的模型。<br>&emsp;&emsp;GBDT中的树都是回归树，不是分类树，但是对GBDT模型调整后可以用于分类问题。GBDT的核心在于，每一棵子树学习的是之前所有树的预测结果与实际观测值之间的残差。Gradient Boosting就是拟合Loss function的梯度，将其作为新的弱回归树加入到总的算法中即可。<br>&emsp;&emsp;Gradient Boost与传统的Boost的区别是，Gradient Boot算法每次的计算是为了减小上一次的残差（residual）。为了消除残差，可以取损失函数的负梯度在当前模型上的取值作为当前残差的近似值，并拟合下一个子树模型。所以说，在Gradient Boost中，每个新子树模型的建立会使之前模型的残差往梯度方向减少，与传统Boosting算法对正确、错误的样本进行加权有着很大的区别。</p>
<p>&emsp;&emsp;GBDT的算法流程如下：给定样本数据集$T={(x^{(1)},y^{(1)}),…,(x^{(i)},y^{(i)}),…,(x^{(m)},y^{(m)})}$，$x^{(i)}\in R^n$，$y(i)\in {-1,+1}$，损失函数为$L(y,f(x))$，设生成的子树的数目是K。</p>
<ol>
<li>初始化第一课子树，估计使损失函数最小化的常数值，建立只有一个根结点的树：<script type="math/tex; mode=display">f_0(x)=\arg\min_c\sum_{i=1}^{m}L(y^{(i)},c)</script></li>
<li>对k=1,2,…,K:<br><strong>1)</strong> 对i=1,2,…,m，计算当前损失函数的负梯度在当前模型的值，作为模型残差的估计：<script type="math/tex; mode=display">g_{ki}=\left[\frac{\partial L(y^{(i)},f(x^{(i)}))}{\partial f(x^{(i)})}\right]_{f(x)=f_{k-1}(x)}</script><strong>2)</strong> 根据$r<em>{ki}$拟合一个回归树，作为下一个子树模型，估计回归树的叶节点对区域$R</em>{kj}$,$j=1,2,…,J$，以拟合残差的近似值：<br><strong>3)</strong> 利用线性搜索估计叶节点区域的值，使损失函数最小化。对$j=1,2,…,J$，计算<script type="math/tex; mode=display">c_{mj}=\arg\min_c\sum_{x_i \in R_{kj}}L(y^{(i)},f_{k-1}(x^{(i)})+c)</script><strong>4)</strong> 更新第k个子回归树模型<script type="math/tex; mode=display">f_k(x)=f_{k-1}(x)+\sum_{j=1}^Jc_{kj}I(x\in R_{kj})</script></li>
<li>得到回归树<script type="math/tex; mode=display">\hat f(x)=f_{K}(x)=\sum_{k=1}^{K}\sum_{j=1}^Jc_{kj}I(x\in R_{kj})</script></li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&amp;emsp;&amp;emsp;从算法模型的角度来说，AdaBoosting算法是前向分布加法算法的特例，也就是说，AdaBoosting算法是一个加法模型，其学习算属于为前向分步算法。&lt;br&gt;&amp;emsp;&amp;emsp;AdaBoosting算法的
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Machine Learning" scheme="http://yaodong.ml/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习算法笔记——偏差与方差</title>
    <link href="http://yaodong.ml/machine-learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE.html"/>
    <id>http://yaodong.ml/machine-learning/机器学习算法笔记——偏差与方差.html</id>
    <published>2017-01-21T06:26:55.000Z</published>
    <updated>2017-03-21T06:28:11.513Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;最近在复习基本机器学习、深度学习的基础模型和理论算法，总结一下机器学习中常见的几个概念。<br><a id="more"></a><br>&emsp;&emsp;偏差可以理解为算法模型对训练数据集的拟合程度，也可以理解为模型再训练数据集上的表现。而方差描述的是用样本集训练得到的模型在测试机上的性能表现。偏差度量了算法模型的期望预测与真实结果的偏离程度，即偏差描述的是算法模型本身的拟合能力；方差度量了同样大小的数据集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;最近在复习基本机器学习、深度学习的基础模型和理论算法，总结一下机器学习中常见的几个概念。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Machine Learning" scheme="http://yaodong.ml/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习算法笔记——正则化、交叉验证</title>
    <link href="http://yaodong.ml/machine-learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%AD%A3%E5%88%99%E5%8C%96%E3%80%81%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81.html"/>
    <id>http://yaodong.ml/machine-learning/机器学习算法笔记——正则化、交叉验证.html</id>
    <published>2017-01-21T06:23:34.000Z</published>
    <updated>2017-03-21T06:24:16.655Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;<br><a id="more"></a></p>
<h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p>&emsp;&emsp;正则化是机器学习算法模型中避免过拟合的一种策略。这里提一句，避免模型陷入过拟合的另外一个策略是增加训练数据集的规模。正则化也是结构风险最小化策略的实现，是在结构风险上加入正则化项（惩罚项），正则化项一般是模型复杂度的单调递增函数。模型越复杂，正则化项对模型的惩罚程度越大。<strong>通常来说，正则化项一般是算法模型参数向量的范数。</strong><br>&emsp;&emsp;从“大牛即权威”的角度来说，正则化操作符合奥卡姆剃刀（Occam’s razor）原理，即在所有可供选择的算法模型中，能够很好拟合数据并且算法复杂度更低的模型才是最好的模型。从贝叶斯概率框架的角度来说，正则化项对应于模型的先验概率：复杂模型具有较小的先验概率，简单模型具有较大的先验概率。也就是说，通过正则化项，我们可以对模型加入人对该模型的先验知识，使得算法模型能get到我们想要的特性，例如稀疏、平滑、低秩等。从算法复杂度的角度来说，正则化操作的目的是在保证误差极小化的基础上降低算法复杂度。因此，为了保证算法的泛化性能，在算法的学习和训练过程中加入正则化项是必要的。<br>&emsp;&emsp;加入正则化项的有监督学习的目标函数如下：</p>
<script type="math/tex; mode=display">w^*=\arg\min_w=\sum_iL(y^{(i)},f(x^{(x)};w))+\lambda\Omega(w)</script><p>&emsp;&emsp;上面说到，正则化项一般是算法模型参数向量的范数，如零范数、1-范数、2-范数、迹范数、Frobenius范数和核范数等等，接下来，简单介绍一下正则化中常用的几个范数。</p>
<h4 id="L0范数和L1范数"><a href="#L0范数和L1范数" class="headerlink" title="L0范数和L1范数"></a>L0范数和L1范数</h4><p>&emsp;&emsp;正则化项为L1范数的目标函数形式如下：</p>
<script type="math/tex; mode=display">J(w,\lambda)=\sum_iL(y^{(i)},f(x^{(x)};w))+\frac {\lambda}{m}\sum_i \mid w \mid</script><p>&emsp;&emsp;L0范数是指参数向量中非0的元素的个数。如果用L0范数来规则化一个参数矩阵$W$的话，就是希望$W$的大部分元素都是0，换句话说，L0正则化项的目的是使参数$W$是稀疏的。<br>&emsp;&emsp;L1范数是指参数向量中各元素的绝对值之和，也叫“稀疏规则算子”（Lasso regularization），L1范数和L0范数的关系是——<strong>L1范数是L0范数的最优凸近似</strong>。<br>但是在大多数paper中，参数矩阵的稀疏化操作都是通过L1范数（$\parallel W \parallel_1$）实现。由于L0范数很难优化求解（NP难问题），L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解。因此，L1范数的使用更为普遍。<br>&emsp;&emsp;<strong>L1范数和L0范数可以实现稀疏，L1因具有比L0更好的优化求解特性而被广泛应用。</strong></p>
<h4 id="L2范数"><a href="#L2范数" class="headerlink" title="L2范数"></a>L2范数</h4><p>&emsp;&emsp;正则化项为L2范数的目标函数形式如下：</p>
<script type="math/tex; mode=display">J(w,\lambda)=\sum_iL(y^{(i)},f(x^{(x)};w))+\frac {\lambda}{2m}\sum_i \parallel w \parallel_2^2</script><p>&emsp;&emsp;L2范数在机器学习算法的正则化项中也很常见，L2范数（$\parallel W \parallel_2$）在回归问题里叫做“岭回归”（Ridge Regression）,部分学者把它叫做“权值衰减”（Weight Decay）。L2范数的强大之处在于，将其应用在机器学习中能有效改善过拟合问题。<br>&emsp;&emsp;L2范数是指参数矩阵$\parallel W \parallel$ 的各元素的平方和然后求平方根。L2范数的规则项$\parallel W \parallel_2$最小，意味着$W$的每个元素都很小，都接近于0但不等于0。而越小的参数说明算法模型越简单，越简单的模型则越不容易产生过拟合现象。总结起来就是，<strong>通过L2范数，可以实现对算法模型空间的限制，从而在一定程度上避免过拟合，提升模型的泛化能力。</strong><br>&emsp;&emsp;此外，L2范数应用于机器学习算法的正则化项亦可以从理论和数值优化计算的角度去解释，大家有兴趣可以去查阅相关的论文。</p>
<h3 id="交叉验证（Cross-Validation，CV）"><a href="#交叉验证（Cross-Validation，CV）" class="headerlink" title="交叉验证（Cross Validation，CV）"></a>交叉验证（Cross Validation，CV）</h3><p>&emsp;&emsp;交叉验证是用来验证算法模型的分类性能的一种统计分析方法，基本思想是把在某种意义下将原始数据集$D$进行分组，一部分做为训练集（training set），另一部分做为验证集（validation set）。首先用训练集对分类器进行模型训练，再利用验证集来测试训练好的模型的性能表现，以此来做为评价分类器的性能指标。下面对常见的交叉验证方法做简单介绍。</p>
<h4 id="Hold-Out-Method"><a href="#Hold-Out-Method" class="headerlink" title="Hold-Out Method"></a>Hold-Out Method</h4><p>&emsp;&emsp;将原始数据随机分为两组，一组做为训练集，一组做为验证集。利用训练集训练分类器，然后利用验证集验证模型，记录最后的分类准确率作为此分类器的性能指标。<br>&emsp;&emsp;此种方法的好处是简单易操作，只需随机把原始数据分为两组即可。但是严格意义来说Hold-Out Method并不能算是CV，因为这种方法只是随机地对原始数据进行分组，并没有使用交叉的思想，所以最后验证集分类准确率的高低与原始数据的分组有很大的关系，所以这种方法得到的结果其实并不具有说服性。</p>
<h4 id="Double-Cross-Validation（2-fold-Cross-Validation，2-CV）"><a href="#Double-Cross-Validation（2-fold-Cross-Validation，2-CV）" class="headerlink" title="Double Cross Validation（2-fold Cross Validation，2-CV）"></a>Double Cross Validation（2-fold Cross Validation，2-CV）</h4><p>&emsp;&emsp;将数据集分成两个样本数目相等的子集，进行两回合的分类器训练。在第一回合中，一个子集作为training set，另一个便作为testing set；在第二回合中，则将training set与testing set对换，再次训练分类器，并得到两次训练的分类性能。<br>&emsp;&emsp;实际中2-CV并不常用，主要原因是training set样本数太少，通常不足以代表母体样本的分布，导致testing阶段辨识率容易出现明显落差。此外，2-CV中子集集的变异度较大，往往无法达到“实验过程必须可以被复制”的要求。</p>
<h4 id="K-fold-Cross-Validation（K-折交叉验证，记为K-CV）"><a href="#K-fold-Cross-Validation（K-折交叉验证，记为K-CV）" class="headerlink" title="K-fold Cross Validation（K-折交叉验证，记为K-CV）"></a>K-fold Cross Validation（K-折交叉验证，记为K-CV）</h4><p>&emsp;&emsp;将原始数据集$D$划分为K个大小相似的互斥子集（一般是均分），每个子集都通过对原始数据的分层采样得到，以尽可能保证数据分布的一致性。然后，每次分别用K-1个子集的并集作为训练集，余下的那个子集当作验证集，这样就可以得到K组训练集/测试集组合，从而可进行K次训练和测试。最后用这K个模型的验证集的分类准确率的均值作为此K-CV下分类器的性能指标。<br>&emsp;&emsp;此外，为减小因样本划分不同而引入的差别，k折交叉验证通常要随机使用不同的划分重复p次，最终的结果是这p次K折交叉验证结果的均值。K-CV可以有效的避免过学习以及欠学习状态的发生，最后得到的结果也比较具有说服性。</p>
<h4 id="Leave-One-Out-Cross-Validation（留一法，LOO-CV）"><a href="#Leave-One-Out-Cross-Validation（留一法，LOO-CV）" class="headerlink" title="Leave-One-Out Cross Validation（留一法，LOO-CV）"></a>Leave-One-Out Cross Validation（留一法，LOO-CV）</h4><p>&emsp;&emsp; 如果设原始数据有m个样本，那么LOO-CV就m-CV，即每个样本单独作为验证集，其余的m-1个样本作为训练集，所以LOO-CV会得到m个模型，用这m个模型最终的验证集的分类准确率的均值作为此下LOO-CV分类器的性能指标。相比于前面的K-CV，LOO-CV有两个明显的优点：<br>（1）每一回合中几乎所有的样本皆用于训练模型，因此最接近原始样本的分布，这样评估所得的结果比较可靠。<br>（2）实验过程中没有随机因素会影响实验数据，确保实验过程是可以被复制的。</p>
<p>&emsp;&emsp;但LOO-CV的缺点则是计算成本高，因为需要建立的模型数量与原始数据样本数量相同，当原始数据样本数量相当多时，LOO-CV在实作上便有困难几乎就是不显示，除非每次训练分类器得到模型的速度很快，或是可以用并行化计算减少计算所需的时间。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Machine Learning" scheme="http://yaodong.ml/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习算法笔记——模型性能度量（Precision/Recall/ROC/AUC）</title>
    <link href="http://yaodong.ml/machine-learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F%EF%BC%88Precision-Recall-ROC-AUC%EF%BC%89.html"/>
    <id>http://yaodong.ml/machine-learning/机器学习算法笔记——模型性能度量（Precision-Recall-ROC-AUC）.html</id>
    <published>2017-01-21T06:21:48.000Z</published>
    <updated>2017-03-21T06:22:54.298Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a>
<p>&emsp;&emsp;分类问题中，最直接的模型性能度量指标是错误率和精度。但在实际的应用中，更常用的模型评价指标是查准率（precision）与查全率（recall）。查准率也叫准确率，查全率也叫召回率。<br>&emsp;&emsp;在二分类问题中，将关注的类别记为正类，其他类记为父类。分类模型在测试数据集上的预测结果可分为以下几类：</p>
<blockquote>
<p>TP（真正例）——将正类样本预测为正类的样本数量<br>FP（假正例）——将负类样本预测为正类的样本数量<br>FN（假反例）——将正类样本预测为负类的样本数量<br>TN（真反例）——将负类样本预测为负类的样本数量</p>
</blockquote>
<p>&emsp;&emsp;查准率定义为</p>
<script type="math/tex; mode=display">precision=\frac{TP}{TP+FP}</script><p>&emsp;&emsp;查全率定义为</p>
<script type="math/tex; mode=display">recall=\frac{TP}{TP+FN}</script><p>&emsp;&emsp;查准率与查全率是一对矛盾的度量。因此还定义了$F_1$度量，$F_1$定义为查准率和查全率的调和均值</p>
<script type="math/tex; mode=display">\frac{1}{F_1}=\frac{1}{precision}+\frac{1}{recall}=\frac{2TP}{2TP+FP+FN}</script><p>&emsp;&emsp;不同的实际应用场景对查准率和查全率的重视程度有所不同。$F<em>1$度量还有更一般的形式——$F</em>{\beta}$，根据$\beta$的取值可调整$F_{\beta}$对查准率和查全率的重视程度。</p>
<script type="math/tex; mode=display">F_\beta=\frac{(1+\beta ^2)\times P \times R}{(\beta^2 \times P)+R}</script><p>&emsp;&emsp;其中$\beta &gt; 0$，度量了查全率对查准率的相对重要性，$\beta = 1$时退化为标准的$F_1$，$\beta &gt; 1$时更重视查全率，$\beta &gt; 1$时更重视查准率。</p>
<h3 id="P-R曲线"><a href="#P-R曲线" class="headerlink" title="P-R曲线"></a>P-R曲线</h3><p>&emsp;&emsp;P-R曲线即Precision-Recall曲线，是以查准率为纵轴、查全率为横轴作图所得的曲线。P-R曲线能直观显示出分类模型在样本集上的查全率、查准率。如果模型A的P-R曲线完全被模型B的P-R曲线包住，则可断言模型B的性能优于模型A。如果两个分类模型的P-R曲线有交叉，则难以一般性地断言二者孰优孰劣，只能在具体的查准率或查全率条件下再对二者进行评估。更多情况下，比较合理的判据是比较P-R曲线与坐标轴围成的面积的大小，它在一定程度上表征了分类模型在查准率与查全率上取得相对“双高”的比例。</p>
<h3 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h3><p>&emsp;&emsp;ROC的全称是Receiver Operating Characteristic，译为受控者工作特征。ROC曲线是以“真正例率”TPR为纵轴、以“假正例率”FPR为横轴绘制的曲线。TPR、FPR定义如下：</p>
<script type="math/tex; mode=display">TPR = \frac{TP}{TP+FN}</script><script type="math/tex; mode=display">FPR=\frac{FP}{TN+FP}</script><p>&emsp;&emsp;TPR也叫灵敏度（sensitivity），TPR即召回率；FPR也叫特异度（specificity），FPR可以理解为在负类样本中，把负类样本误分为正样本的比例。TPR、FPR二者是相互对立的关系，此消彼长。<br>&emsp;&emsp;一般分类器对样本进分类的思路为，对测试样本产生一个回归值或概率预测，记为score，并将score与分类阈值（Threshold）比较，进而对样本进行分类。可以根据模型对每个样本计算出的“似然值”对测试样本进行从大到小重新排序，从这个角度来说，分类问题就相当于是将该排序序列在某个“截断点”将样本集分为两部分，前一部分的最终预测结果为正例，后一半的预测结果为反例。对于一个分类模型，选择不同的分类阈值，可以生成不同对（TPR，FPR），将这些（TPR，FPR）坐标值绘制在坐标轴中，就得到了当前分类模型的ROC曲线。一般来说，分类阈值可取每个样本的预测“似然值”score，这样就可以生成m对（TPR，FPR）坐标值，m为样本的数量。显然，在ROC曲线空间中，离右上角（0,1）点越近，说明算法的性能越好。<br>&emsp;&emsp;如果分类模型的ROC曲线被模型B的ROC曲线完全“包住”，则可断言模型B的分类性能由于模型A；若两个模型的ROC曲线发生交叉，则不能直接判断那个模型的性能更好。较为合理的判据是比较ROC曲线下的面积，即AUC值。<br>&emsp;&emsp;ROC曲线有个很好的特性：当测试集中的正负样本的分布发生变化时，ROC曲线能保持不变。</p>
<h3 id="AUC值"><a href="#AUC值" class="headerlink" title="AUC值"></a>AUC值</h3><p>&emsp;&emsp;AUC的全称是Area Under Curve，AUC值定义为ROC曲线下的面积。显然，AUC的取值在0.5~1之间。AUC作为数值，可以直接评价分类模型的好坏，AUC值越大，模型的分类性能越好。<br>&emsp;&emsp;另外，根据ROC曲线的绘制规则可知，AUC值可以从概率的角度理解：随机地挑选一个正样本，当前的分类模型根据样本的“似然值”将该正样本排在负样本的前面的概率即为AUC，AUC值越大，当前的分类算法将正样本排在负样本前面的概率越大。</p>
]]></content>
    
    <summary type="html">
    
      &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&amp;emsp;&amp;emsp;分类问题中，最直接的模型性能度量指标是错误率和精度。但在实际的应用中，更常用的模型评价指标是查准率（precision）与查全率（recall）。查准率也叫准确率，查全率也叫召回率。&lt;br&gt;&amp;emsp;&amp;emsp;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Machine Learning" scheme="http://yaodong.ml/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>数据结构与算法之堆与堆排序</title>
    <link href="http://yaodong.ml/algorithms/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E4%B9%8B%E5%A0%86%E4%B8%8E%E5%A0%86%E6%8E%92%E5%BA%8F.html"/>
    <id>http://yaodong.ml/algorithms/数据结构与算法之堆与堆排序.html</id>
    <published>2017-01-05T06:19:24.000Z</published>
    <updated>2017-03-21T06:21:06.954Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;<br><a id="more"></a></p>
<h3 id="优先队列"><a href="#优先队列" class="headerlink" title="优先队列"></a>优先队列</h3><blockquote>
<p>&emsp;&emsp;<strong>优先队列</strong>（priority queue）是一种特殊的队列。优先队列与普通队列的不同之处在于，优先队列中的元素被赋予优先级。当访问元素时，队列中具有最高优先级的元素将优先出队，优先队列的行为特征是<strong>最高级先出</strong>（First In Largest Out）。</p>
</blockquote>
<p>优先队列至少满足以下两种操作：</p>
<ol>
<li>允许插入（insert）操作。优先队列允许插入操作，相当于普通队列中的入队操作。二者的区别在于，普通队列的入队操作只需将待操作元素插入队尾，而优先队列的插入操作，需要将元素插入到优先队列的正确位置中。</li>
<li>允许删除操作（deleteMin，deleteMax）。优先队列中的元素如果是数字，则元素的优先级对应于数字的大小。优先队列的删除操作相当于普通队列的出队操作，但优先队列需要找到并删除优先级最高的元素。</li>
</ol>
<p>优先队列的应用：</p>
<ol>
<li>作业系统中的调度程序：当一个作业完成后，需要在所有等待调度的作业队列中选择一个优先级最高的作业来执行，并且也可以添加一个新的作业到作业的优先队列中。</li>
</ol>
<hr>
<p>&emsp;&emsp;可以使用链表实现优先队列，但这将花费$O(N)$的时间复杂度进行删除；也可以使用二叉查找树实现优先队列，但二叉查找树的插入和删除操作的平均时间复杂度都是$O(logN)$，且二叉树的许多操作对于优先队列来说是不必要的，因此将二叉查找树用于优先队列过于复杂了。<br>&emsp;&emsp;一般使用二叉堆(Heap)来实现优先队列。二叉堆的插入和删除的最差时间复杂度为$O(logN)$ ，插入操作的的平均时间复杂对为$O(1)$ ，因此二叉堆可以以线性时间建立有$N$项的优先队列。</p>
<h3 id="二叉堆"><a href="#二叉堆" class="headerlink" title="二叉堆"></a>二叉堆</h3><p>&emsp;&emsp;二叉堆也称为堆，这里所说的堆是数据结构中的堆，而不是内存模型中的堆。二叉堆就结构上来说，是一棵完全二叉树。二叉堆满足<strong>结构性</strong>和<strong>堆序性</strong>：</p>
<ul>
<li>结构性：二叉堆应该满足完全二叉树的树结构</li>
<li>堆序性：二叉堆中，父节点的键值总是大于等于（或小于等于）任何一个子节点的键值。且每个节点的左子树和右子树仍然是一个二叉堆（最大堆或最小堆）。</li>
</ul>
<p>&emsp;&emsp;将根节点最大的堆叫做<strong>最大堆</strong>或大根堆，根节点最小的堆叫做<strong>最小堆</strong>或小根堆。常见的堆有二叉堆、左倾堆、斜堆、斐波那契堆等等。<br>&emsp;&emsp;由于二叉堆中的元素是有序排列的，而且二叉堆结构上是一棵完全二叉树，因此二叉堆可以使用一个数组和一个代表当前堆的大小的整数$N$组成。<br>&emsp;&emsp;回忆完全二叉树的性质：</p>
<blockquote>
<ol>
<li>一棵高度为$h$的完全二叉树的节点个数为$2^h$到$2^{h+1}-1$个；</li>
<li>完全二叉树的高度为$\lfloor O(logN)\rfloor$；</li>
<li>假设完全二叉树中的第一个元素在数组中的索引为$0$，那么父节点与子节点的位置关系如下：索引为$i$的左孩子的索引是 $2i+1$；索引为$i$的右孩子的索引是$2i+2$；索引为$i$的父结点的索引是$\lfloor \frac {i-1} 2\rfloor$。</li>
<li>假设完全二叉树中的第一个元素在数组中的索引为$1$，那么父节点与子节点的位置关系如下：索引为$i$的左孩子的索引是 $2i$；索引为$i$的右孩子的索引是$2i+1$；索引为$i$的父结点的索引是$\lfloor \frac i 2\rfloor$。</li>
</ol>
</blockquote>
<h4 id="插入（insert）操作"><a href="#插入（insert）操作" class="headerlink" title="插入（insert）操作"></a>插入（insert）操作</h4><p>&emsp;&emsp; 为将一个元素$X$插入到堆中，我们在下一个可用位置创建一个空穴，否则该堆将不是完全树。如果$X$可以放在该空穴中而不破坏堆的堆序性，那么插入操作完成。否则，我们把空穴的父节点上的元素移入该空穴中，这样空穴就朝着根的方向上冒一步。继续改过程直到 X 能被放入空穴中为止。这种实现过程称之为<strong>上滤（percolate up）</strong>：新元素在堆中上滤直到找出正确的插入位置。参考代码如下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">insert</span><span class="params">( AnyType x )</span></span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">if</span>( currentSize == array.length - <span class="number">1</span> )</div><div class="line">        enlargeArray( array.length * <span class="number">2</span> + <span class="number">1</span> );</div><div class="line"></div><div class="line">        <span class="comment">// Percolate up</span></div><div class="line">    <span class="keyword">int</span> hole = ++currentSize;</div><div class="line">    <span class="keyword">for</span>( array[ <span class="number">0</span> ] = x; x.compareTo( array[ hole / <span class="number">2</span> ] ) &lt; <span class="number">0</span>; hole /= <span class="number">2</span> )</div><div class="line">        array[ hole ] = array[ hole / <span class="number">2</span> ];</div><div class="line">    array[ hole ] = x;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h4 id="删除最小值（deleteMin）操作"><a href="#删除最小值（deleteMin）操作" class="headerlink" title="删除最小值（deleteMin）操作"></a>删除最小值（deleteMin）操作</h4><p>&emsp;&emsp;基于二叉堆的优先队列模型，出队的应该是最小元素。按照最小堆的堆序性，找出最小元十分容易，困难之处在于删除该节点，因为删除节点会破坏二叉堆的结构型，还需要进行额外的操作以保证二叉堆的结构完整。<br>&emsp;&emsp;当删除一个最小元素时，要在根节点建立一个空穴。由于现在堆少了一个元素，因此堆中最后一个元素 X 必须移动到该堆的某个地方。如果$X$可以直接被放到空穴中，那么<strong>deleteMin</strong>完成。不过这一般不太可能，因此我们将空穴的两个儿子中的较小者移入空穴，这样就把空穴向下推了一层，重复该步骤直到$X$可以被放入空穴中。因此，我们的做法是将 $X$置入沿着从根开始包含最小儿子的一条路径上的一个正确的位置。这种策略称为下滤（percolate down）。参考代码如下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> AnyType <span class="title">findMin</span><span class="params">( )</span></span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">if</span>( isEmpty( ) )</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnderflowException( );</div><div class="line">    <span class="keyword">return</span> array[ <span class="number">1</span> ];</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">public</span> AnyType <span class="title">deleteMin</span><span class="params">( )</span></span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">if</span>( isEmpty( ) )</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnderflowException( );</div><div class="line"></div><div class="line">    AnyType minItem = findMin( );</div><div class="line">    array[ <span class="number">1</span> ] = array[ currentSize-- ];</div><div class="line">    percolateDown( <span class="number">1</span> );</div><div class="line"></div><div class="line">    <span class="keyword">return</span> minItem;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">percolateDown</span><span class="params">( <span class="keyword">int</span> hole )</span></span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">int</span> child;</div><div class="line">    AnyType tmp = array[ hole ];</div><div class="line"></div><div class="line">    <span class="keyword">for</span>( ; hole * <span class="number">2</span> &lt;= currentSize; hole = child )</div><div class="line">    &#123;</div><div class="line">        child = hole * <span class="number">2</span>;</div><div class="line">        <span class="keyword">if</span>( child != currentSize &amp;&amp;</div><div class="line">                array[ child + <span class="number">1</span> ].compareTo( array[ child ] ) &lt; <span class="number">0</span> )</div><div class="line">            child++;</div><div class="line">        <span class="keyword">if</span>( array[ child ].compareTo( tmp ) &lt; <span class="number">0</span> )</div><div class="line">            array[ hole ] = array[ child ];</div><div class="line">        <span class="keyword">else</span></div><div class="line">            <span class="keyword">break</span>;</div><div class="line">    &#125;</div><div class="line">    array[ hole ] = tmp;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h3 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h3><p>&emsp;&emsp;堆排序是基于二叉堆的排序算法。堆分为“最大堆”和“最小堆”。最大堆用于升序排序，最小堆用于降序排序。<br>&emsp;&emsp;以升序排序为例，堆排序就是把最大堆堆顶的最大元素取出（为了尽可能不使用外部存储空间，取出操作通过与数组末位元素交换实现），再将剩余的堆调整为最大堆，如此反复进行，直至将堆中元素取完结束。</p>
<h4 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h4><p>堆排序算法的基本思想总结如下：</p>
<ol>
<li>初始化堆：将待排序的数组<strong>array[0,n-1]</strong>构造为最大堆（buildHeap）</li>
<li>交换数据：将array[0]与array[n-1]交换，使得当前堆的最大值存于array[n-1]</li>
<li>将array[0]~array[n-2]调整为最大堆，并将array[0]与array[n-2]交换</li>
<li>不断将数组相应位置的元素调整为最大堆，并将最大值取出，一直进行下去，直到堆中所有元素都被取出。</li>
</ol>
<p>&emsp;&emsp;由于堆的性质，堆排序可借助数组实现，假设第一个元素的下标为0，则索引为$i$的结点的左孩子索引为$2i+1$，右孩子的索引为$2i+2$，索引为$i$的结点如果有孩子结点，则其左孩子结点的索引下标为$\lfloor \frac{i-1}{2}\rfloor$。<br>&emsp;&emsp;将数组的部分元素调整为最大堆的程序buildHeap如下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">adjustHeap</span><span class="params">(<span class="keyword">int</span>[] array,<span class="keyword">int</span> start,<span class="keyword">int</span> end)</span> </span>&#123;</div><div class="line">    <span class="keyword">int</span> cur = start;</div><div class="line">    <span class="keyword">int</span> left = <span class="number">2</span> * cur + <span class="number">1</span>;</div><div class="line">    <span class="keyword">int</span> temp = array[cur];</div><div class="line">    <span class="keyword">for</span>(;left &lt;= end;current = left,left = <span class="number">2</span> * cur + <span class="number">1</span>) &#123;</div><div class="line">        <span class="keyword">if</span>(left &lt; end &amp;&amp; array[left] &lt; array[left + <span class="number">1</span>]) &#123;</div><div class="line">            left++;<span class="comment">//找到当前节点的孩子结点中较大的那个</span></div><div class="line">        &#125;</div><div class="line">        <span class="keyword">if</span>(temp &gt;= a[left]) &#123;</div><div class="line">            <span class="keyword">break</span>;</div><div class="line">        &#125;<span class="keyword">else</span> &#123;</div><div class="line">            array[cur] = array[left];</div><div class="line">            a[left] = temp;         </div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">swap</span><span class="params">(<span class="keyword">int</span>[] array,<span class="keyword">int</span> start,<span class="keyword">int</span> end)</span> </span>&#123;</div><div class="line">    <span class="keyword">int</span> temp = array[start];</div><div class="line">    array[start] = array[end];</div><div class="line">    array[end] =  temp;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">heapSort</span><span class="params">(<span class="keyword">int</span>[] array)</span> </span>&#123;</div><div class="line">    <span class="keyword">int</span>  i = <span class="number">0</span>;</div><div class="line">    <span class="keyword">int</span> n = array.length;</div><div class="line">    <span class="comment">//从最后一个非叶子结点开始，依次将array[i]~array[n-1]调整为最大堆</span></div><div class="line">    <span class="keyword">for</span>(i = n/<span class="number">2</span> - <span class="number">1</span>;i &gt;= <span class="number">0</span>;i--) &#123;</div><div class="line">        adjustHeap(array,i,n-<span class="number">1</span>);</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = n - <span class="number">1</span>;i &gt; <span class="number">0</span>;i--) &#123;</div><div class="line">        swap(aray,<span class="number">0</span>,i);</div><div class="line">        adjustHeap(array,<span class="number">0</span>,i-<span class="number">1</span>);</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h4 id="算法稳定性"><a href="#算法稳定性" class="headerlink" title="算法稳定性"></a>算法稳定性</h4><p>&emsp;&emsp;堆排序是不稳定的算法，它不满足稳定算法的定义。堆排序算法在交换数据的时候，是比较父结点和子节点之间的数据，所以，即便是存在两个数值相等的兄弟节点，它们的相对顺序在排序也可能发生变化。</p>
<h4 id="算法复杂度"><a href="#算法复杂度" class="headerlink" title="算法复杂度"></a>算法复杂度</h4><p>&emsp;&emsp;堆排序的时间复杂度是$O(Nlog_2N)$。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="数据结构" scheme="http://yaodong.ml/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning算法笔记——判别模型与生成模型</title>
    <link href="http://yaodong.ml/machine-learning/Machine%20Learning%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B.html"/>
    <id>http://yaodong.ml/machine-learning/Machine Learning算法笔记——判别模型与生成模型.html</id>
    <published>2017-01-03T10:43:52.000Z</published>
    <updated>2017-03-14T07:30:12.275Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;本博文简单介绍了判别模型与生成模型的定义、优缺点以及相互之间的关系。<br><a id="more"></a></p>
<h3 id="生成模型和判别模型的定义"><a href="#生成模型和判别模型的定义" class="headerlink" title="生成模型和判别模型的定义"></a>生成模型和判别模型的定义</h3><p>&emsp;&emsp;监督学习的任务就是从数据中学习一个模型（也叫分类器），对给定的输入<strong>X</strong>预测相应的输出<strong>Y</strong>。决策函数为<strong>Y=<em>f</em>(X)</strong>或者条件概率分布<strong>P(Y|X)</strong>。实际上通过条件概率分布<strong>P(Y|X)</strong>进行预测也是隐含着表达成决策函数<strong>Y=<em>f</em>(X)</strong>的形式的。<br>&emsp;&emsp;监督学习方法分为<strong>生成方法</strong>（Generative approach）和<strong>判别方法</strong>（Discriminative approach），相应的机器学习模型分别称为<strong>生成模型</strong>（Generative Model）和<strong>判别模型</strong>（Discriminative Model）：</p>
<ul>
<li><strong>判别模型</strong>：由数据直接学习决策函数Y=f(X)或者条件概率分布P(Y|X)作为预测的模型，即判别模型。<strong>基本思想</strong>是有限样本条件下建立判别函数，不考虑样本的产生模型，直接研究预测模型。典型的判别模型包括<strong>k近邻</strong>，<strong>感知级</strong>，<strong>决策树</strong>，<strong>支持向量机</strong>等。</li>
<li><strong>生成模型</strong>：由数据学习联合概率密度分布<strong>P(X,Y)</strong>，然后求出条件概率分布<strong>P(Y|X)</strong>作为预测的模型，即生成模型：<strong>P(Y|X)= P(X,Y)/ P(X)</strong>。基本思想是首先建立样本的联合概率概率密度模型<strong>P(X,Y)</strong>，然后再得到后验概率<strong>P(Y|X)</strong>，再利用它进行分类。注意是先求<strong>P(X,Y)</strong>才得到<strong>P(Y|X)</strong>的，这个过程还要先求出<strong>P(X)</strong>。<strong>P(X)</strong>就是你的训练样本数据的概率分布。当数据样本非常多时，得到的<strong>P(X)</strong>才能很好的描述你数据真正的分布。典型的生成模型有：<strong>朴素贝叶斯</strong>和<strong>隐马尔科夫模型</strong>等。<h3 id="生成模型和判别模型的优缺点"><a href="#生成模型和判别模型的优缺点" class="headerlink" title="生成模型和判别模型的优缺点"></a>生成模型和判别模型的优缺点</h3>&emsp;&emsp;<strong>生成模型</strong>的特点：</li>
</ul>
<ul>
<li>生成方法学习联合概率密度分布<strong>P(X,Y)</strong>，所以可以从统计学的角度表示数据的分布情况，能够反映同类数据本身的相似度，但它不关心到底划分各类的分类边界在哪；生成方法可以还原出联合概率分布P(Y|X)，而判别方法不能；</li>
<li>生成方法的学习收敛速度更快，即当样本容量增加的时候，学习模型可以更快的收敛于真实模型，当存在隐变量时，仍可以用生成方法学习。此时判别方法就不能用。</li>
</ul>
<p>&emsp;&emsp;<strong>判别模型</strong>的特点：</p>
<ul>
<li>判别模型直接学习决策函数<strong>Y=<em>f</em>(X)</strong>或者条件概率分布<strong>P(Y|X)</strong>，因此不能反映训练数据本身的特性；</li>
<li>但它寻找不同类别之间的最优分类面，反映的是异类数据之间的差异。</li>
<li>直接面对预测，往往学习的准确率更高。</li>
<li>由于直接学习<strong>P(Y|X)</strong>或<strong>P(X)</strong>，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。</li>
</ul>
<h3 id="生成模型和判别模型的联系"><a href="#生成模型和判别模型的联系" class="headerlink" title="生成模型和判别模型的联系"></a>生成模型和判别模型的联系</h3><p>&emsp;&emsp;<strong>由生成模型可以得到判别模型，但由判别模型得不到生成模型</strong>。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;本博文简单介绍了判别模型与生成模型的定义、优缺点以及相互之间的关系。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yaodong.ml/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning算法笔记——特征工程</title>
    <link href="http://yaodong.ml/machine-learning/Machine%20Learning%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B.html"/>
    <id>http://yaodong.ml/machine-learning/Machine Learning算法笔记——特征工程.html</id>
    <published>2017-01-03T08:48:47.000Z</published>
    <updated>2017-03-14T07:30:00.897Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;<strong>特征工程（Feature Engineering）</strong>包括特征构建(<strong>Construction</strong>)、特征提取(<strong>Extraction</strong>)、特征选择(<strong>Selection</strong>)三个部分。本博文简单记录了特征工程的相关知识和实战应用经验。<br><a id="more"></a></p>
<blockquote>
<p><strong>Feature engineering</strong> is the process of using domain knowledge of the data to create features that make machine learning algorithms work. Feature engineering is fundamental to the application of machine learning, and is both difficult and expensive. The need for manual feature engineering can be obviated by automated feature learning<br><strong>特征工程</strong>是利用数据科学领域的相关知识来创建、提取、选择能使机器学习算法达到最佳性能的特征的过程。</p>
</blockquote>
<h3 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h3><p>&emsp;&emsp;特征选择，即从特征集合中挑选一组最具统计意义的特征子集，以提高机器学习算法的性能表现，并达到数据降维的效果。通常需要衡量单独每个特征与类别标签之间的相关性。实际机器学习应用中，表示单个特征与类别标签之间相关关系的参数指标有：<strong>皮尔逊相关系数</strong>、<strong>信息增益</strong>、<strong>信息增益比</strong>和<strong>基尼指数</strong>等。</p>
<h4 id="皮尔逊相关系数"><a href="#皮尔逊相关系数" class="headerlink" title="皮尔逊相关系数"></a>皮尔逊相关系数</h4><p>&emsp;&emsp;两个变量（特征与标签向量）之间的pearson相关系数定义为两个变量之间的协方差和标准差的商。计算公式为：</p>
<script type="math/tex; mode=display">\gamma^2_{xy}=\dfrac{cov(x,y)}{\sigma_x\sigma_y}=\dfrac{E[(X-\mu_x)(Y-\mu_y)]}{\sigma_x\sigma_y}</script><p>&emsp;&emsp;这里的$x$表示某个特征的观测值，$Y$表示类别标签。pearson相关系数的取值在0到1之间。</p>
<h4 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h4><p>&emsp;&emsp;信息增益的概念来源于信息科学的分支。<strong>熵</strong>（entropy）是随机变量不确定性的度量。熵越大，表示随机变量的不确定性就越大。<br>设随机变量X为有限个值的离散随机变量，其概率分布为</p>
<script type="math/tex; mode=display">P(X=x_i)=p_i</script><p>&emsp;&emsp;熵的定义为</p>
<script type="math/tex; mode=display">H(X)=-\sum_{i=1}^np_ilog(p_i)</script><p>&emsp;&emsp;条件熵：H(Y|X)表示已知随机变量X的条件下随机变量Y的不确定性，定义<script type="math/tex">H(Y|X)=\sum_{i=1}^np_iH(Y|X=x_i)</script><br>&emsp;&emsp;其中$p_i=P(X=x_i)$。这里X表示样本数据集的某个特征，即表示根据某个特征划分后，数据Y的熵。如果某个特征有更强的分类能力，则条件熵$H(Y|X)$越小，表示不确定性越小。</p>
<h4 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h4><p>&emsp;&emsp;信息增益定义为特征A对训练数据集D的信息增益 $g(D,A)$ 定义为集合D的经验熵 $H(D)$ 与特征A在给定条件下D的经验条件熵$H(D|A)$之差，即<script type="math/tex">g(D,A)=H(D)-H(D|A)</script><br> &emsp;&emsp;信息增益$g(D,A)$表示特征A使得对数据集D的分裂的不确定性减少的程度。所以信息增益越大，表明不确定性减小越多，即特征具有更强的分类能力。<br>&emsp;&emsp;根据信息增益准则的特征选择方法是：对训练数据集（或其子集）D,计算每个特征的信息增益，并比较其大小将其排序，选择最大的信息增益对应的特征。</p>
<h4 id="信息增益比"><a href="#信息增益比" class="headerlink" title="信息增益比"></a>信息增益比</h4><p>&emsp;&emsp;信息增益比也是度量特征分类能力的方法。特征A对训练数据集D的信息增益比$g_R(D,A)$定义为其信息增益与训练数据集D关于特征A的熵之比，即</p>
<script type="math/tex; mode=display">g_R(D,A)=\frac{g(D,A)}{H_A(D)}</script><p>&emsp;&emsp;其中</p>
<script type="math/tex; mode=display">H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}</script><p>&emsp;&emsp;$|D|$表示训练样本集D中样本数量，$|D_i|$表示训练数据D中特征A取第i个值的总数目。信息增益比越大，表明特征分类能力越强。<br>&emsp;&emsp;需要注意的是，<strong>以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题</strong>。</p>
<h4 id="基尼指数"><a href="#基尼指数" class="headerlink" title="基尼指数"></a>基尼指数</h4><p>&emsp;&emsp;基尼指数表示样本集合的不确定性程度，基尼指数越小，对应的特征分类能力越强。</p>
<h3 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h3><p>&emsp;&emsp;原则上讲，特征提取应该在特征选择之前。特征提取的对象是未经处理的原始数据（raw data），它的目的是自动地构建新的特征，将原始数据转换为一组具有明显物理现实意义或者统计意义或核的特征。实际的机器学习应用中，常见的特征提取的方法有：</p>
<ul>
<li><strong>PCA</strong>（Principal Component Analysis，主成分分析）</li>
<li><strong>ICA</strong> （Independent component analysis，独立成分分析）</li>
<li><strong>LDA</strong> （Linear Discriminant Analysis，线性判别分析）</li>
</ul>
<h3 id="特征构建"><a href="#特征构建" class="headerlink" title="特征构建"></a>特征构建</h3><p>&emsp;&emsp;特征构建指的是结合所研究问题的实际背景从原始数据中人工构建新的特征。这一步需要花大量的时间和精力去研究真实的数据，思考问题的潜在形式和数据结构，同时能够更好地应用到预测模型中。<br>&emsp;&emsp;特征构建需要很强的洞察力和分析能力，从原始数据中找出具有物理意义的特征，并将其处理成一个或一组新的特征，便于应用到机器学习算法模型中。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>&emsp;&emsp;特征工程包括特征提取、特征构建和特征选择这三个子问题。在实际的机器学习应用中，每一个步骤都很重要。将这三个子问题的重要性排序为：<script type="math/tex">特征构建>特征提取>特征选择</script><br>如果特征构建做的不好，则会直接影响特征提取，进而影响了特征选择，最终影响机器学习算法模型的性能表现。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;&lt;strong&gt;特征工程（Feature Engineering）&lt;/strong&gt;包括特征构建(&lt;strong&gt;Construction&lt;/strong&gt;)、特征提取(&lt;strong&gt;Extraction&lt;/strong&gt;)、特征选择(&lt;strong&gt;Selection&lt;/strong&gt;)三个部分。本博文简单记录了特征工程的相关知识和实战应用经验。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yaodong.ml/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Machine Learning" scheme="http://yaodong.ml/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning算法笔记——决策树与随机森林</title>
    <link href="http://yaodong.ml/machine-learning/Machine%20Learning%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.html"/>
    <id>http://yaodong.ml/machine-learning/Machine Learning算法笔记——决策树与随机森林.html</id>
    <published>2017-01-03T04:54:14.000Z</published>
    <updated>2017-03-14T07:29:29.345Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;本博文介绍了机器学习中经典的随机森林算法和决策树算法。<br><a id="more"></a></p>
<h2 id="决策树-Decision-Tree"><a href="#决策树-Decision-Tree" class="headerlink" title="决策树(Decision Tree)"></a>决策树(Decision Tree)</h2><p>&emsp;&emsp;决策树是一种基本的分类与回归算法，属于<strong>贪婪算法</strong>，其模型呈现为树形结构，可理解为基于特征或模型属性对实例进行分类或回归的过程<br>&emsp;&emsp;决策树的特点：</p>
<ul>
<li><strong>优点</strong>：计算复杂度不高，输出结果可直观理解数据，对中间值得缺失不敏感，可以处理不相关特征数据</li>
<li><strong>缺点</strong>：可能会出现Over Fittting</li>
<li>适用数据类型：数值型和标称型</li>
</ul>
<p>&emsp;&emsp;决策树的路径及其对应的<code>if-then</code>重要性质：路径之间是互斥且完备的。也就是说，每一个实例都被决策树的一条路径覆盖，且只能被一条路径或者一条规则覆盖。这里的覆盖是指实例的特征与路径上的特征或实例满足规则的条件</p>
<h3 id="决策树模型"><a href="#决策树模型" class="headerlink" title="决策树模型"></a>决策树模型</h3><p>&emsp;&emsp;决策树模型学习过程可分为3个步骤：<strong>特征选择</strong>，<strong>决策树的生成</strong>，<strong>决策树的修剪</strong></p>
<h4 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h4><p>&emsp;&emsp;构建决策树，实质是对训练数据集进行超平面划分，不同的样本特征在划分数据集时重要性不同，因此选择特征顺序的不同将会生成不同的决策树。为使数据集的分类结果更纯净，更能直观表达数据的本质属性，构造决策树之前先评估不同特征的重要性。</p>
<h3 id="决策树生成算法"><a href="#决策树生成算法" class="headerlink" title="决策树生成算法"></a>决策树生成算法</h3><p>&emsp;&emsp;<strong>ID3</strong>与<strong>C4.5</strong>都是决策树的经典分类决策树算法。<strong>ID3</strong>算法与<strong>C4.5</strong>算法的不同之处在于ID3算法采用信息增益作为特征选择准则，而C4.5采用的是信息增益比作为准则</p>
<h4 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a>ID3算法</h4><p>&emsp;&emsp;<strong>ID3</strong>算法的核心是在决策树的各个节点上应用信息增益准则选择特征，递归地构建决策树。<br>&emsp;&emsp;<strong>ID3</strong>算法的实质是用最大似然法进行概率模型的选择。算法思路为：</p>
<ul>
<li>从根节点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子节点</li>
<li>再对子结点递归调用以上方法，构建决策树。直到所有特征的信息增益均很小或没有特征可以选择为止。</li>
</ul>
<h4 id="C4-5算法"><a href="#C4-5算法" class="headerlink" title="C4.5算法"></a>C4.5算法</h4><p>&emsp;&emsp;<strong>ID3</strong>算法由于只有树的生成，所以该算法生成的树容易产生过拟合。<strong>C4.5</strong>算法对<strong>ID3</strong>算法进行了改进，选用信息增益比作为特征选择准则。</p>
<h3 id="决策树的剪枝"><a href="#决策树的剪枝" class="headerlink" title="决策树的剪枝"></a>决策树的剪枝</h3><p>&emsp;&emsp;用决策树生成算法递归产生决策树，容易出现过拟合，原因在于决策树的生成过程过多考虑如何提高对训练数据的正确分类，从而构建的决策树趋于复杂<br>&emsp;&emsp;在决策树学习中，将已生成的树进行简化的过程称为剪枝，具体来说就是剪掉一些子树或叶子结点，并将其根节点或父结点作为新的叶子结点，进而简化决策树模型<br>&emsp;&emsp;决策树的剪枝通过极小化决策树整体的损失函数（代价函数）来实现，即<script type="math/tex">C_\alpha(T)=C(T)+\alpha|T|</script></p>
<h4 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a>CART算法</h4><p>&emsp;&emsp;<strong>CART</strong>算法是一种既可以用于分类也可用作回归的决策树算法。<strong>CART</strong>算法分为以下两步：</p>
<ol>
<li>决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大</li>
<li>决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，此过程中，用损失函数最小化作为剪枝的标准</li>
</ol>
<hr>
<h2 id="随机森林算法"><a href="#随机森林算法" class="headerlink" title="随机森林算法"></a>随机森林算法</h2><h3 id="Bagging方法"><a href="#Bagging方法" class="headerlink" title="Bagging方法"></a>Bagging方法</h3><p>&emsp;&emsp;Bagging方法就是将所有training data放进一个“黑色”的bag中，然后从这个bag中随机抽取部分数据生成新的训练集。随机森林算法中，样本训练集本省可以使用bagging方法，同样，样本的feature也可以进行bagging。从随机性来看，bagging技术可以有效的减小方差，即减小过拟合程度<br>&emsp;&emsp;随机森林是一种经典而强大的机器学习算法，具有回归和分类的功能。随机森林算法由若干决策树组成，这些决策树一般采用随机的方法生成，因此也叫做随机决策树。随机森林算法中的各决策树之间是没有关联的。</p>
<p>&emsp;&emsp;随机森林算法的<strong>特点</strong>：</p>
<ul>
<li>适合用于多分类问题，算法训练和预测速度快，容易实现并行化</li>
<li>可有效估计缺失数据，即有一定程度的数据容错能力，当数据集中有大比例的数据缺失时仍然可以保持精度不变和能够有效地处理大的数据集</li>
<li>不会出现过拟合</li>
<li>能够处理很高维度（feature很多）的数据，并且不用做特征选择</li>
<li>对数据集的适应能力强：既能处理离散型数据，也能处理连续型数据，数据集无需规范化</li>
<li>可检测样本的各特征（维度）之间的相互影响程度，判断特征对所解决问题的重要性程度</li>
<li>可直接处理大规模的的变量（群）</li>
<li>在创建森林即分类的过程中，对泛化误差的估计是内部无偏估计</li>
<li>随机森林算法也是一种数据降维的方法，用于处理缺失值、异常值</li>
</ul>
<h3 id="Bootstrap抽样"><a href="#Bootstrap抽样" class="headerlink" title="Bootstrap抽样"></a>Bootstrap抽样</h3><p>&emsp;&emsp;随机森林算法中包含了对输入数据的重复自抽样过程，即所谓的bootstrap抽样。大约三分之一的数据集将用于测试而不是模型的训练，这样的数据被称为out of bag samples。</p>
<p>&emsp;&emsp;bootstrap抽样与bagging的区别是：在生成每棵树的时候，每个节点变量都仅仅在随机选出的少数变量中产生。因此，不但样本是随机的，连每个节点变量（Features）的产生都是随机的。<br>&emsp;&emsp;综上可知，随机森林算法的的两个随机采样的过程保证了随机性，所以即使对最终的各决策树不剪枝，也不会出现over-fitting。</p>
<h3 id="随机森林算法的具体步骤："><a href="#随机森林算法的具体步骤：" class="headerlink" title="随机森林算法的具体步骤："></a>随机森林算法的具体步骤：</h3><ol>
<li>通过自助法（bootstrap）重采样技术，从原始训练样本集中有放回地重复随机抽取N个样本生成新的训练样本集合</li>
<li>对随机采样得到的新训练数据集，构建决策树，在每个节点执行以下操作：</li>
</ol>
<ul>
<li>从样本数据的M个features中随机选取m($m&lt;&lt;M$)个feature</li>
<li>对这m个features，选择特定的度量准则分割节点</li>
<li>重复上述操作N次，从而生成与样本数量相等的决策树</li>
</ul>
<ol>
<li>对于每一个测试样例，对k颗决策树的预测结果进行投票。票数最多的结果就是随机森林的分裂（预测）结果</li>
</ol>
<h3 id="scikit-learn中的Random-Forest算法"><a href="#scikit-learn中的Random-Forest算法" class="headerlink" title="scikit-learn中的Random Forest算法"></a>scikit-learn中的Random Forest算法</h3><p>参考代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sklearn.ensemble.RandomForestClassfier</div><div class="line">clf = RandomForestClassifier([parameters]) <span class="comment">#generator the entity object of classifier</span></div><div class="line">parameters:</div><div class="line">n_estimators：指定随机森林中树的数目，越多越好，不超过内存即可</div><div class="line">criterion:指定在分裂使用的决策算法，取值有“entropy”、“gini”等</div><div class="line">max_features:单个决策树使用特征的最大数量，取值为<span class="string">"Auto"</span>，<span class="string">"None"</span>，<span class="string">"sqrt"</span>，<span class="string">"0.X"</span>。回归问题，max_features=n_features,分类问题，max_features=sqrt(n_features),<span class="string">"sqrt"</span>即为全部特征数目的平均根</div><div class="line">max_depth:默认为<span class="keyword">None</span>，一般可不改动</div><div class="line">min_simples_split:</div><div class="line">min_samples_leaf:最小叶片大小。默认值为<span class="number">1</span>，可设置为<span class="number">50</span>。叶是决策树的末端节点，较小的叶子使模型更容易捕捉训练数据中的噪声。</div><div class="line">min_weight_fraction_leaf:</div><div class="line">max_leaf_nodes:</div><div class="line">min_impurity_split:</div><div class="line">bootstrap:</div><div class="line">oob_score:这是一个随机森林交叉验证方法，取值为boolean类型，<span class="string">"True"</span>,<span class="string">"False"</span></div><div class="line">n_jobs:指定并行训练时使用的进程数。“<span class="number">-1</span>”表示使用所有处理器</div><div class="line">random_state:经验值<span class="string">"random_state=50"</span></div><div class="line">verbose:</div><div class="line">warm_state:</div><div class="line">class_weight:</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;本博文介绍了机器学习中经典的随机森林算法和决策树算法。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Machine Learning" scheme="http://yaodong.ml/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>不忘初心，未来可期——2017启笔</title>
    <link href="http://yaodong.ml/writings/%E4%B8%8D%E5%BF%98%E5%88%9D%E5%BF%83%EF%BC%8C%E6%9C%AA%E6%9D%A5%E5%8F%AF%E6%9C%9F%E2%80%94%E2%80%942017%E5%90%AF%E7%AC%94.html"/>
    <id>http://yaodong.ml/writings/不忘初心，未来可期——2017启笔.html</id>
    <published>2017-01-01T14:39:05.000Z</published>
    <updated>2017-03-14T06:53:14.794Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;博客好久没更了，感觉一直都在碌碌无为地忙碌着。毫无准备地跨进了2017，哦，本命年。坐在这里也不知道该写点什么，过去的一年里有遗憾，有后悔，一切都回不去了。<br>&emsp;&emsp;2017，不忘初心，不辜负。<br><a id="more"></a><br>&emsp;&emsp;总感觉新年和圣诞节是不一样的。圣诞节是西方的一个宗教节日，国人大概只是给狂欢冠以堂皇之名吧。一直在寻找一个时间节点，去重启自己的生活节奏，1月1日新年伊始，这个time point不能再合适。嗯，过往的二十年里一直都是这样拖延和给自己找借口，大多杰出的人总能从当下开始，而我，总要拖到下一个自己设定的时间点。<br>&emsp;&emsp;不知道习惯性三分钟热度的自己，十天半个月之后会不会已经忘了自己写过这篇博客，忘记博客中吹过的牛逼。或者，未来的某个时间，觉得这篇博客在打脸，偷偷地删掉博客也是有可能的哦。<br>&emsp;&emsp;过去这一年都干了什么？脑海里好像已经没有太多值得回忆的片段，过去的一年里，没有上进心，没有太多生活的动力。似乎还没有从保研的那一年的生活节奏中走出来。<strong>保研后遗症好严重，嗯，把自己懒和不努力的锅甩给了保研</strong>。<br>&emsp;&emsp;仔细想来，能第一时间浮现在脑海的应该也是大喜大悲过的吧。1月份时研一考试周天天泡纪忠楼206复习看书，期末考试没考好，好几门课的成绩渣到爆，后来因为学位课成绩很差而不开心好一阵子。从研一下开始已经决意脱离通信，入了计算机的坑。研一下天天看教程学Java，现在撸代码的速度已经可以和单身贵族的手速相媲美了。研一时的冲动和不成熟导致跟女朋友分手，之后的整个研一下都在浑浑噩噩中度过，那段时间算是读研以来最灰暗的日子了，从那之后研一剩下的几个月都发生了什么，我都没有太多记忆了。从那时才知道网易云音乐每首歌的评论列表都是一个个伤心的故事，也一天天地单曲循环<code>我不愿让你一个人</code>、<code>只是没有如果</code>、<code>走着走着就散了</code>；也在从那时开始养成了一个人打篮球的习惯，到现在还能保持空位投篮60%以上的命中率。校运动会时跟范特、柳旭一起组队代表学院参加了1分钟投篮大赛，也是从那时开始，三人变老铁。大概是因为研一下跟大师姐经常一起上课吧，上课经常性跑神，大师姐会经常给灌鸡汤，就这样后来和大师姐也成为了很好的朋友。感谢你们一直都在，昨天还一起跨年，特哥，旭哥，大师姐，新年快乐！<br>&emsp;&emsp;研一结束时搬回四牌楼校区，在九龙湖校区的一年基本没有留下太多回忆，更多的是不甘。下半年的生活似乎也没有太多波澜了：每个月按时给导师交学习报告、刷算法刷leetcode、看看机器学习知识、水一水比赛。暑假基本没回家，7月份主要在刷数据结构和算法，用Github+Hexo搭建了博客；8月份主要学了机器学习的理论基础和Java Web。9月中旬左右被导师派往苏州出差到现在。平时除了做导师的通信项目之外，一天天都在瞎折腾，机器学习、算法、Java、Python、数据比赛、Linux、网络爬虫…，也没学明白个所以然来。研二上这半年的黄金时间又被自己霍霍没了，WTF。12月份把微信朋友圈停用了，转向<a href="http://weibo.com/u/2685489433" target="_blank" rel="external">微博</a>的怀抱。会不会，朋友圈是下一个QQ空间，微博又是下一个朋友圈…<br>&emsp;&emsp;现在总结来，在过去的一年里，应该是在不断交学费的过程中更成熟了吧：不再高傲自大；多一点耐心，不要对自己爱的人发脾气；做决定先考虑后果，任何时候不要把事情搞成无法挽回的局面。</p>
<hr>
<p>&emsp;&emsp;今天是2017年第一天，新的一年祝看到博客的大家天天好心情，校招季拿到心仪的offer！<br>&emsp;&emsp;深夜坐在电脑前写这篇博客，依然会有20多年一如既往的那种“放假回家我要把书都带上，回家好好学习”的既视感。新的一年，总该给自己定几个小目标，哪怕实现不了装装逼也是好的。<br>&emsp;&emsp;背景音乐是<code>who is fancy</code>的<strong><code>goodbye</code></strong>，音乐荒，求推荐好听的歌曲，语种不限。<br>&emsp;&emsp;晚安！</p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=30706076&auto=1&height=66"></iframe>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;博客好久没更了，感觉一直都在碌碌无为地忙碌着。毫无准备地跨进了2017，哦，本命年。坐在这里也不知道该写点什么，过去的一年里有遗憾，有后悔，一切都回不去了。&lt;br&gt;&amp;emsp;&amp;emsp;2017，不忘初心，不辜负。&lt;br&gt;
    
    </summary>
    
      <category term="随笔" scheme="http://yaodong.ml/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
  </entry>
  
  <entry>
    <title>算法设计思想之分支界限</title>
    <link href="http://yaodong.ml/algorithms/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3%E4%B9%8B%E5%88%86%E6%94%AF%E7%95%8C%E9%99%90.html"/>
    <id>http://yaodong.ml/algorithms/算法设计思想之分支界限.html</id>
    <published>2016-12-15T08:13:30.000Z</published>
    <updated>2017-03-14T07:30:38.507Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;本博文记录了博主关于分支界限算法设计思想的学习总结。<br><a id="more"></a><br>&emsp;&emsp; 类似于回溯法，<strong>分支限界法（branch-and-bound method）</strong>也是一种在问题的解空间树T上搜索问题解的算法。但分支限界法的求解目标则是找出满足约束条件的一个解，或是在满足约束条件的解中找出使某一目标函数值达到极大或极小的解，即在某种意义下的最优解。<br>&emsp;&emsp;<strong>分支限界法（branch-and-bound method）</strong>， “<strong>分支</strong>”是指采用广度优先遍历的策略，依次生成扩展结点的所有分支（即：儿子结点）；“<strong>限界</strong>”是在结点扩展过程中，计算结点的上界（或下界），并根据“剪枝函数”剪去搜索树的某些分支，从而提高搜索效率。</p>
<h3 id="分支限界算法的基本思想"><a href="#分支限界算法的基本思想" class="headerlink" title="分支限界算法的基本思想"></a>分支限界算法的基本思想</h3><p>&emsp;&emsp;按照广度优先遍历的原则，一个活结点一旦成为扩展结点（E-结点）R后，算法将依次生成它的全部孩子结点，将那些导致不可行解或导致非最优解的儿子舍弃，其余儿子加入活结点表中。然后，从活结点表中取出一个结点作为当前扩展结点。重复上述结点扩展过程，直至找到问题的解或判定无解为止。</p>
<h3 id="常见的分支限界法"><a href="#常见的分支限界法" class="headerlink" title="常见的分支限界法"></a>常见的分支限界法</h3><h4 id="FIFO分支限界法-队列式分支限界法"><a href="#FIFO分支限界法-队列式分支限界法" class="headerlink" title="FIFO分支限界法(队列式分支限界法)"></a>FIFO分支限界法(队列式分支限界法)</h4><p>&emsp;&emsp;基本思想：按照队列先进先出(FIFO)原则选取下一个活结点为扩展结点。<br>&emsp;&emsp;搜索策略：一开始，根结点是唯一的活结点，根结点入队。从活结点队中取出根结点后，作为当前扩展结点。对当前扩展结点，先从左到右地产生它的所有儿子，用约束条件检查，把所有满足约束函数的儿子加入活结点队列中。再从活结点表中取出队首结点（队中最先进来的结点）为当前扩展结点，……，直到找到一个解或活结点队列为空为止。</p>
<h4 id="Least-Cost分支限界法-优先队列式分支限界法"><a href="#Least-Cost分支限界法-优先队列式分支限界法" class="headerlink" title="Least Cost分支限界法(优先队列式分支限界法)"></a>Least Cost分支限界法(优先队列式分支限界法)</h4><p>&emsp;&emsp;基本思想：为了加速搜索的进程，应采用有效地方式选择活结点进行扩展。按照优先队列中规定的优先级选取优先级最高的结点成为当前扩展结点。<br>&emsp;&emsp;搜索策略：对每一活结点计算一个优先级（某些信息的函数值），并根据这些优先级；从当前活结点表中优先选择一个优先级最高（最有利）的结点作为扩展结点，使搜索朝着解空间树上有最优解的分支推进，以便尽快地找出一个最优解。再从活结点表中下一个优先级别最高的结点为当前扩展结点，……，直到找到一个解或活结点队列为空为止。</p>
<h3 id="分支限界算法与回溯法的区别"><a href="#分支限界算法与回溯法的区别" class="headerlink" title="分支限界算法与回溯法的区别"></a>分支限界算法与回溯法的区别</h3><p>&emsp;&emsp;分支限界算法与回溯法都是在所给定问题的解空间树上搜索问题的解的算法。但二者也有一些不同之处：</p>
<ul>
<li><strong>算法目标不同</strong>：回溯算法的目的是找出解空间树中满足约束条件的所有解，而分支限界法的求解目标则是找出满足约束条件的一个解，或是在满足约束条件的条件下找出在某种意义下的最优解。 </li>
<li><strong>搜索方式的不同</strong>：回溯法以深度优先遍历的方式搜索解空间树，而分支限界法则以广度优先遍历或以最小耗费优先的方式搜索解空间树</li>
</ul>
<h3 id="分支界限的应用"><a href="#分支界限的应用" class="headerlink" title="分支界限的应用"></a>分支界限的应用</h3><ol>
<li>队列式分支限界法：按照队列先进先出（FIFO）原则选取下一个结点为扩展结点。 </li>
<li>优先队列式分支限界法：按照优先队列中规定的优先级选取优先级最高的结点成为当前扩展结点。</li>
<li>单源最短路径问题</li>
<li>装载问题、批处理作业问题、布线问题</li>
<li>0-1背包问题</li>
<li>旅行售货员问题</li>
<li>栈式搜索方法，按照FILO</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;本博文记录了博主关于分支界限算法设计思想的学习总结。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="算法" scheme="http://yaodong.ml/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="算法设计" scheme="http://yaodong.ml/tags/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>算法设计思想之回溯算法</title>
    <link href="http://yaodong.ml/algorithms/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3%E4%B9%8B%E5%9B%9E%E6%BA%AF%E7%AE%97%E6%B3%95.html"/>
    <id>http://yaodong.ml/algorithms/算法设计思想之回溯算法.html</id>
    <published>2016-12-15T08:12:20.000Z</published>
    <updated>2017-03-14T07:30:53.607Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;本博文记录了博主关于回溯算法设计思想的学习总结。<br><a id="more"></a></p>
<blockquote>
<p><strong>回溯（Backtracking）算法</strong>也叫试探法，属于暴力求解的范畴。<strong>回溯（Backtracking）算法</strong>是一种既有系统性又有跳跃性的的搜索算法，适用于求解具有约束条件，并且有多个候选解的问题。</p>
</blockquote>
<h3 id="回溯算法的设计思想"><a href="#回溯算法的设计思想" class="headerlink" title="回溯算法的设计思想"></a>回溯算法的设计思想</h3><p>&emsp;&emsp;<strong>回溯算法</strong>采用试探的思想，尝试分步解决一个问题。在分步解决问题的过程中，当它通过尝试发现现有的分步答案不能得到有效的正确的解答的时候，它将取消上一步甚至是上几步的计算，再通过其它的可能的分步解答再次尝试寻找问题的答案。</p>
<p>&emsp;&emsp;<strong>回溯算法</strong>在包含问题的所有解的解空间树中，按照深度优先遍历的策略，从根结点出发搜索整个解空间树。<br>&emsp;&emsp;当算法搜索至解空间树的任一结点时，先判断该结点是否肯定不包含问题的解。如果肯定不包含，则跳过对以该结点为根的子树的所有搜索，病逐层向其祖先结点回溯。否则，进入该子树，继续按深度优先遍历的策略进行搜索。<br>&emsp;&emsp;<strong>回溯算法</strong>通常用最简单的递归方法来实现，反复重复上述的步骤后会出现两种情况：</p>
<ol>
<li>找到了符合要求的正确答案</li>
<li>遍历了所有可能的分步方法后，宣告该问题没有符合要求的解</li>
</ol>
<p>&emsp;&emsp;回溯法是设计递归过程的一种重要方法，回溯算法的实质是先序遍历一颗状态树的过程，只不过这棵状态树不是遍历前预先建立的，而是隐含在遍历过程中。</p>
<h3 id="回溯算法的相关概念"><a href="#回溯算法的相关概念" class="headerlink" title="回溯算法的相关概念"></a>回溯算法的相关概念</h3><h4 id="约束函数-amp-限界函数"><a href="#约束函数-amp-限界函数" class="headerlink" title="约束函数&amp;限界函数"></a>约束函数&amp;限界函数</h4><p>&emsp;&emsp;约束函数可根据所求解问题中的限制条件构造。约束函数描述了给定问题的合法解的一般特征，用于DFS深度优先遍历过程中去除不合法的解，从而避免无效搜索。此外，约束函数是对于任何状态空间树上的节点都有效、等价的。</p>
<h4 id="状态空间树"><a href="#状态空间树" class="headerlink" title="状态空间树"></a>状态空间树</h4><p>&emsp;&emsp;状态空间树是对问题的所有解的图形描述。树上的每个子节点的解都只有一个部分与父节点不同。</p>
<h4 id="扩展节点、活结点、死结点"><a href="#扩展节点、活结点、死结点" class="headerlink" title="扩展节点、活结点、死结点"></a>扩展节点、活结点、死结点</h4><p>&emsp;&emsp;扩展节点，是当前正在求出它的子节点的节点，在DFS中，只允许有一个扩展节点。<br>&emsp;&emsp;节点本身和其父节点满足约束函数和限界条件的结点称为活结点。活结点需要进行DFS递归遍历<br>&emsp;&emsp;死结点反之，死结点是不满足约束函数的结点，DFS过程不必遍历死结点的子节点。</p>
<h3 id="回溯算法的求解步骤"><a href="#回溯算法的求解步骤" class="headerlink" title="回溯算法的求解步骤"></a>回溯算法的求解步骤</h3><ul>
<li>针对所给问题，定义问题的解空间</li>
<li>确定易于搜索的解空间结构</li>
<li>构造约束函数和限界函数，避免冗余的无效搜索</li>
<li>以深度优先遍历（<strong>DFS</strong>）搜索解空间树，并在搜索过程中用剪枝函数避免无效搜索</li>
</ul>
<h3 id="回溯算法的适用情形"><a href="#回溯算法的适用情形" class="headerlink" title="回溯算法的适用情形"></a>回溯算法的适用情形</h3><ol>
<li>给定的问题有很多组解，要求寻找问题的解集或者寻找满足某些约束条件的最佳解时，可以考虑使用回溯法</li>
<li>回溯算法虽然属于暴力求解范畴，但是回溯思想能避免很多不必要的穷举式搜索。因此回溯算法适用于求解一些组合数很大的问题</li>
<li>回溯算法的终止条件要注意以下两种情况：</li>
</ol>
<ul>
<li>回溯算法用于求问题的所有解时，要回溯到解状态树的根结点，且根结点的所有子树都已被搜索遍才结束</li>
<li>回溯算法用于求问题的任一解时，只要搜索到问题的一个解就可以结束</li>
</ul>
<h3 id="回溯算法的应用"><a href="#回溯算法的应用" class="headerlink" title="回溯算法的应用"></a>回溯算法的应用</h3><ul>
<li>八皇后问题</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;本博文记录了博主关于回溯算法设计思想的学习总结。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="算法" scheme="http://yaodong.ml/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="算法设计" scheme="http://yaodong.ml/tags/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>算法设计思想之贪心算法</title>
    <link href="http://yaodong.ml/algorithms/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3%E4%B9%8B%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95.html"/>
    <id>http://yaodong.ml/algorithms/算法设计思想之贪心算法.html</id>
    <published>2016-12-15T08:11:25.000Z</published>
    <updated>2017-03-14T07:31:00.618Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;本博文记录了博主关于贪心算法设计思想的学习总结。<br><a id="more"></a></p>
<blockquote>
<p><strong>贪心算法（greedy algorithm）</strong>，又称<strong>贪婪算法</strong>，在每一步都选择当前状态下能达到最好或最优的策略，从而希望算法结束时的结果就是全局最优的算法。换句话说，贪心算法不考虑全局只考虑局部最优，基于当前状态做出局部最优选择。</p>
</blockquote>
<h3 id="贪心算法的基本步骤"><a href="#贪心算法的基本步骤" class="headerlink" title="贪心算法的基本步骤"></a>贪心算法的基本步骤</h3><p>&emsp;&emsp;贪心算法把原问题可分解为多个子问题，然后贪心算法以迭代的方式作出每一步的贪心选择策略，每作一次贪心选择就将原复杂问题化简为规模更小的子问题。一直迭代求解子问题的局部最优解，最终子问题的最优解能递推到原复杂问题的最优解。<br>&emsp;&emsp;<strong>贪心算法求解问题的步骤</strong>：</p>
<ol>
<li>对问题进行分析，确定原问题的最优子结构</li>
<li>针对分解后的各子问题，设计递归求解算法</li>
<li>证明原问题总是有一个最优解是贪心选择得到的，即证明贪心选择是安全的</li>
<li>证明剩余的子问题的最优解结合贪心选择策略，可以得到原问题的最优解</li>
<li>设计出一个实现贪心策略的递归算法，并将递归算法转换成迭代算法</li>
</ol>
<p>&emsp;&emsp;贪心算法通常用于求解一些最优化问题。 贪心算法并不总能求得问题的整体最优解，但是由于贪心法的高效性以及其所求得的答案通常很接近最优结果，贪心法也可以用作辅助算法或者直接解决一些要求结果不特别精确的问题。</p>
<h3 id="贪心算法的适用情形"><a href="#贪心算法的适用情形" class="headerlink" title="贪心算法的适用情形"></a>贪心算法的适用情形</h3><ol>
<li>给定的问题是否能使用贪心算法策略求解，首先要确定该问题是否具有<strong>贪心选择性质</strong>。贪心选择性质是指一系列子问题的局部最优策略可以导致产生全局最优解。给定问题的整体最优解可以通过这些子问题的局部最优解得到。这也是贪心算法与动态规划算法的主要区别</li>
<li>给定问题是否具有贪心选择性质，需要证明子问题每一步进行的贪心选择最终导致问题的整体最优解。</li>
<li>一个问题的最优解包含其子问题的最优解，或者局部最优解能决定全局最优解，则称此问题具有<strong>最优子结构性质</strong>。贪心算法可用于求解具有最优子结构性质的问题。<br>因此当问题具有最优子结构性质时，可以考虑使用贪心算法。</li>
<li>贪心算法中，每一步的贪心选择可以依赖于以往的选择策略，但决不依赖于未来子问题的选择，也不依赖于将来子问题的解。简单来说，<strong>Greedy Algorithm</strong>选择的贪心策略必须具备无后效性，即将来子问题的选择策略不会影响先前子问题的求解。所以对所采用的贪心策略一定要仔细分析其是否满足<strong>无后效性</strong>。</li>
</ol>
<h3 id="贪心算法与动态规划算法的区别"><a href="#贪心算法与动态规划算法的区别" class="headerlink" title="贪心算法与动态规划算法的区别"></a>贪心算法与动态规划算法的区别</h3><ol>
<li>动态规划算法通常以自底向上的方式迭代求解各子问题，而贪心算法则通常以自顶向下的方式对问题进行求解。</li>
<li>在动态规划算法中，原问题每步所作的选择往往依赖于相关子问题的解。只有在求解相关子问题之后，原问题才能作出在当前状态的选择。而在贪心算法中，仅在当前状态下作出最好选择，即局部最优解，然后再求解相应的子问题。</li>
<li>贪心算法与动态规划的不同在于它每对每个子问题的解决方案都做出选择，不能回退。动态规划则会保存以前的运算结果，并根据以前的结果对当前进行选择，有回退功能。</li>
</ol>
<h3 id="贪心算法的应用"><a href="#贪心算法的应用" class="headerlink" title="贪心算法的应用"></a>贪心算法的应用</h3><ul>
<li><code>Dijkstra</code>算法</li>
<li><code>Prim</code>算法</li>
<li><code>Kruskal</code>算法</li>
<li><code>Huffman</code>编码</li>
<li>磁盘文件的存储算法</li>
<li>生产调度问题</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;本博文记录了博主关于贪心算法设计思想的学习总结。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="算法" scheme="http://yaodong.ml/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="算法设计" scheme="http://yaodong.ml/tags/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>算法设计思想之分治算法</title>
    <link href="http://yaodong.ml/algorithms/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3%E4%B9%8B%E5%88%86%E6%B2%BB%E7%AE%97%E6%B3%95.html"/>
    <id>http://yaodong.ml/algorithms/算法设计思想之分治算法.html</id>
    <published>2016-12-15T08:08:14.000Z</published>
    <updated>2017-03-14T07:30:46.078Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;本博文记录了博主关于分治算法设计思想的学习总结。<br><a id="more"></a></p>
<blockquote>
<p><strong>分治法（Divide and Conquer）</strong>是基于多项分支递归的一种很重要的算法范式思想。分治的意思是“分而治之”，把复杂问题分解成多个规模较小、相对独立、与原问题形式相同的子问题，子问题的求解很简单很直观，原问题的解即子问题的解的合并。</p>
</blockquote>
<p>&emsp;&emsp;分治与递归经常同时应用在算法设计中，是诸多高效算法的基础，因此也产生了很多著名的高效算法。如（快速排序、归并排序）、傅里叶变换等。</p>
<h3 id="分治算法的设计思想"><a href="#分治算法的设计思想" class="headerlink" title="分治算法的设计思想"></a>分治算法的设计思想</h3><ul>
<li>将计算规模较大的复杂问题，分割成规模较小的相同性质的多个子问题，对简单的子问题进行逐个击破，分而治之</li>
<li>使用数学上的不完全归纳法，寻找求解原问题的方程表达式，根据方程式设计递归程序Solution</li>
</ul>
<h3 id="分治算法的递归求解步骤"><a href="#分治算法的递归求解步骤" class="headerlink" title="分治算法的递归求解步骤"></a>分治算法的递归求解步骤</h3><ol>
<li><strong>分解</strong>：将原问题分解为若干个规模较小，相对独立，与原问题形式相同的子问题。</li>
<li><strong>求解</strong>：若子问题规模较小且易于解决时，则直接解。否则，递归地解决各子问题。</li>
<li><strong>合并</strong>：将各子问题的解合并为原问题的解。</li>
</ol>
<h3 id="分治算法的适用情形"><a href="#分治算法的适用情形" class="headerlink" title="分治算法的适用情形"></a>分治算法的适用情形</h3><ol>
<li>原复杂问题的规模缩减到一定程度后可以很容易求解</li>
<li>原问题可分解成若干个规模较小、与原问题性质相同的子问题，即原问题具有<strong>最优子结构</strong>性质，这条特性反映了递归思想</li>
<li>原复杂问题分解得到的子问题相互之间是独立的，即<strong>子问题之间不包含公共的问题</strong></li>
<li>子问题的解可以合并为原复杂问题的解</li>
<li>算法求解过程中至少含有两个递归调用，也就是说，只进行一次递归调用的不属于分治算法范畴</li>
</ol>
<h3 id="分治算法与动态规划算法的区别"><a href="#分治算法与动态规划算法的区别" class="headerlink" title="分治算法与动态规划算法的区别"></a>分治算法与动态规划算法的区别</h3><ol>
<li>分治法与动态规划都要求原问题具有最有子结构，都是将问题分解成若干个规模较小的子问题</li>
<li>动态规划是将原问题分解为多个相同性质的子问题，这些子问题相互之间有联系，有重叠。分治法将分解后的子问题看成是相互独立的。</li>
<li>动态规划通过迭代法自底向上求解，通过子问题的求解进而寻找原问题的最优解。而分治法是利用递归对各个子问题独立求解，最后将各子问题的解进行合并形成原问题的解。</li>
<li>若原复杂问题分解之后得到的各子问题之间不独立，则可以考虑使用动态规划算法</li>
</ol>
<h3 id="分治算法的复杂度分析"><a href="#分治算法的复杂度分析" class="headerlink" title="分治算法的复杂度分析"></a>分治算法的复杂度分析</h3><p>&emsp;&emsp;假设分治算法策略将原问题分解为<code>K</code>个计算规模为<code>n/m</code>的同质子问题，<code>m</code>表示问题规模减小的因子，$f(n)$为分解与合并子问题的计算复杂度。原问题的时间复杂度可用如下的递归表达式表示：</p>
<script type="math/tex; mode=display">T(n)=KT(n/m)+f(n)</script><h3 id="分治算法的应用"><a href="#分治算法的应用" class="headerlink" title="分治算法的应用"></a>分治算法的应用</h3><ul>
<li>二分查找</li>
<li>大整数乘法</li>
<li>棋盘覆盖</li>
<li>快速排序</li>
<li>合并排序</li>
<li>Strassen矩阵乘法</li>
<li>最接近点对问题</li>
<li>汉诺塔</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;本博文记录了博主关于分治算法设计思想的学习总结。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="算法" scheme="http://yaodong.ml/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="算法设计" scheme="http://yaodong.ml/tags/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>算法设计思想之动态规划</title>
    <link href="http://yaodong.ml/algorithms/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3%E4%B9%8B%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92.html"/>
    <id>http://yaodong.ml/algorithms/算法设计思想之动态规划.html</id>
    <published>2016-12-15T07:59:11.000Z</published>
    <updated>2017-03-14T07:30:26.883Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;动态规划算法作为五大算法设计思想之一，重要性不言而喻。本博文记录了博主动态规划的学习总结。<br><a id="more"></a></p>
<blockquote>
<p><strong>动态规划</strong>（Dynamic programming，简称<strong>DP</strong>）是一种在数学、管理科学、计算机科学、经济学和生物信息学中使用的，通过把原问题分解为相对简单的子问题的方式求解复杂问题的方法。</p>
</blockquote>
<p>&emsp;&emsp;动态规划的基本思想：要求解给定问题的最优解，把给定的复杂问题分解成多个规模较小的子问题，且子问题是同质的。动态规划中，欲求给定问题的最优解，首先要求解子问题的最优解，$DP$对同 的子问题只求解一次，并存储子问题的求解结果，便于后续求解使用时查询。</p>
<p>&emsp;&emsp;动态规划算法设计思想中的两个重要概念：状态和状态转移方程。根据子问题定义状态，描述状态之间的如何转移的方程称为状态转移方程。大部分情况下，某个状态只与它前面出现的状态有关， 而独立于后面的状态。<br>&nbsp;<br>&emsp;&emsp;动态规划算法主要分为有两步：1.将给定的问题（或子问题）抽象为状态（对应空间复杂度） 2.根据状态推导出子问题之间的状态转移方程（对应时间复杂度）。</p>
<h3 id="动态规划算法的总结"><a href="#动态规划算法的总结" class="headerlink" title="动态规划算法的总结"></a>动态规划算法的总结</h3><p>&emsp;&emsp;动态规划通常用于解决最优化问题。<br>&emsp;&emsp;动态规划算法的时间复杂度远小于暴力求解和回溯法。<br>&emsp;&emsp;采用动态规划思想解决问题的前提是所给定问题具有<strong>最优子结构</strong>和<strong>重叠子问题</strong>的性质：</p>
<ul>
<li><strong>最优子结构</strong>是指问题的最优解包含其子问题的最优解。动态规划按照自底向上的策略利用最优子结构，即首先找到子问题的最优解，然后逐步向上寻找问题的一个最优解。最优子结构在求解的过程中以两种方式变化：</li>
</ul>
<ol>
<li>有多少个子问题被使用在原问题的一个最优解中</li>
<li>构建问题的最优解时使用子问题时有多少种选择或组合</li>
</ol>
<ul>
<li><strong>重叠子问题</strong>是指用来解决原问题的递归算法可以反复地解同样的子问题，而不是总是产生新的子问题。重叠子问题的一个重要性质是同一个算法被不断递归调用以求解同一性质的问题</li>
<li>简而言之，动态规划的核心就是寻找问题的最优子结构，找到最优子结构之后将问题分解成几个同质的子问题，使用同一算法递归求解子问题，并把每个子问题的求解结果保存在辅助表中，供求解后续问题时查询使用，每次查表的时间复杂度为常数</li>
</ul>
<p>&emsp;&emsp;<strong>算法思想</strong>是将给定的问题分解成几个相互不独立的子问题，各子问题之间是同质的，又称重叠子问题。对每个子问题只求解一次，并将其计算结果保存到辅助表中，便于后续子问题的计算。动态规划算法的设计步骤如下：</p>
<ol>
<li>描述问题的最优解的结构：分析给定问题出现最优解的情形，将给定的问题抽象化</li>
<li>将给定问题分解成同质的子问题，分析子问题如何才能得到最优解，并给出子问题最优解的递归公式</li>
<li>根据递归公式，采用自底向上的策略，计算每个子问题的最优解，将结果保存到辅助表中</li>
<li>根据每个子问题的最优解，选择或构造给定问题的最优解</li>
</ol>
<h3 id="动态规划的应用"><a href="#动态规划的应用" class="headerlink" title="动态规划的应用"></a>动态规划的应用</h3><ul>
<li><p>斐波那契数列</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * 使用动态规划算法思想求解斐波那契数列</div><div class="line"> * <span class="doctag">@param</span> num: 要求的斐波那契数列的第idx个数</div><div class="line"> * <span class="doctag">@return</span></div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">fibonacci</span><span class="params">(<span class="keyword">int</span> num)</span> </span>&#123;</div><div class="line">    <span class="keyword">int</span>[] fib = <span class="keyword">new</span> <span class="keyword">int</span>[num];<span class="comment">//数组默认初始化元素全为0</span></div><div class="line">    <span class="keyword">if</span>(num &lt;= <span class="number">0</span>)</div><div class="line">        <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">    <span class="keyword">if</span>(num == <span class="number">1</span> || num == <span class="number">2</span>)</div><div class="line">        <span class="keyword">return</span> <span class="number">1</span>;</div><div class="line">    fib[<span class="number">0</span>] = <span class="number">1</span>;</div><div class="line">    fib[<span class="number">1</span>] = <span class="number">1</span>;</div><div class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">2</span>;i &lt; num;i++) &#123;</div><div class="line">        fib[i] = fib[i-<span class="number">1</span>] + fib[i-<span class="number">2</span>];</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> fib[num-<span class="number">1</span>];</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>求最长递增子序列（不连续）（LIS）</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * 动态规划思想求解最长非降子序列(longest increasing subsequence,LIS)</div><div class="line"> * 代码以辅助理解动态规划算法的思想为目的，根据实际需求对形参进行改动</div><div class="line"> * <span class="doctag">@param</span> </div><div class="line"> * <span class="doctag">@return</span> 最长非降子序列的长度</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">solution_LIS</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">int</span>[] series = &#123;<span class="number">5</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">8</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">9</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">1</span>&#125;;</div><div class="line">    <span class="keyword">int</span> num_length = series.length;</div><div class="line">    <span class="keyword">if</span> (num_length == <span class="number">0</span>) &#123;</div><div class="line">        <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">    &#125;</div><div class="line">    <span class="comment">//定义longest数组，longest[i]表示前i个数中以series[i]结尾的最长非降子序列的长度</span></div><div class="line">    <span class="keyword">int</span>[] longest = <span class="keyword">new</span> <span class="keyword">int</span>[num_length];</div><div class="line">    </div><div class="line">    <span class="keyword">int</span> len = <span class="number">1</span>;</div><div class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; num_length;i++) &#123;</div><div class="line">        longest[<span class="number">0</span>] = <span class="number">1</span>;</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>;j &lt; i;j++) &#123;</div><div class="line">            <span class="keyword">if</span>((series[j] &lt;= series[i]) &amp;&amp; (longest[j]+<span class="number">1</span> &gt; longest[i])) &#123;</div><div class="line">                longest[i] = longest[j] + <span class="number">1</span>;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">if</span>(longest[i] &gt; len)</div><div class="line">            len = longest[i];</div><div class="line">    &#125;   </div><div class="line">    <span class="keyword">return</span> len;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>0-1背包问题<br>&emsp;&emsp;一位游客去森林里探险，发现了一堆宝石，宝石的数量为$N$。他的背包的容量有限，只能带走一部分宝石。游客该如何选择哪些宝石带走，以获得最大利益？（宝石的编号为$0\sim N-1$）,编号为$i$的宝石的体积和价值分别为$vol[i]$和$w[i]$，背包的容量为$C$。</li>
<li>最大连续子序列之和<ul>
<li>最长公共子序列</li>
<li><strong>Leetcode 198</strong></li>
</ul>
</li>
</ul>
<h3 id="使用动态规划思想的算法"><a href="#使用动态规划思想的算法" class="headerlink" title="使用动态规划思想的算法"></a>使用动态规划思想的算法</h3><ul>
<li><strong>Floyd-Warshall</strong>算法</li>
<li><strong>Viterbi</strong>算法</li>
</ul>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul>
<li>Anker关于动态规划的总结：<a href="http://www.cnblogs.com/Anker/archive/2013/03/15/2961725.html" target="_blank" rel="external">《算法导论》读书笔记之动态规划</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;动态规划算法作为五大算法设计思想之一，重要性不言而喻。本博文记录了博主动态规划的学习总结。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="算法" scheme="http://yaodong.ml/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="算法设计" scheme="http://yaodong.ml/tags/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>算法学习笔记——树和二叉树</title>
    <link href="http://yaodong.ml/algorithms/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%A0%91%E5%92%8C%E4%BA%8C%E5%8F%89%E6%A0%91.html"/>
    <id>http://yaodong.ml/algorithms/算法学习笔记——树和二叉树.html</id>
    <published>2016-12-03T05:13:10.000Z</published>
    <updated>2017-03-14T07:31:19.952Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;本博文总结了树和二叉树的基本概念，简单介绍了二叉树的遍历。<br><a id="more"></a></p>
<h3 id="树与二叉树的相关概念"><a href="#树与二叉树的相关概念" class="headerlink" title="树与二叉树的相关概念"></a>树与二叉树的相关概念</h3><p>&emsp;&emsp;二叉树的左右子树有严格的顺序，不可颠倒，这是二叉树与普通树的关键区别。<br>&emsp;&emsp;<strong>二叉搜索树</strong>：二叉树的一个节点的左子节点的关键字值小于这个节点，右子节点的关键字值大于（或等于）这个父节点。<br>&emsp;&emsp;<strong>平衡树与非平衡树</strong>：左子节点与左子节点对称的树为平衡树，否则就是非平衡树。<br>&emsp;&emsp;<strong>满二叉树</strong>：满二叉树除最后一层无任何子节点外，其余每一层的所有结点都有两个子结点。也就是说，除叶子结点外的所有结点均有两个子结点。满二叉树的结点数达到最大值。<strong>满二叉树的所有叶子结点必须在同一层上</strong>。<br>&emsp;&emsp;<strong>完全二叉树</strong>：完全二叉树是由满二叉树而引出来的。对于深度为<strong>K</strong>的，有<strong>N</strong>个结点的二叉树，当且仅当其编号为<strong><em>i</em></strong>的结点与深度为<strong>K</strong>的满二叉树中编号为<strong><em>i</em></strong>的结点在二叉树中的位置一一对应时称之为完全二叉树。<br>&emsp;&emsp;<strong>霍夫曼树</strong>：一种特殊结构的二叉树，每个节点要么没有子节点，要么有两个子节点。<br>&emsp;&emsp;二叉树结合了有序数组和链表的优点：在二叉树中，数据的查找效率跟在有序数组中查找一样高，同时插入、删除数据的效率和在链表中一样高。<br>&emsp;&emsp;二叉树的工作效率：时间复杂度为<strong>O(logN)</strong>。总的来说，树对所有的数据存储操作都很高效。<br>&emsp;&emsp;满二叉树的相关性质：</p>
<ul>
<li>叶子结点只能出现在深度最大的一层；</li>
<li>非叶子结点的度一定为2；</li>
<li>在同等深度的二叉树中，满二叉树的结点个数最多，叶子结点也最多；</li>
</ul>
<p>&emsp;&emsp;完全二叉树的相关性质：</p>
<ul>
<li>叶子结点只能出现在最下的两层；</li>
<li>最下层的叶子结点一定集中在左部连续位置；</li>
<li>倒数第二层如有叶子结点存在，则一定都在右部连续位置；</li>
<li>如果某结点的度为1，则该结点一定只有左孩子，不可能存在只有右孩子的情况；</li>
<li>对于同样结点数的二叉树，完全二叉树的深度最小。</li>
<li>完全二叉树除最后一层外，每一层上的结点数均达到最大值；在最后一层上只缺少右子树的若干结点。</li>
</ul>
<p>&emsp;&emsp;<strong>满二叉树与完全二叉树的重要区别</strong>：<strong>满二叉树肯定是完全二叉树，完全二叉树不一定是满二叉树。</strong></p>
<h3 id="二叉树的性质"><a href="#二叉树的性质" class="headerlink" title="二叉树的性质"></a>二叉树的性质</h3><p>&emsp;&emsp;1. 在二叉树的第<strong>i</strong>层最多有$2^{i}-1$个结点（$i$）；<br>&emsp;&emsp;2. 深度为<strong>k</strong>的二叉树至多有个结点；<br>&emsp;&emsp;3. 对任何一棵二叉树T，如果其终端结点数为$n<em>0$，度为<strong>2</strong>的结点数为$n_2$，则$n_0=n_2+1$；<br>&emsp;&emsp;4. 具有<strong>n</strong>个结点的完全二叉树的深度为$\lfloor log</em>{2} n \rfloor+1$<br>&emsp;&emsp;5. 如果对一棵有<strong>n</strong>个结点的完全二叉树（其深度为$\lfloor log_{2} n \rfloor+1$）的结点按层序编号，对任一结点$i$都有：</p>
<ul>
<li>如果$i=1$，则结点$i$是二叉树的根结点，无双亲；如果$i&gt;1$，则其双亲是结点$\lfloor i/2 \rfloor$;</li>
<li>如果$2i&gt;n$，则结点$i$无左孩子，结点$i$为叶子结点，否则其左孩子是结点$2i$；</li>
<li>如果$2i+1&gt;n$，则结点$i$无右孩子，否则其右孩子是结点$2i+1$。</li>
</ul>
<h3 id="二叉树的存储结构"><a href="#二叉树的存储结构" class="headerlink" title="二叉树的存储结构"></a>二叉树的存储结构</h3><p>&emsp;&emsp;二叉树常用的存储结构是：<strong>孩子兄弟表示法</strong>和<strong>二叉链表</strong>。</p>
<h3 id="二叉树的遍历"><a href="#二叉树的遍历" class="headerlink" title="二叉树的遍历"></a>二叉树的遍历</h3><h4 id="前序遍历：根-左-右"><a href="#前序遍历：根-左-右" class="headerlink" title="前序遍历：根 左 右"></a><strong>前序遍历</strong>：根 左 右</h4><ol>
<li>递归遍历（较简单）</li>
<li>非递归遍历（借助<strong>Stack</strong>结构实现）：</li>
</ol>
<ul>
<li>当访问到任一结点<strong>p</strong>：输出结点<strong>p</strong>，并将该结点入栈；</li>
<li>判断结点<strong>p</strong>的左孩子是否为空：<ol>
<li>若不为空，并将<strong>p</strong>的左孩子置为当前结点，回到1，循环；</li>
<li>若为空，则对栈顶结点进行出栈操作，但不输出，并将待出栈结点的右孩子置为当前结点，判断是否为空：<pre><code> 1. 若不为空，回到1，循环操作；
 2. 若为空，则对栈顶结点进行出栈操作，但不输出，即重复上述操作
</code></pre></li>
</ol>
</li>
<li>直到栈为空且当前结点为空，则遍历结束<h4 id="中序遍历：左-根-右"><a href="#中序遍历：左-根-右" class="headerlink" title="中序遍历：左 根 右"></a><strong>中序遍历</strong>：左 根 右</h4></li>
<li>递归遍历（较简单）</li>
<li>非递归遍历（借助<strong>Stack</strong>结构实现）：</li>
</ul>
<ol>
<li>对任一结点<strong>p</strong>，若<strong>p</strong>的左孩子结点不为空，则将<strong>p</strong>入栈并将<strong>p</strong>的左孩子结点置为当前结点，一直对当前结点做相同处理；</li>
<li>若当前结点<strong>p</strong>左孩子为空，则输出结点<strong>p</strong>，然后将<strong>p</strong>的右孩子置为当前结点，并判断其是否为空：<ul>
<li>(1)：若不为空，重复1,2操作</li>
<li>(2)：若为空，则执行出栈操作，输出栈顶结点，并将栈顶结点的右孩子结点置为当前结点，并判断是否为空；重复(1), (2)操作；</li>
</ul>
</li>
<li>直至当前结点<strong>p</strong>为<code>NULL</code>且栈为空，遍历结束；<h4 id="后序遍历：左-右-根"><a href="#后序遍历：左-右-根" class="headerlink" title="后序遍历：左 右 根"></a><strong>后序遍历</strong>：左 右 根</h4></li>
</ol>
<ul>
<li>递归遍历（较简单）</li>
<li>非递归遍历（借助<strong>Stack</strong>结构实现）,后序遍历的非递归的实现相对来说要难一些，因为<strong>后序遍历要保证根节点在左子树和右子树被访问后才能访问</strong>，思路如下：<ul>
<li><strong>思路1</strong>：核心思想是将暂时不访问的右、左孩子结点依次入栈</li>
</ul>
</li>
</ul>
<ol>
<li>对于任意结点<strong>p</strong>，先将<strong>p</strong>入栈；</li>
<li>若P不存在左孩子和右孩子，或者P存在左孩子或右孩子，但左右孩子已经被输出，则可以直接输出结点P，并将其出栈，将出栈结点P标记为上一个输出的结点，再将此时的栈顶结点设为当前结点；</li>
<li>若不满足2中的条件，则将<strong>p</strong>的右孩子和左孩子依次入栈（先右后左），当前节点重新置为栈顶结点，重复步骤2；</li>
<li>直至栈空，遍历结束。<ul>
<li><strong>思路2</strong>：<ol>
<li>对于任一结点<strong>current</strong>，当结点<strong>current</strong>非空时，将其右孩子结点入栈，然后<strong>current</strong>入栈，设置<strong>current</strong>的左孩子为<strong>current</strong>然后沿左子树一直深度搜索，一直down到最左孩子结点，</li>
<li>设置栈顶元素为<strong>current</strong>，并将其出栈，判断该出栈结点的右孩子是否为空：<ul>
<li>若<strong>current</strong>的右孩子不为空，栈不为空且右孩子是栈顶结点，则再将栈顶结点出栈（<strong>current</strong>的右孩子结点）， 设置<code>current=current.right</code></li>
<li>如果出栈结点的右孩子为空，则可以访问该结点，并且设置<strong>current=null</strong></li>
</ul>
</li>
<li>重复步骤2直至栈为空(null)，遍历结束。</li>
</ol>
</li>
</ul>
</li>
</ol>
<h4 id="层序遍历：从上到下，从左到右"><a href="#层序遍历：从上到下，从左到右" class="headerlink" title="层序遍历：从上到下，从左到右"></a><strong>层序遍历</strong>：从上到下，从左到右</h4><ul>
<li>递归结构</li>
<li>层序遍历的非递归结构可借助队列Queue来实现：</li>
</ul>
<ol>
<li>将根结点入队；</li>
<li>如果队列不空，则进入以下循环：<ul>
<li>将队首的结点出队，并输出该结点；</li>
<li>如果该结点有左孩子，则将其左孩子入队；</li>
<li>如果该结点有右孩子吗，则将其右孩子入队。</li>
</ul>
</li>
</ol>
<hr>
<p>&emsp;&emsp;霍夫曼树以及其他二叉树的知识后续会在博文下次更新时整理。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;本博文总结了树和二叉树的基本概念，简单介绍了二叉树的遍历。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="数据结构" scheme="http://yaodong.ml/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
      <category term="算法" scheme="http://yaodong.ml/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>python学习笔记之面向对象高级编程</title>
    <link href="http://yaodong.ml/python/python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E9%AB%98%E7%BA%A7%E7%BC%96%E7%A8%8B.html"/>
    <id>http://yaodong.ml/python/python学习笔记之面向对象高级编程.html</id>
    <published>2016-11-12T06:44:55.000Z</published>
    <updated>2017-03-14T07:02:58.089Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;本博文是python面向对象高级编程的学习总结，主要参考<a href="http://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000" target="_blank" rel="external">廖雪峰的python教程</a>整理而来。<br><a id="more"></a></p>
<h3 id="使用-slot-限制动态添加属性"><a href="#使用-slot-限制动态添加属性" class="headerlink" title="使用__slot__限制动态添加属性"></a>使用<code>__slot__</code>限制动态添加属性</h3><p>&emsp;&emsp;python作为一门动态语言，比静态语言更灵活，功能更强大。python可以实现对类的具体对象添加属性和方法，但是对类的一个具体实例动态添加属性和方法只对当前实例对象有效，对该类的其他实例对象是不起作用的。<br>&emsp;&emsp;给类的所有实例对象都绑定属性方法，可以<code>MethodType()</code>函数通过给class绑定属性和方法实现：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> types <span class="keyword">import</span> MethodType</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyObject</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="keyword">pass</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">new_method</span><span class="params">(self,var)</span>:</span></div><div class="line">    self.var = var</div><div class="line">% 给MyObject类添加new_method方法</div><div class="line">MyObject.new_method = MethodType(new_method,<span class="keyword">None</span>,MyObject)</div></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;python语言的动态特性允许在程序运行的过程中为对象的所属类添加属性或方法。相应地，为了保护自定义类的固有属性和方法，在定义class时需要通过一定的措施来限制对该class动态添加属性和方法。在定义class时，可添加一个<code>__slots__</code>变量，来限制该class能添加的属性：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span><span class="params">(object)</span>:</span></div><div class="line">    __slots__ = (<span class="string">"name"</span>,<span class="string">"age"</span>)   <span class="comment"># 用tuple定义允许绑定的属性名称</span></div></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;通过<code>__slots__</code>变量对class进行约束后，如果在程序运行过程中对class添加<code>__slots__</code>中不存在的属性，则会抛出AttributeError。<br>&emsp;&emsp;使用<code>__slots__</code>要注意，<code>__slots__</code>定义的属性仅对当前class起作用，对继承该class的子类是不起作用的。除非在子类中也定义<code>__slots__</code>，这样，子类允许定义的属性就是自身的<code>__slots__</code>加上父类的<code>__slots__</code>。</p>
<h3 id="使用-property"><a href="#使用-property" class="headerlink" title="使用@property"></a>使用@property</h3><p>&emsp;&emsp;对class添加属性时，要进行参数检查，检查参数的格式或数值范围。传统的实现方法是在set_xx()方法里加入判断：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_score</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> self._score</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_score</span><span class="params">(self, value)</span>:</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(value, int):</div><div class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'score must be an integer!'</span>)</div><div class="line">        <span class="keyword">if</span> value &lt; <span class="number">0</span> <span class="keyword">or</span> value &gt; <span class="number">100</span>:</div><div class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'score must between 0 ~ 100!'</span>)</div><div class="line">        self._score = value</div></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;python的装饰器（generator）可以给函数在程序运行期间添加属性和方法。Python内置的<code>@property</code>装饰器可以把一个方法变成属性以便于调用。把一个<code>getter()</code>方法变成属性，只需在相应函数前加上<code>@property</code>即可。此外，@property的另一个装饰器<code>@score.setter</code>，可以把一个setter()方法变成属性赋值：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span><span class="params">(object)</span>:</span></div><div class="line"><span class="meta">    @property</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span><span class="params">(self)</span>:</span>   <span class="comment">#实现get_score功能</span></div><div class="line">        <span class="keyword">return</span> self._score</div><div class="line"><span class="meta">    @score.setter</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span><span class="params">(self, value)</span>:</span>    <span class="comment"># 实现set_score功能</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(value, int):</div><div class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'score must be an integer!'</span>)</div><div class="line">        <span class="keyword">if</span> value &lt; <span class="number">0</span> <span class="keyword">or</span> value &gt; <span class="number">100</span>:</div><div class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'score must between 0 ~ 100!'</span>)</div><div class="line">        self._score = value</div></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;定义只读属性：只定义<code>getter()</code>方法，不定义<code>setter()</code>方法。有了<code>@property</code>，对类的指定属性只添加<code>@property</code>，不添加<code>@property.setter</code>即可实现只读属性。</p>
<h3 id="多重继承"><a href="#多重继承" class="headerlink" title="多重继承"></a>多重继承</h3><p>&emsp;&emsp;通过继承，子类可以在实现父类所有功能的基础上，扩展更多自定义功能。Python支持多重继承，这一点区别于Java。通过多重继承，子类可以同时获得多个父类的所有功能。<br>&emsp;&emsp; <strong>Mixin</strong>是python多重继承中的一种设计模式，可以看作多重继承在特定场景下的一种应用。Mixin实质上是利用语言特性来更简洁地实现组合模式。类比于Java，Mixin相当于Java中以<code>-able</code>结尾的接口，不同的是，传统的<code>interface</code>概念并不包含实现，而Mixin包含实现。多重继承中，继承Mixin类是为了在原有父类的基础上添加某些可选功能。通俗的讲：<strong>继承强调$I \ am$，Mixin强调$I \ can$</strong>。<br>&emsp;&emsp;<strong>Mixin</strong>的目的就是给一个类增加多个功能。在设计类时，优先考虑通过多重继承来组合多个Mixin的功能，而不是设计多层次的复杂的继承关系。</p>
<h3 id="定制类"><a href="#定制类" class="headerlink" title="定制类"></a>定制类</h3><p>&emsp;&emsp;<strong>python</strong>中，形如<code>__xxx__</code>的变量或者函数都是有特殊用途的，这些特殊的函数可以帮助设计定制类。<br>&emsp;&emsp;<code>__str__</code>可用于打印class的基本信息，比如变量和方法等。<code>__str__</code>相当于Java中的<code>toString()</code>方法。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name)</span>:</span></div><div class="line">        self.name = name</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> <span class="string">'Student object (name: %s)'</span> % self.name</div><div class="line"></div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> Student(<span class="string">'Michael'</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>Student object (name: Michael)</div></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;<code>__iter__</code>用于生成可迭代对象，使得class可以用于循环中。该方法返回一个迭代对象。class定义<code>__iter__</code>的同时，还需要定义<code>next()</code>方法，便于访问可迭代对象。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Fib</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.a, self.b = <span class="number">0</span>, <span class="number">1</span> <span class="comment"># 初始化两个计数器a，b</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> self <span class="comment"># 实例本身就是迭代对象，故返回自己</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">next</span><span class="params">(self)</span>:</span></div><div class="line">        self.a, self.b = self.b, self.a + self.b <span class="comment"># 计算下一个值</span></div><div class="line">        <span class="keyword">if</span> self.a &gt; <span class="number">100000</span>: <span class="comment"># 退出循环的条件</span></div><div class="line">            <span class="keyword">raise</span> StopIteration();</div><div class="line">        <span class="keyword">return</span> self.a <span class="comment"># 返回下一个值</span></div></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;<code>__getitem__()</code>用于获取可迭代对象指定索引位置的元素。若class仅实现<code>__iter__</code>方法，则只能通过<code>next()</code>方法从前到后逐个获取class对象的元素，不能像list一样按照下标访问元素，也不能使用切片（slice）访问部分元素，<code>__getitem__()</code>方法可以实现这个功能。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Fib</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, n)</span>:</span></div><div class="line">        <span class="keyword">if</span> isinstance(n, int):</div><div class="line">            a, b = <span class="number">1</span>, <span class="number">1</span></div><div class="line">            <span class="keyword">for</span> x <span class="keyword">in</span> range(n):</div><div class="line">                a, b = b, a + b</div><div class="line">            <span class="keyword">return</span> a</div><div class="line">        <span class="keyword">if</span> isinstance(n, slice):</div><div class="line">            start = n.start</div><div class="line">            stop = n.stop</div><div class="line">            a, b = <span class="number">1</span>, <span class="number">1</span></div><div class="line">            L = []</div><div class="line">            <span class="keyword">for</span> x <span class="keyword">in</span> range(stop):</div><div class="line">                <span class="keyword">if</span> x &gt;= start:</div><div class="line">                    L.append(a)</div><div class="line">                a, b = b, a + b</div><div class="line">            <span class="keyword">return</span> L</div></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;<code>__setitem__()</code>方法，把class对象视为list或dict来对集合赋值。<br>&emsp;&emsp;<code>__delitem__()</code>方法，用于删除某个元素<br>&emsp;&emsp;<code>__getattr__()</code>方法，用于解决class对象调用不存在的方法或属性，系统报错的问题。注意，只有在没有找到属性的情况下，才调用<code>__getattr__</code>，已存在的属性，不会在<code>__getattr__</code>中查找。<br>&emsp;&emsp;<code>__call__()</code>方法，实现调用class对象自身：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name)</span>:</span></div><div class="line">        self.name = name</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self)</span>:</span></div><div class="line">        print(<span class="string">'My name is %s.'</span> % self.name)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>s = Student(<span class="string">'Michael'</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>s()</div><div class="line">My name <span class="keyword">is</span> Michael.</div></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;<code>__call__()</code>还可以定义参数。对class实例对象进行直接调用就好比对一个函数进行调用一样，所以完全可以把对象看成函数，把函数看成对象，因为这两者之间本来就没有根本的区别。<br>&emsp;&emsp;<code>callable()</code>函数，我们就可以判断一个class实例对象是否是“可调用”对象。</p>
<h3 id="元类metaclass"><a href="#元类metaclass" class="headerlink" title="元类metaclass"></a>元类metaclass</h3><p>&emsp;&emsp;动态语言和静态语言最大的不同，就是类和函数不是编译时定义的，而是在运行时动态创建。也就是说，class的定义是运行时动态创建的。</p>
<h4 id="type-函数创建类"><a href="#type-函数创建类" class="headerlink" title="type()函数创建类"></a>type()函数创建类</h4><p>&emsp;&emsp;创建class可以使用type()函数。type()函数可以查看一个类型或变量的类型，上述代码中，Hello是一个class，它的类型就是type，而h是一个实例，它的类型就是class Hello。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Hello</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hello</span><span class="params">(self, name=<span class="string">'world'</span>)</span>:</span></div><div class="line">        print(<span class="string">'Hello, %s.'</span> % name)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> hello <span class="keyword">import</span> Hello</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>h = Hello()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>h.hello()</div><div class="line">Hello, world.</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(type(Hello))</div><div class="line">&lt;type <span class="string">'type'</span>&gt;</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(type(h))</div><div class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">hello</span>.<span class="title">Hello</span>'&gt;</span></div></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;type()函数既可以返回一个对象的类型，又可以创建出新的类型。比如，可以通过type()函数创建出Hello类：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">fn</span><span class="params">(self, name=<span class="string">'world'</span>)</span>:</span> <span class="comment"># 先定义函数</span></div><div class="line">    print(<span class="string">'Hello, %s.'</span> % name)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>Hello = type(<span class="string">'Hello'</span>, (object,), dict(hello=fn)) <span class="comment"># (object,)是tuple的单元素写法</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>h = Hello()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>h.hello()</div><div class="line">Hello, world.</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(type(Hello))</div><div class="line">&lt;type <span class="string">'type'</span>&gt;</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(type(h))</div><div class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">__main__</span>.<span class="title">Hello</span>'&gt;</span></div></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;事实上，底层的Python解释器检测到class定义时，仅仅是扫描一下class定义的语法，然后调用type()函数创建出class。</p>
<h3 id="通过metaclass创建类"><a href="#通过metaclass创建类" class="headerlink" title="通过metaclass创建类"></a>通过metaclass创建类</h3><p>&emsp;&emsp;Python面向对象编程的顺序是：先定义metaclass，根据metaclass创建class类，最后创建class类的实例对象。metaclass允许创建class类或者修改class类。换句话说，可以把class类看成是metaclass创建出来的“实例”。<br>&emsp;&emsp;按照默认习惯，metaclass的类名总是以<code>Metaclass</code>结尾，以便清楚地表示这是一个metaclass。<code>__new__()</code>方法接收到的参数依次是：当前准备创建的类的对象；类的名字；类继承的父类集合；类的方法集合。<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">% metaclass是创建class类，所以必须从`type`类型派生</div><div class="line">class ListMetaclass(type):</div><div class="line">    def __new__(cls, name, bases, attrs):</div><div class="line">        attrs['add'] = lambda self, value: self.append(value)</div><div class="line">        return type.__new__(cls, name, bases, attrs)</div><div class="line">class MyList(list):</div><div class="line">    __metaclass__ = ListMetaclass # 魔术语句，指示Python解释器在创建MyList时，要通过ListMetaclass.__new__()来创建</div></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;一般情况下，不需使用metaclass创建class类，只有特殊情形下需要通过metaclass修改类定义。ORM就是一个典型的例子。ORM全称“Object Relational Mapping”，即对象-关系映射，就是把关系数据库的一行映射为一个对象，也就是一个类对应一个表，这样，后续写代码更简单，无需直接操作SQL语句。要编写一个ORM框架，所有的类都只能动态定义，因为只有使用者才能根据表的结构定义出对应的类来。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;本博文是python面向对象高级编程的学习总结，主要参考&lt;a href=&quot;http://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000&quot;&gt;廖雪峰的python教程&lt;/a&gt;整理而来。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yaodong.ml/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="python" scheme="http://yaodong.ml/tags/python/"/>
    
  </entry>
  
</feed>
